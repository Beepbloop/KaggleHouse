{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import ReLU\n",
    "from keras.layers import LeakyReLU\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 405)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('~/Datas/KaggleHouse/X_train_V2.csv',header=None).to_numpy()\n",
    "y = pd.read_csv('~/Datas/KaggleHouse/Y_train_V2_Nolog.csv',header=None).to_numpy()\n",
    "X_final = pd.read_csv('~/Datas/KaggleHouse/X_test_V2.csv',header=None).to_numpy()\n",
    "Id = pd.read_csv('~/Datas/KaggleHouse/Id.csv',header=None,names=['Id'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = X.shape[1]\n",
    "TRAIN_LEN = X.shape[0]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model):\n",
    "    return model.predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_square_expd_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.exp(y_pred)-K.exp(y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_org = pd.read_csv('~/Datas/KaggleHouse/Y_train_V2_Nolog.csv',header=None).to_numpy()\n",
    "# y_log = pd.read_csv('~/Datas/KaggleHouse/Y_train_V2.csv',header=None).to_numpy()\n",
    "# arr = y_log[100]\n",
    "# K.exp(arr),np.exp(arr),y_org[100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_26 (Dense)             (None, 512)               207872    \n",
      "_________________________________________________________________\n",
      "re_lu_13 (ReLU)              (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "re_lu_14 (ReLU)              (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "re_lu_15 (ReLU)              (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 733,697\n",
      "Trainable params: 733,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_500E_Adam = Sequential()\n",
    "NN_500E_Adam.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam.add(ReLU())\n",
    "NN_500E_Adam.add(Dense(512))\n",
    "NN_500E_Adam.add(ReLU())\n",
    "NN_500E_Adam.add(Dense(512))\n",
    "NN_500E_Adam.add(ReLU())\n",
    "NN_500E_Adam.add(Dense(1))\n",
    "NN_500E_Adam.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 437 samples\n",
      "Epoch 1/500\n",
      "1019/1019 [==============================] - 0s 131us/step - loss: nan - val_loss: nan\n",
      "Epoch 2/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 3/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 4/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 5/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 6/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 7/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 8/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 9/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 10/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 11/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: nan - val_loss: nan\n",
      "Epoch 12/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 13/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 14/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 15/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 16/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 17/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 18/500\n",
      "1019/1019 [==============================] - 0s 20us/step - loss: nan - val_loss: nan\n",
      "Epoch 19/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 20/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 21/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 22/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 23/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 24/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 25/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 26/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 27/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 28/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 29/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: nan - val_loss: nan\n",
      "Epoch 30/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 31/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 32/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 33/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: nan - val_loss: nan\n",
      "Epoch 34/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 35/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 36/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 37/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 38/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 39/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 40/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: nan - val_loss: nan\n",
      "Epoch 41/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 42/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 43/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 44/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 45/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 46/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 47/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 48/500\n",
      "1019/1019 [==============================] - 0s 21us/step - loss: nan - val_loss: nan\n",
      "Epoch 49/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Epoch 50/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: nan - val_loss: nan\n",
      "Restoring model weights from the end of the best epoch\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-d9a60e38a230>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrestore_best_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mNN_500E_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroot_mean_squared_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_500E_Adam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTRAIN_LEN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mes\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    214\u001b[0m                         \u001b[0mepoch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m             \u001b[0mcallback\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_epoch_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/callbacks/callbacks.py\u001b[0m in \u001b[0;36mon_epoch_end\u001b[0;34m(self, epoch, logs)\u001b[0m\n\u001b[1;32m    832\u001b[0m                         print('Restoring model weights from the end of '\n\u001b[1;32m    833\u001b[0m                               'the best epoch')\n\u001b[0;32m--> 834\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    835\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/network.py\u001b[0m in \u001b[0;36mset_weights\u001b[0;34m(self, weights)\u001b[0m\n\u001b[1;32m    514\u001b[0m         \u001b[0mown_weight_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_trainable_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_non_trainable_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m         \u001b[0mnum_param\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_weight_vars\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m         \u001b[0mown_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnum_param\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    517\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mown_weight_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mown_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m             \u001b[0mtuples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience = 50, mode='min', restore_best_weights=True, verbose=1)\n",
    "NN_500E_Adam.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "history = NN_500E_Adam.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=500,validation_split=0.3,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# NN_500E_Adam.save('/home/louisliu2096/Models/KaggleHouse/NN_500E_Adam_V2_Es.h5')\n",
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_500E_NewAdam = Sequential()\n",
    "NN_500E_NewAdam.add(Dense(512,input_dim = IN_DIM,activation = 'relu'))\n",
    "NN_500E_NewAdam.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_NewAdam.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_NewAdam.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1164/1164 [==============================] - 0s 300us/step - loss: 148543.6407 - val_loss: 92017.5689\n",
      "Epoch 2/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 95069.6305 - val_loss: 63413.2130\n",
      "Epoch 3/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 65197.5775 - val_loss: 50614.4189\n",
      "Epoch 4/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 49147.6446 - val_loss: 45944.5010\n",
      "Epoch 5/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 45520.8952 - val_loss: 43939.2352\n",
      "Epoch 6/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 42889.4300 - val_loss: 42513.9320\n",
      "Epoch 7/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 41225.1446 - val_loss: 41413.3865\n",
      "Epoch 8/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 40358.3156 - val_loss: 40452.0210\n",
      "Epoch 9/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 40023.0346 - val_loss: 40025.8317\n",
      "Epoch 10/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 39262.7343 - val_loss: 40795.7306\n",
      "Epoch 11/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38845.6364 - val_loss: 41281.1888\n",
      "Epoch 12/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 38921.7134 - val_loss: 39276.1199\n",
      "Epoch 13/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39902.0201 - val_loss: 39431.3181\n",
      "Epoch 14/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39023.4593 - val_loss: 39311.4817\n",
      "Epoch 15/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38523.3484 - val_loss: 40710.1223\n",
      "Epoch 16/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38435.5028 - val_loss: 41224.1091\n",
      "Epoch 17/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39382.4933 - val_loss: 39034.8523\n",
      "Epoch 18/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39083.4810 - val_loss: 40083.7702\n",
      "Epoch 19/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 38276.1268 - val_loss: 39471.7453\n",
      "Epoch 20/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38490.2226 - val_loss: 39158.4069\n",
      "Epoch 21/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37866.7621 - val_loss: 38976.2904\n",
      "Epoch 22/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 39217.8846 - val_loss: 39937.6647\n",
      "Epoch 23/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37562.8151 - val_loss: 38906.1183\n",
      "Epoch 24/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 38547.1749 - val_loss: 44772.6740\n",
      "Epoch 25/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 38359.1716 - val_loss: 38902.8941\n",
      "Epoch 26/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 37437.8380 - val_loss: 39369.8074\n",
      "Epoch 27/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 38780.1477 - val_loss: 39765.8765\n",
      "Epoch 28/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 38136.3702 - val_loss: 38837.2691\n",
      "Epoch 29/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 37836.2283 - val_loss: 39282.1423\n",
      "Epoch 30/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 37926.9797 - val_loss: 39032.9026\n",
      "Epoch 31/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36902.6873 - val_loss: 39039.8823\n",
      "Epoch 32/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37227.0302 - val_loss: 40036.3962\n",
      "Epoch 33/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 37928.4655 - val_loss: 39225.7449\n",
      "Epoch 34/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37492.5398 - val_loss: 38522.2016\n",
      "Epoch 35/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36842.5385 - val_loss: 38929.6638\n",
      "Epoch 36/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38402.8919 - val_loss: 39227.8980\n",
      "Epoch 37/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 37161.3837 - val_loss: 39874.2767\n",
      "Epoch 38/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 36695.6533 - val_loss: 41370.5740\n",
      "Epoch 39/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36582.1180 - val_loss: 39463.2577\n",
      "Epoch 40/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 36489.3075 - val_loss: 38714.8542\n",
      "Epoch 41/500\n",
      "1164/1164 [==============================] - 0s 166us/step - loss: 36088.6029 - val_loss: 38751.5040\n",
      "Epoch 42/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 35763.8288 - val_loss: 40399.1791\n",
      "Epoch 43/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 36290.8722 - val_loss: 38053.2480\n",
      "Epoch 44/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 35873.0835 - val_loss: 38673.2439\n",
      "Epoch 45/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36202.9673 - val_loss: 38114.6500\n",
      "Epoch 46/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 35960.6971 - val_loss: 39544.1947\n",
      "Epoch 47/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 35727.2730 - val_loss: 38042.1552\n",
      "Epoch 48/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 36216.6497 - val_loss: 38525.1109\n",
      "Epoch 49/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 36043.3691 - val_loss: 37936.2178\n",
      "Epoch 50/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 36563.9113 - val_loss: 38400.9868\n",
      "Epoch 51/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 35705.7379 - val_loss: 39139.7305\n",
      "Epoch 52/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 35224.4780 - val_loss: 37751.9561\n",
      "Epoch 53/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36192.1846 - val_loss: 40176.4567\n",
      "Epoch 54/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 35057.2067 - val_loss: 37912.3700\n",
      "Epoch 55/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 35133.7500 - val_loss: 37912.6365\n",
      "Epoch 56/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34982.5675 - val_loss: 38237.6900\n",
      "Epoch 57/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 36790.6518 - val_loss: 39853.9280\n",
      "Epoch 58/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 36181.3465 - val_loss: 38740.7345\n",
      "Epoch 59/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 36185.8245 - val_loss: 38897.7032\n",
      "Epoch 60/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34944.9452 - val_loss: 37749.9111\n",
      "Epoch 61/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 34693.2834 - val_loss: 37907.7400\n",
      "Epoch 62/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 34457.6854 - val_loss: 37919.2896\n",
      "Epoch 63/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 34075.1614 - val_loss: 37768.0381\n",
      "Epoch 64/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34045.9859 - val_loss: 38013.8103\n",
      "Epoch 65/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34399.5015 - val_loss: 38595.2138\n",
      "Epoch 66/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34619.7333 - val_loss: 38058.4076\n",
      "Epoch 67/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 35054.9943 - val_loss: 37847.6618\n",
      "Epoch 68/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 34108.9022 - val_loss: 38620.2705\n",
      "Epoch 69/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 35001.6549 - val_loss: 40650.9433\n",
      "Epoch 70/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 34941.5694 - val_loss: 38526.1198\n",
      "Epoch 71/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 33604.5765 - val_loss: 37610.9923\n",
      "Epoch 72/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34520.3321 - val_loss: 40756.9881\n",
      "Epoch 73/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33801.9173 - val_loss: 38212.9342\n",
      "Epoch 74/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 34622.5953 - val_loss: 38874.0683\n",
      "Epoch 75/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34560.0943 - val_loss: 38130.1095\n",
      "Epoch 76/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34121.2530 - val_loss: 37705.3349\n",
      "Epoch 77/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 33761.6682 - val_loss: 37678.3809\n",
      "Epoch 78/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 34488.6496 - val_loss: 38135.3879\n",
      "Epoch 79/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33566.9425 - val_loss: 38152.0231\n",
      "Epoch 80/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 33350.5716 - val_loss: 38915.0794\n",
      "Epoch 81/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 33038.3593 - val_loss: 38205.9915\n",
      "Epoch 82/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 34341.5155 - val_loss: 38641.3517\n",
      "Epoch 83/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 33748.2984 - val_loss: 38016.7200\n",
      "Epoch 84/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 33769.5689 - val_loss: 37541.2317\n",
      "Epoch 85/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 33614.7244 - val_loss: 38043.5220\n",
      "Epoch 86/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 33272.2463 - val_loss: 42478.0633\n",
      "Epoch 87/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34613.9309 - val_loss: 37512.6988\n",
      "Epoch 88/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32826.2375 - val_loss: 39375.6263\n",
      "Epoch 89/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33698.2653 - val_loss: 37521.9827\n",
      "Epoch 90/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32910.8246 - val_loss: 37170.9016\n",
      "Epoch 91/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 33721.2737 - val_loss: 37575.0855\n",
      "Epoch 92/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 32659.7575 - val_loss: 38470.1016\n",
      "Epoch 93/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32569.3491 - val_loss: 37513.2313\n",
      "Epoch 94/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 32050.2923 - val_loss: 37525.5856\n",
      "Epoch 95/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 32377.1063 - val_loss: 37358.9418\n",
      "Epoch 96/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32529.3891 - val_loss: 37247.0393\n",
      "Epoch 97/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33198.9818 - val_loss: 37721.2317\n",
      "Epoch 98/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32704.7623 - val_loss: 37749.6313\n",
      "Epoch 99/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 32517.4010 - val_loss: 38106.7773\n",
      "Epoch 100/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32910.8036 - val_loss: 37371.7634\n",
      "Epoch 101/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32515.7649 - val_loss: 39225.5967\n",
      "Epoch 102/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 32207.3082 - val_loss: 37370.9420\n",
      "Epoch 103/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32702.7573 - val_loss: 38246.3472\n",
      "Epoch 104/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32885.4731 - val_loss: 37240.7846\n",
      "Epoch 105/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32354.9823 - val_loss: 38681.4775\n",
      "Epoch 106/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32272.6336 - val_loss: 37330.1860\n",
      "Epoch 107/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31557.8731 - val_loss: 37596.4870\n",
      "Epoch 108/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31771.4983 - val_loss: 37970.8753\n",
      "Epoch 109/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31837.1466 - val_loss: 37395.7945\n",
      "Epoch 110/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 31532.9808 - val_loss: 37257.8425\n",
      "Epoch 111/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 32811.4897 - val_loss: 39368.4255\n",
      "Epoch 112/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 32102.6311 - val_loss: 37603.1785\n",
      "Epoch 113/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 31319.1244 - val_loss: 37314.5582\n",
      "Epoch 114/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 31453.7761 - val_loss: 36976.5889\n",
      "Epoch 115/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 31558.1958 - val_loss: 38135.7990\n",
      "Epoch 116/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32455.6183 - val_loss: 36996.3074\n",
      "Epoch 117/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32172.0743 - val_loss: 42368.0679\n",
      "Epoch 118/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32261.6823 - val_loss: 37889.8272\n",
      "Epoch 119/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 30942.8186 - val_loss: 37583.5994\n",
      "Epoch 120/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 31511.3067 - val_loss: 37898.9994\n",
      "Epoch 121/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31120.5221 - val_loss: 37585.5035\n",
      "Epoch 122/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 30844.8691 - val_loss: 37264.2702\n",
      "Epoch 123/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31156.6265 - val_loss: 37917.1937\n",
      "Epoch 124/500\n",
      "1164/1164 [==============================] - 0s 171us/step - loss: 30775.7900 - val_loss: 37664.2216\n",
      "Epoch 125/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 31925.2584 - val_loss: 37133.4248\n",
      "Epoch 126/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 32461.5698 - val_loss: 37262.7513\n",
      "Epoch 127/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 29857.2676 - val_loss: 37198.6891\n",
      "Epoch 128/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 30454.8552 - val_loss: 37630.1721\n",
      "Epoch 129/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 31486.7042 - val_loss: 38385.0885\n",
      "Epoch 130/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30630.0513 - val_loss: 37383.9728\n",
      "Epoch 131/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30267.7791 - val_loss: 37583.3065\n",
      "Epoch 132/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 29524.0867 - val_loss: 37170.0038\n",
      "Epoch 133/500\n",
      "1164/1164 [==============================] - ETA: 0s - loss: 29289.508 - 0s 177us/step - loss: 29741.0795 - val_loss: 38355.5923\n",
      "Epoch 134/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 29871.0136 - val_loss: 37729.5106\n",
      "Epoch 135/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 31201.3171 - val_loss: 38585.5762\n",
      "Epoch 136/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 30824.2771 - val_loss: 37027.9282\n",
      "Epoch 137/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 30050.3028 - val_loss: 38903.7132\n",
      "Epoch 138/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 29629.9388 - val_loss: 39083.8024\n",
      "Epoch 139/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 30560.7771 - val_loss: 38496.6070\n",
      "Epoch 140/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30635.0331 - val_loss: 37577.1777\n",
      "Epoch 141/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 29067.5359 - val_loss: 37348.1656\n",
      "Epoch 142/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 29043.3603 - val_loss: 40615.0231\n",
      "Epoch 143/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 173us/step - loss: 30300.9578 - val_loss: 37120.4281\n",
      "Epoch 144/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28803.7107 - val_loss: 39837.7526\n",
      "Epoch 145/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 29633.7259 - val_loss: 37700.4394\n",
      "Epoch 146/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 28818.7375 - val_loss: 37649.8676\n",
      "Epoch 147/500\n",
      "1164/1164 [==============================] - 0s 178us/step - loss: 28790.7979 - val_loss: 37344.8975\n",
      "Epoch 148/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 29145.6958 - val_loss: 37731.7047\n",
      "Epoch 149/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30112.2863 - val_loss: 37618.3568\n",
      "Epoch 150/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 28796.9386 - val_loss: 38545.3280\n",
      "Epoch 151/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28313.8811 - val_loss: 38097.6434\n",
      "Epoch 152/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28885.6029 - val_loss: 37625.0024\n",
      "Epoch 153/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28253.2433 - val_loss: 37185.0954\n",
      "Epoch 154/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 27724.2884 - val_loss: 38256.5359\n",
      "Epoch 155/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 28850.9137 - val_loss: 38470.9125\n",
      "Epoch 156/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 28216.8561 - val_loss: 37623.4275\n",
      "Epoch 157/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 27330.7649 - val_loss: 37937.8291\n",
      "Epoch 158/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 27638.0963 - val_loss: 37460.5820\n",
      "Epoch 159/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 28751.4052 - val_loss: 38402.1224\n",
      "Epoch 160/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 28888.2668 - val_loss: 37348.5605\n",
      "Epoch 161/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28056.0472 - val_loss: 37844.0356\n",
      "Epoch 162/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 27831.7652 - val_loss: 38193.5613\n",
      "Epoch 163/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 27959.6307 - val_loss: 39414.2193\n",
      "Epoch 164/500\n",
      "1164/1164 [==============================] - 0s 172us/step - loss: 28421.7049 - val_loss: 37000.8839\n",
      "Epoch 165/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 28009.7382 - val_loss: 37130.0997\n",
      "Epoch 166/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 26813.2289 - val_loss: 37958.7974\n",
      "Epoch 167/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 27250.3008 - val_loss: 37674.2099\n",
      "Epoch 168/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 26603.1641 - val_loss: 37406.6289\n",
      "Epoch 169/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 26598.3796 - val_loss: 38686.6198\n",
      "Epoch 170/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 27769.1186 - val_loss: 37508.6566\n",
      "Epoch 171/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 27427.4461 - val_loss: 37678.8554\n",
      "Epoch 172/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 26865.5808 - val_loss: 38421.6253\n",
      "Epoch 173/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 26033.3942 - val_loss: 38736.1558\n",
      "Epoch 174/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25849.9523 - val_loss: 37709.4021\n",
      "Epoch 175/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 26305.7334 - val_loss: 37959.3617\n",
      "Epoch 176/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 27214.1016 - val_loss: 37284.7237\n",
      "Epoch 177/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25991.2694 - val_loss: 37799.5433\n",
      "Epoch 178/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 26112.0083 - val_loss: 39340.2163\n",
      "Epoch 179/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25494.6472 - val_loss: 39185.8844\n",
      "Epoch 180/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 25974.2881 - val_loss: 38645.5101\n",
      "Epoch 181/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25556.5378 - val_loss: 37559.9154\n",
      "Epoch 182/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 25280.5966 - val_loss: 37396.2794\n",
      "Epoch 183/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 25650.2815 - val_loss: 39637.0727\n",
      "Epoch 184/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 26955.1238 - val_loss: 37523.6091\n",
      "Epoch 185/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 25887.3478 - val_loss: 38265.5188\n",
      "Epoch 186/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25122.6399 - val_loss: 37925.8438\n",
      "Epoch 187/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25835.5196 - val_loss: 38689.2182\n",
      "Epoch 188/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 24850.3151 - val_loss: 38273.2514\n",
      "Epoch 189/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 25316.4020 - val_loss: 37641.4595\n",
      "Epoch 190/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25028.0693 - val_loss: 37845.3955\n",
      "Epoch 191/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25611.9346 - val_loss: 37793.2922\n",
      "Epoch 192/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 24731.9866 - val_loss: 37716.8632\n",
      "Epoch 193/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25211.2186 - val_loss: 38131.8334\n",
      "Epoch 194/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 24206.1303 - val_loss: 37929.9528\n",
      "Epoch 195/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 25052.9708 - val_loss: 41099.1232\n",
      "Epoch 196/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25461.4176 - val_loss: 39361.3773\n",
      "Epoch 197/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 26418.5227 - val_loss: 37472.1597\n",
      "Epoch 198/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 25267.8392 - val_loss: 39730.6871\n",
      "Epoch 199/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25600.6047 - val_loss: 37666.6244\n",
      "Epoch 200/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 25616.1147 - val_loss: 40511.6755\n",
      "Epoch 201/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 24358.1464 - val_loss: 37225.3182\n",
      "Epoch 202/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 23208.9980 - val_loss: 37616.1899\n",
      "Epoch 203/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23981.9591 - val_loss: 37815.3722\n",
      "Epoch 204/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23663.9609 - val_loss: 37665.8900\n",
      "Epoch 205/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23900.9642 - val_loss: 38211.8616\n",
      "Epoch 206/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23300.2330 - val_loss: 38359.6709\n",
      "Epoch 207/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 24213.9648 - val_loss: 38123.5437\n",
      "Epoch 208/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 24132.9672 - val_loss: 37427.4294\n",
      "Epoch 209/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23567.7269 - val_loss: 38903.8411\n",
      "Epoch 210/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 24840.0212 - val_loss: 38310.6309\n",
      "Epoch 211/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23766.9105 - val_loss: 39170.4030\n",
      "Epoch 212/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23726.8965 - val_loss: 37463.7416\n",
      "Epoch 213/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 24165.2890 - val_loss: 39359.5987\n",
      "Epoch 214/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 24401.0984 - val_loss: 39681.9774\n",
      "Epoch 215/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 23616.3471 - val_loss: 38754.3473\n",
      "Epoch 216/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23108.1409 - val_loss: 37380.7876\n",
      "Epoch 217/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22562.6091 - val_loss: 37394.2743\n",
      "Epoch 218/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22837.3616 - val_loss: 38803.2634\n",
      "Epoch 219/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 23034.9099 - val_loss: 37851.3889\n",
      "Epoch 220/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 22472.5834 - val_loss: 38274.6259\n",
      "Epoch 221/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23944.4890 - val_loss: 38943.7165\n",
      "Epoch 222/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23138.5698 - val_loss: 37759.1853\n",
      "Epoch 223/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 22763.8303 - val_loss: 37454.0975\n",
      "Epoch 224/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 24394.0377 - val_loss: 37517.4260\n",
      "Epoch 225/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 25066.4371 - val_loss: 38181.1810\n",
      "Epoch 226/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 23701.7663 - val_loss: 39166.5416\n",
      "Epoch 227/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 21775.2738 - val_loss: 37486.1422\n",
      "Epoch 228/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 22067.9933 - val_loss: 37151.0756\n",
      "Epoch 229/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 22845.1926 - val_loss: 40254.9620\n",
      "Epoch 230/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 22841.6374 - val_loss: 38087.3340\n",
      "Epoch 231/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 23795.0349 - val_loss: 38458.2839\n",
      "Epoch 232/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21987.0046 - val_loss: 39307.5137\n",
      "Epoch 233/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21667.5782 - val_loss: 37730.6135\n",
      "Epoch 234/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21725.4113 - val_loss: 39489.9604\n",
      "Epoch 235/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 22551.8857 - val_loss: 37456.6099\n",
      "Epoch 236/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21766.8432 - val_loss: 38089.5209\n",
      "Epoch 237/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 21450.9326 - val_loss: 39590.1899\n",
      "Epoch 238/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22647.9046 - val_loss: 40827.3250\n",
      "Epoch 239/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 23612.6508 - val_loss: 37786.8867\n",
      "Epoch 240/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 22085.3160 - val_loss: 37980.4027\n",
      "Epoch 241/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 21139.3389 - val_loss: 38093.7318\n",
      "Epoch 242/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21476.7382 - val_loss: 37647.7593\n",
      "Epoch 243/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 20727.4548 - val_loss: 38788.3942\n",
      "Epoch 244/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 20733.8662 - val_loss: 39366.0122\n",
      "Epoch 245/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 21793.5414 - val_loss: 38913.1234\n",
      "Epoch 246/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21168.6101 - val_loss: 38176.9354\n",
      "Epoch 247/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20983.9584 - val_loss: 38002.9442\n",
      "Epoch 248/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 20550.9601 - val_loss: 37834.4754\n",
      "Epoch 249/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22429.8972 - val_loss: 37803.7357\n",
      "Epoch 250/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 21513.6880 - val_loss: 39095.9978\n",
      "Epoch 251/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 22123.1730 - val_loss: 37385.3510\n",
      "Epoch 252/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 21329.6272 - val_loss: 38259.5141\n",
      "Epoch 253/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20689.1722 - val_loss: 37801.4029\n",
      "Epoch 254/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 21106.6295 - val_loss: 37518.5321\n",
      "Epoch 255/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 20835.7314 - val_loss: 37438.0989\n",
      "Epoch 256/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 21129.6880 - val_loss: 37915.1560\n",
      "Epoch 257/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21246.4216 - val_loss: 39090.9880\n",
      "Epoch 258/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20713.5809 - val_loss: 41687.4096\n",
      "Epoch 259/500\n",
      "1164/1164 [==============================] - 0s 154us/step - loss: 21996.1363 - val_loss: 37931.6601\n",
      "Epoch 260/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21125.0885 - val_loss: 38291.9193\n",
      "Epoch 261/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23932.0272 - val_loss: 38044.9541\n",
      "Epoch 262/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 20368.8539 - val_loss: 38416.1695\n",
      "Epoch 263/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21099.3296 - val_loss: 37414.1254\n",
      "Epoch 264/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21702.3986 - val_loss: 38152.4858\n",
      "Epoch 265/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 19863.1092 - val_loss: 37645.1324\n",
      "Epoch 266/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19217.9720 - val_loss: 37539.5672\n",
      "Epoch 267/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 20202.2089 - val_loss: 39647.9312\n",
      "Epoch 268/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 20887.0760 - val_loss: 37111.4004\n",
      "Epoch 269/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 21620.5414 - val_loss: 37603.1153\n",
      "Epoch 270/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20163.5765 - val_loss: 38587.8747\n",
      "Epoch 271/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19923.9087 - val_loss: 37614.7137\n",
      "Epoch 272/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19569.1868 - val_loss: 38325.1320\n",
      "Epoch 273/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19681.5746 - val_loss: 37763.4749\n",
      "Epoch 274/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 19105.3479 - val_loss: 38980.0793\n",
      "Epoch 275/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 20857.6504 - val_loss: 37648.0350\n",
      "Epoch 276/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 19100.0269 - val_loss: 39036.2216\n",
      "Epoch 277/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19652.6779 - val_loss: 38913.7207\n",
      "Epoch 278/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18983.3385 - val_loss: 37693.3802\n",
      "Epoch 279/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 18430.1724 - val_loss: 37902.4269\n",
      "Epoch 280/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19392.6579 - val_loss: 38478.9181\n",
      "Epoch 281/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19146.8597 - val_loss: 37224.9610\n",
      "Epoch 282/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18781.4567 - val_loss: 40000.4411\n",
      "Epoch 283/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19076.2832 - val_loss: 38050.7733\n",
      "Epoch 284/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19080.7690 - val_loss: 39219.9004\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 160us/step - loss: 19223.7877 - val_loss: 38843.5109\n",
      "Epoch 286/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18868.5739 - val_loss: 37600.3272\n",
      "Epoch 287/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18485.2802 - val_loss: 38107.1495\n",
      "Epoch 288/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18463.5082 - val_loss: 40261.4744\n",
      "Epoch 289/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19218.3906 - val_loss: 40034.7559\n",
      "Epoch 290/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19275.4340 - val_loss: 38474.6564\n",
      "Epoch 291/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 18869.3455 - val_loss: 37746.1772\n",
      "Epoch 292/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 20223.0720 - val_loss: 37732.5231\n",
      "Epoch 293/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 20127.6555 - val_loss: 37881.6365\n",
      "Epoch 294/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 20102.7116 - val_loss: 37571.3051\n",
      "Epoch 295/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 18875.3649 - val_loss: 38576.2475\n",
      "Epoch 296/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18821.4416 - val_loss: 38011.4563\n",
      "Epoch 297/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 19650.7209 - val_loss: 38353.2147\n",
      "Epoch 298/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19720.5634 - val_loss: 37847.6343\n",
      "Epoch 299/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 17759.6121 - val_loss: 37856.1966\n",
      "Epoch 300/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18638.2556 - val_loss: 37898.9138\n",
      "Epoch 301/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 17435.6651 - val_loss: 38000.3481\n",
      "Epoch 302/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17191.8700 - val_loss: 37770.5442\n",
      "Epoch 303/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 18646.6198 - val_loss: 40796.6436\n",
      "Epoch 304/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 19026.9206 - val_loss: 41464.2781\n",
      "Epoch 305/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18678.4735 - val_loss: 38931.1043\n",
      "Epoch 306/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18044.2267 - val_loss: 38389.7065\n",
      "Epoch 307/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 17442.1048 - val_loss: 39335.8297\n",
      "Epoch 308/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18827.4116 - val_loss: 39772.3940\n",
      "Epoch 309/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17324.6472 - val_loss: 38547.4348\n",
      "Epoch 310/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18413.5673 - val_loss: 38043.9988\n",
      "Epoch 311/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20688.6768 - val_loss: 38838.3060\n",
      "Epoch 312/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18593.1461 - val_loss: 40592.2323\n",
      "Epoch 313/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18379.1248 - val_loss: 38968.5753\n",
      "Epoch 314/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 18270.8143 - val_loss: 38675.8415\n",
      "Epoch 315/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 19632.3524 - val_loss: 38623.8301\n",
      "Epoch 316/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 17586.1073 - val_loss: 40241.1493\n",
      "Epoch 317/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 17398.4044 - val_loss: 37696.6504\n",
      "Epoch 318/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16544.6381 - val_loss: 39740.0193\n",
      "Epoch 319/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17537.4563 - val_loss: 38654.3097\n",
      "Epoch 320/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18039.6075 - val_loss: 38779.5340\n",
      "Epoch 321/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 17608.3681 - val_loss: 40690.8045\n",
      "Epoch 322/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 17210.4401 - val_loss: 38471.6595\n",
      "Epoch 323/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17388.5854 - val_loss: 39548.6759\n",
      "Epoch 324/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16895.4578 - val_loss: 38807.2674\n",
      "Epoch 325/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16660.2819 - val_loss: 39207.6022\n",
      "Epoch 326/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16515.8929 - val_loss: 38792.1101\n",
      "Epoch 327/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16183.1116 - val_loss: 40038.3401\n",
      "Epoch 328/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16553.9980 - val_loss: 38404.0399\n",
      "Epoch 329/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16844.2291 - val_loss: 39725.0360\n",
      "Epoch 330/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 16406.5155 - val_loss: 39450.1851\n",
      "Epoch 331/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16618.7569 - val_loss: 39087.1997\n",
      "Epoch 332/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18734.0362 - val_loss: 39386.1453\n",
      "Epoch 333/500\n",
      "1164/1164 [==============================] - 0s 155us/step - loss: 16880.3776 - val_loss: 39326.3884\n",
      "Epoch 334/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16235.9938 - val_loss: 38325.6917\n",
      "Epoch 335/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16987.4169 - val_loss: 39240.1021\n",
      "Epoch 336/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16183.6002 - val_loss: 38397.3274\n",
      "Epoch 337/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 16206.6551 - val_loss: 40297.5967\n",
      "Epoch 338/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 16493.8527 - val_loss: 39056.5242\n",
      "Epoch 339/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16576.2259 - val_loss: 39242.8662\n",
      "Epoch 340/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 16157.8047 - val_loss: 39509.8276\n",
      "Epoch 341/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 16155.5570 - val_loss: 43999.5747\n",
      "Epoch 342/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 17659.4900 - val_loss: 39872.9080\n",
      "Epoch 343/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16041.9440 - val_loss: 39447.9648\n",
      "Epoch 344/500\n",
      "1164/1164 [==============================] - 0s 172us/step - loss: 15673.1208 - val_loss: 39720.3668\n",
      "Epoch 345/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 15723.7619 - val_loss: 41059.1589\n",
      "Epoch 346/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 15659.5345 - val_loss: 39314.5558\n",
      "Epoch 347/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 17793.4485 - val_loss: 38946.9845\n",
      "Epoch 348/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 17895.6257 - val_loss: 40081.7414\n",
      "Epoch 349/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19470.7780 - val_loss: 38344.0825\n",
      "Epoch 350/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16654.1534 - val_loss: 40551.8722\n",
      "Epoch 351/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16154.9522 - val_loss: 38950.8379\n",
      "Epoch 352/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 15783.5996 - val_loss: 39842.1618\n",
      "Epoch 353/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 15364.7552 - val_loss: 39696.4467\n",
      "Epoch 354/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16348.9560 - val_loss: 40639.8262\n",
      "Epoch 355/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 16932.3546 - val_loss: 39019.6820\n",
      "Epoch 356/500\n",
      "1164/1164 [==============================] - 0s 172us/step - loss: 15884.4802 - val_loss: 39214.2293\n",
      "Epoch 357/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 16879.8036 - val_loss: 39219.8838\n",
      "Epoch 358/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 15991.7709 - val_loss: 39620.2223\n",
      "Epoch 359/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 15310.2276 - val_loss: 40906.7883\n",
      "Epoch 360/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 18096.1780 - val_loss: 41176.5976\n",
      "Epoch 361/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15911.4205 - val_loss: 39136.8223\n",
      "Epoch 362/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 15799.2978 - val_loss: 39932.8179\n",
      "Epoch 363/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15710.5642 - val_loss: 39267.6482\n",
      "Epoch 364/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 14945.9055 - val_loss: 40002.8494\n",
      "Epoch 365/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 15202.7582 - val_loss: 39705.6155\n",
      "Epoch 366/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 15532.0875 - val_loss: 40776.6758\n",
      "Epoch 367/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15719.4604 - val_loss: 40346.9848\n",
      "Epoch 368/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 16742.3957 - val_loss: 40549.4224\n",
      "Epoch 369/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 16026.7432 - val_loss: 38870.5817\n",
      "Epoch 370/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 14611.4658 - val_loss: 40391.4457\n",
      "Epoch 371/500\n",
      "1164/1164 [==============================] - 0s 166us/step - loss: 14564.3721 - val_loss: 39229.4577\n",
      "Epoch 372/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 15510.9331 - val_loss: 40573.2604\n",
      "Epoch 373/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15812.8844 - val_loss: 39948.1112\n",
      "Epoch 374/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 15604.5701 - val_loss: 40308.5121\n",
      "Epoch 375/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 14015.1608 - val_loss: 39699.2989\n",
      "Epoch 376/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 14978.8721 - val_loss: 40573.9130\n",
      "Epoch 377/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 13895.4619 - val_loss: 41221.4015\n",
      "Epoch 378/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 14631.4050 - val_loss: 39479.7165\n",
      "Epoch 379/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 14707.3950 - val_loss: 40691.0412\n",
      "Epoch 380/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 14559.2160 - val_loss: 40187.4277\n",
      "Epoch 381/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14275.8056 - val_loss: 39656.5324\n",
      "Epoch 382/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 15039.0907 - val_loss: 41235.5670\n",
      "Epoch 383/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 15476.3939 - val_loss: 43451.1041\n",
      "Epoch 384/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 15079.5113 - val_loss: 40103.2007\n",
      "Epoch 385/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 14770.9515 - val_loss: 39697.1952\n",
      "Epoch 386/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14142.5222 - val_loss: 41700.0600\n",
      "Epoch 387/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16187.0358 - val_loss: 40964.1201\n",
      "Epoch 388/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14780.9313 - val_loss: 40297.9461\n",
      "Epoch 389/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 14936.2823 - val_loss: 42972.8662\n",
      "Epoch 390/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14895.1076 - val_loss: 39739.1348\n",
      "Epoch 391/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 15707.6732 - val_loss: 40071.4268\n",
      "Epoch 392/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 15542.7464 - val_loss: 42189.0947\n",
      "Epoch 393/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 14705.1113 - val_loss: 38787.2683\n",
      "Epoch 394/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 14134.2575 - val_loss: 40374.2666\n",
      "Epoch 395/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17462.5428 - val_loss: 39805.8993\n",
      "Epoch 396/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 15576.3146 - val_loss: 39487.0349\n",
      "Epoch 397/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 13837.6611 - val_loss: 39335.2313\n",
      "Epoch 398/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 15341.9047 - val_loss: 40784.2808\n",
      "Epoch 399/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 15018.2918 - val_loss: 40199.1077\n",
      "Epoch 400/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14319.0199 - val_loss: 40958.8238\n",
      "Epoch 401/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14340.1669 - val_loss: 41162.9404\n",
      "Epoch 402/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 13210.1901 - val_loss: 40457.6597\n",
      "Epoch 403/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 13862.2007 - val_loss: 39547.8804\n",
      "Epoch 404/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13677.2392 - val_loss: 41091.0568\n",
      "Epoch 405/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13497.2910 - val_loss: 41295.4080\n",
      "Epoch 406/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16136.6917 - val_loss: 40396.4076\n",
      "Epoch 407/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13348.2018 - val_loss: 40516.7990\n",
      "Epoch 408/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13212.9638 - val_loss: 40736.4814\n",
      "Epoch 409/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13018.9031 - val_loss: 41725.0881\n",
      "Epoch 410/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 15133.7006 - val_loss: 41138.9313\n",
      "Epoch 411/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13922.3944 - val_loss: 40220.1880\n",
      "Epoch 412/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12791.4169 - val_loss: 40093.1999\n",
      "Epoch 413/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12926.0074 - val_loss: 40673.8010\n",
      "Epoch 414/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13763.5508 - val_loss: 41442.7875\n",
      "Epoch 415/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 14844.3035 - val_loss: 41575.0179\n",
      "Epoch 416/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 15052.7153 - val_loss: 39805.4062\n",
      "Epoch 417/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13690.6358 - val_loss: 41202.6159\n",
      "Epoch 418/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13240.6836 - val_loss: 39461.4615\n",
      "Epoch 419/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13353.3957 - val_loss: 41960.9988\n",
      "Epoch 420/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13432.8167 - val_loss: 39793.3657\n",
      "Epoch 421/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 14099.8454 - val_loss: 42768.1267\n",
      "Epoch 422/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 14344.7231 - val_loss: 41009.1168\n",
      "Epoch 423/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 13294.0246 - val_loss: 41272.8337\n",
      "Epoch 424/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12981.1915 - val_loss: 40183.4171\n",
      "Epoch 425/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12785.8488 - val_loss: 40887.3032\n",
      "Epoch 426/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 14137.0451 - val_loss: 41753.9664\n",
      "Epoch 427/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 158us/step - loss: 14523.6876 - val_loss: 43397.1966\n",
      "Epoch 428/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13716.2277 - val_loss: 41724.9843\n",
      "Epoch 429/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13037.9280 - val_loss: 40554.8354\n",
      "Epoch 430/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13784.9459 - val_loss: 40448.4096\n",
      "Epoch 431/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13848.4145 - val_loss: 42653.8635\n",
      "Epoch 432/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13699.0293 - val_loss: 40299.0116\n",
      "Epoch 433/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12071.9218 - val_loss: 40882.2461\n",
      "Epoch 434/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13329.4149 - val_loss: 40500.4345\n",
      "Epoch 435/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13399.1613 - val_loss: 41515.7128\n",
      "Epoch 436/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12139.7064 - val_loss: 39964.3952\n",
      "Epoch 437/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12492.2845 - val_loss: 40996.6708\n",
      "Epoch 438/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 13216.6302 - val_loss: 40405.4676\n",
      "Epoch 439/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12384.8943 - val_loss: 41735.8005\n",
      "Epoch 440/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12183.7744 - val_loss: 40200.0452\n",
      "Epoch 441/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13289.3649 - val_loss: 41797.8221\n",
      "Epoch 442/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12110.8267 - val_loss: 41346.9310\n",
      "Epoch 443/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 13496.3509 - val_loss: 41270.8564\n",
      "Epoch 444/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12977.6221 - val_loss: 42602.7133\n",
      "Epoch 445/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 13413.1574 - val_loss: 41457.1247\n",
      "Epoch 446/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13786.4421 - val_loss: 40575.0891\n",
      "Epoch 447/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 12596.0192 - val_loss: 41243.4157\n",
      "Epoch 448/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12690.7170 - val_loss: 41137.9997\n",
      "Epoch 449/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12243.9426 - val_loss: 40923.2136\n",
      "Epoch 450/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11777.9568 - val_loss: 41085.5662\n",
      "Epoch 451/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12157.9833 - val_loss: 41052.5875\n",
      "Epoch 452/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12615.7354 - val_loss: 41664.9140\n",
      "Epoch 453/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11981.8073 - val_loss: 41212.2406\n",
      "Epoch 454/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11677.4469 - val_loss: 41101.9188\n",
      "Epoch 455/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12259.8033 - val_loss: 41345.0915\n",
      "Epoch 456/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11849.1602 - val_loss: 40411.4397\n",
      "Epoch 457/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14455.9818 - val_loss: 41332.6718\n",
      "Epoch 458/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12712.6012 - val_loss: 40931.2631\n",
      "Epoch 459/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13065.7323 - val_loss: 45985.9686\n",
      "Epoch 460/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14444.2488 - val_loss: 39517.3395\n",
      "Epoch 461/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12357.7335 - val_loss: 41919.5432\n",
      "Epoch 462/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11412.1529 - val_loss: 40308.6211\n",
      "Epoch 463/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11422.9647 - val_loss: 41414.0864\n",
      "Epoch 464/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12482.5546 - val_loss: 40244.0855\n",
      "Epoch 465/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13277.9149 - val_loss: 41419.1516\n",
      "Epoch 466/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12674.3513 - val_loss: 40451.3993\n",
      "Epoch 467/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11708.2323 - val_loss: 41640.4858\n",
      "Epoch 468/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12769.9747 - val_loss: 40133.0025\n",
      "Epoch 469/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11936.4459 - val_loss: 40729.1540\n",
      "Epoch 470/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12336.1627 - val_loss: 41100.9954\n",
      "Epoch 471/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 11579.1097 - val_loss: 41493.7196\n",
      "Epoch 472/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11258.9750 - val_loss: 40930.2070\n",
      "Epoch 473/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11994.1730 - val_loss: 41697.8211\n",
      "Epoch 474/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 11835.7246 - val_loss: 41206.8512\n",
      "Epoch 475/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11982.2333 - val_loss: 41689.4320\n",
      "Epoch 476/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12396.6449 - val_loss: 40007.4434\n",
      "Epoch 477/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 11323.8524 - val_loss: 42641.9654\n",
      "Epoch 478/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12158.9246 - val_loss: 40723.4179\n",
      "Epoch 479/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11218.3632 - val_loss: 41915.6149\n",
      "Epoch 480/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12726.1265 - val_loss: 39920.1422\n",
      "Epoch 481/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 10879.9961 - val_loss: 41786.0981\n",
      "Epoch 482/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 12085.6461 - val_loss: 40185.9988\n",
      "Epoch 483/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11708.8747 - val_loss: 40460.8083\n",
      "Epoch 484/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 10982.3738 - val_loss: 42510.6145\n",
      "Epoch 485/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 13200.6554 - val_loss: 39975.2106\n",
      "Epoch 486/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 12919.2656 - val_loss: 40477.4426\n",
      "Epoch 487/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 12355.5672 - val_loss: 40031.9141\n",
      "Epoch 488/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11300.3811 - val_loss: 42109.0684\n",
      "Epoch 489/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 10973.9273 - val_loss: 41111.4598\n",
      "Epoch 490/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 11971.8062 - val_loss: 41790.1564\n",
      "Epoch 491/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 11364.2863 - val_loss: 41920.8279\n",
      "Epoch 492/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 10829.0715 - val_loss: 40846.4341\n",
      "Epoch 493/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13952.9954 - val_loss: 41342.0292\n",
      "Epoch 494/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11444.7508 - val_loss: 40826.4385\n",
      "Epoch 495/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11279.7510 - val_loss: 42676.8892\n",
      "Epoch 496/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 10620.3781 - val_loss: 40238.4325\n",
      "Epoch 497/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 10827.0776 - val_loss: 41711.3166\n",
      "Epoch 498/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 12505.8379 - val_loss: 42329.2046\n",
      "Epoch 499/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11581.0677 - val_loss: 41041.9296\n",
      "Epoch 500/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11132.4347 - val_loss: 42397.6687\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "newAdam = Adam(learning_rate=0.0005)\n",
    "NN_500E_NewAdam.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = NN_500E_NewAdam.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_500E_NewAdam.save('NN_5000E_NewAdam_V2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               169472    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 695,297\n",
      "Trainable params: 695,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_Sig = Sequential()\n",
    "NN_5000E_Adam_Sig.add(Dense(512,input_dim = 330,activation = 'sigmoid'))\n",
    "NN_5000E_Adam_Sig.add(Dense(512,activation = 'sigmoid'))\n",
    "NN_5000E_Adam_Sig.add(Dense(512,activation = 'sigmoid'))\n",
    "NN_5000E_Adam_Sig.add(Dense(1))\n",
    "NN_5000E_Adam_Sig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 364us/step - loss: 37765465534.6290 - val_loss: 38856311962.3014\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37760839083.3248 - val_loss: 38851632927.5616\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37756176252.3805 - val_loss: 38846826748.4931\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37751193261.2991 - val_loss: 38841367622.1370\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37745548971.5441 - val_loss: 38835620204.7123\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37739652777.7892 - val_loss: 38829458361.8630\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37733663901.0660 - val_loss: 38823570894.9041\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37727911614.8483 - val_loss: 38817877006.0274\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37722319242.8586 - val_loss: 38812273734.1370\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37716798786.0291 - val_loss: 38806748174.0274\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37711336034.7147 - val_loss: 38801228168.7671\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37705886606.8072 - val_loss: 38795753387.8356\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37700468985.1997 - val_loss: 38790304978.4110\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37695077452.3393 - val_loss: 38784852697.4247\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37689687407.6572 - val_loss: 38779443452.4931\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37684336289.0146 - val_loss: 38773981127.8904\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 37678964479.7806 - val_loss: 38768613291.8356\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37673629230.9443 - val_loss: 38763233897.2055\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37668293715.3590 - val_loss: 38757846759.4521\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37662964371.8526 - val_loss: 38752457489.5342\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37657637643.1877 - val_loss: 38747105195.8356\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37652331194.4610 - val_loss: 38741736013.1507\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37647024177.1380 - val_loss: 38736370926.4658\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37641713795.6195 - val_loss: 38731015658.9589\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37636415628.3942 - val_loss: 38725684518.5753\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 37631123020.7781 - val_loss: 38720334525.3699\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 37625830946.6598 - val_loss: 38714992836.3836\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37620546639.8492 - val_loss: 38709638915.5069\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37615245229.5184 - val_loss: 38704294140.4931\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37609942443.3248 - val_loss: 38698978486.3562\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37604684702.6015 - val_loss: 38693630008.1096\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37599393523.4961 - val_loss: 38688331186.8493\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37594135414.2382 - val_loss: 38682998924.2740\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37588857109.2785 - val_loss: 38677657515.8356\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 37583581963.1877 - val_loss: 38672350278.1370\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37578291987.9623 - val_loss: 38667061724.9315\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 37573038374.8278 - val_loss: 38661679749.2603\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37567736295.8698 - val_loss: 38656375036.4931\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37562480097.7275 - val_loss: 38651053715.2877\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37557219478.9237 - val_loss: 38645727063.6712\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37551939696.3153 - val_loss: 38640386833.5342\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37546659359.1500 - val_loss: 38635104620.7123\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37541417953.2888 - val_loss: 38629788517.6986\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37536160465.2751 - val_loss: 38624502489.4247\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37530913073.3573 - val_loss: 38619158107.1781\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37525648042.6667 - val_loss: 38613881112.5479\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37520407607.2802 - val_loss: 38608596430.9041\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37515154127.5201 - val_loss: 38603312198.1370\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37509925992.4182 - val_loss: 38597969891.9452\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37504646111.5338 - val_loss: 38592705465.8630\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37499414807.0334 - val_loss: 38587396600.9863\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37494167399.3213 - val_loss: 38582122131.2877\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37488929294.4781 - val_loss: 38576804625.5342\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37483697800.4456 - val_loss: 38571488298.0822\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37478438696.1440 - val_loss: 38566246876.9315\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37473214530.6872 - val_loss: 38560950580.6027\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37467961073.7412 - val_loss: 38555669994.9589\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37462729218.1937 - val_loss: 38550376167.4521\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37457483360.0823 - val_loss: 38545103549.3699\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37452250307.6744 - val_loss: 38539755295.5616\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 37446980712.4182 - val_loss: 38534491767.2329\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37441751536.6444 - val_loss: 38529196368.6575\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 37436511606.6769 - val_loss: 38523888682.0822\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37431263393.4533 - val_loss: 38518638339.5069\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37426032510.1354 - val_loss: 38513364374.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37420806614.3205 - val_loss: 38508010622.2466\n",
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37415536322.3582 - val_loss: 38502743671.2329\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 37410309570.1388 - val_loss: 38497450797.5890\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37405076672.1645 - val_loss: 38492161066.0822\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37399840706.5776 - val_loss: 38486900736.0000\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37394613480.5278 - val_loss: 38481583735.2329\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37389368189.2579 - val_loss: 38476314427.6164\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37384131820.0377 - val_loss: 38471026547.7260\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37378893752.0480 - val_loss: 38465764871.0137\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 37373654076.5450 - val_loss: 38460444840.3288\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37368407909.5664 - val_loss: 38455184285.8082\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37363190939.3111 - val_loss: 38449917334.7945\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37357977947.4756 - val_loss: 38444616044.7123\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 232us/step - loss: 37352746407.8149 - val_loss: 38439353806.9041\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37347513144.8158 - val_loss: 38434082198.7945\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37342292452.3599 - val_loss: 38428770079.5616\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37337063094.0737 - val_loss: 38423503857.9726\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37331837698.4130 - val_loss: 38418243303.4521\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37326635123.8252 - val_loss: 38412945828.8219\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37321405409.2888 - val_loss: 38407698403.9452\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37316182065.1380 - val_loss: 38402445368.1096\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37310959214.1217 - val_loss: 38397139982.0274\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37305726830.3410 - val_loss: 38391896877.5890\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37300516069.0180 - val_loss: 38386602601.2055\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37295286014.0257 - val_loss: 38381376441.8630\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37290090327.5270 - val_loss: 38376056916.1644\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37284859151.1362 - val_loss: 38370802926.4658\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37279636545.8098 - val_loss: 38365580582.5753\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37274431644.1885 - val_loss: 38360293376.0000\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 261us/step - loss: 37269211490.4953 - val_loss: 38355026424.9863\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37263998726.8003 - val_loss: 38349771313.0959\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37258781661.7789 - val_loss: 38344476419.5069\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37253551715.5921 - val_loss: 38339209412.3836\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37248341363.1671 - val_loss: 38333921981.3699\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37243107001.5835 - val_loss: 38328704182.3562\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37237913622.8141 - val_loss: 38323423877.2603\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37232689752.1851 - val_loss: 38318160012.2740\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37227471353.4190 - val_loss: 38312884925.3699\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37222256734.7661 - val_loss: 38307633685.0411\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37217034966.5398 - val_loss: 38302391983.3425\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 37211833859.0711 - val_loss: 38297087214.4658\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37206617750.4850 - val_loss: 38291842819.5069\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37201403226.5981 - val_loss: 38286597021.8082\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37196193325.1894 - val_loss: 38281309366.3562\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37190956275.0574 - val_loss: 38276083150.9041\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 37185759844.4696 - val_loss: 38270775239.8904\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37180552915.9075 - val_loss: 38265488706.6301\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37175335580.6273 - val_loss: 38260260190.6849\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37170134476.6684 - val_loss: 38255014841.8630\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37164942464.9871 - val_loss: 38249735434.5205\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37159723299.3179 - val_loss: 38244519655.4521\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37154513106.5913 - val_loss: 38239258652.0548\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37149297175.2528 - val_loss: 38233980310.7945\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37144087101.8612 - val_loss: 38228714481.9726\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37138891680.3565 - val_loss: 38223466888.7671\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 266us/step - loss: 37133700274.1251 - val_loss: 38218225635.9452\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 279us/step - loss: 37128497918.9032 - val_loss: 38212981016.5479\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37123279497.3231 - val_loss: 38207658012.0548\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37118050898.9203 - val_loss: 38202443242.9589\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 37112870410.0908 - val_loss: 38197196996.3836\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37107671666.5090 - val_loss: 38191969883.1781\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37102483220.8398 - val_loss: 38186710170.3014\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37097282608.2605 - val_loss: 38181456629.4795\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37092088992.5758 - val_loss: 38176209316.8219\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37086897475.7841 - val_loss: 38170950782.2466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37081676500.7849 - val_loss: 38165743924.6027\n",
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37076492333.6281 - val_loss: 38160474673.0959\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37071288535.8560 - val_loss: 38155209068.7123\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 37066082467.2082 - val_loss: 38149985097.6438\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 37060892203.4344 - val_loss: 38144735708.9315\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37055698521.0626 - val_loss: 38139490808.9863\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37050510521.1448 - val_loss: 38134242766.9041\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37045310631.5955 - val_loss: 38129019974.1370\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37040122110.4644 - val_loss: 38123787867.1781\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37034932345.0900 - val_loss: 38118537075.7260\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37029733970.0428 - val_loss: 38113249364.1644\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37024533102.9991 - val_loss: 38108021128.7671\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37019349769.4327 - val_loss: 38102786328.5479\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37014152413.9983 - val_loss: 38097569483.3973\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37008977656.7609 - val_loss: 38092310219.3973\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37003781055.9452 - val_loss: 38087067844.3836\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36998602095.6572 - val_loss: 38081815201.3151\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36993409782.1285 - val_loss: 38076591959.6712\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36988220771.3727 - val_loss: 38071369615.7808\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36983046335.2871 - val_loss: 38066106985.2055\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36977850176.7129 - val_loss: 38060896704.8767\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36972665072.4250 - val_loss: 38055629753.8630\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36967467817.0214 - val_loss: 38050398600.7671\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36962292052.4559 - val_loss: 38045169860.3836\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 36957100470.7318 - val_loss: 38039933601.3151\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36951917293.3539 - val_loss: 38034705983.1233\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36946739851.0780 - val_loss: 38029463608.1096\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36941544127.7258 - val_loss: 38024268196.8219\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36936383102.7935 - val_loss: 38019013982.6849\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36931187955.0574 - val_loss: 38013806171.1781\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36926004456.5278 - val_loss: 38008553584.2192\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36920812358.8552 - val_loss: 38003322318.9041\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36915638272.0000 - val_loss: 37998049700.8219\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36910428823.3625 - val_loss: 37992847388.0548\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36905246484.8398 - val_loss: 37987606415.7808\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36900053135.9040 - val_loss: 37982389290.0822\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36894877671.4310 - val_loss: 37977123012.3836\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36889692790.0189 - val_loss: 37971896291.9452\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36884513408.5484 - val_loss: 37966680120.1096\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36879343073.7275 - val_loss: 37961446666.5205\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36874157518.4233 - val_loss: 37956237059.5069\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36868993311.8081 - val_loss: 37950990307.9452\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36863806082.3033 - val_loss: 37945746810.7397\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36858624101.7858 - val_loss: 37940518294.7945\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36853450601.0763 - val_loss: 37935292808.7671\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36848265182.6564 - val_loss: 37930089766.5753\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36843104766.6838 - val_loss: 37924861755.6164\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36837918342.6907 - val_loss: 37919636494.0274\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36832744552.4182 - val_loss: 37914377005.5890\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36827554623.8355 - val_loss: 37909184455.8904\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36822380243.9075 - val_loss: 37903976700.4931\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36817219738.4336 - val_loss: 37898697405.3699\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36812023181.4910 - val_loss: 37893487125.0411\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36806838556.2982 - val_loss: 37888244020.6027\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36801664183.3899 - val_loss: 37883020610.6301\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36796487246.5330 - val_loss: 37877820317.8082\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36791324728.1577 - val_loss: 37872539002.7397\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36786132021.5253 - val_loss: 37867385224.7671\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36780971103.6435 - val_loss: 37862153959.4521\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36775797185.2614 - val_loss: 37856912257.7534\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36770605436.8192 - val_loss: 37851695580.9315\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36765436755.1397 - val_loss: 37846427283.2877\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36760254213.0454 - val_loss: 37841238380.7123\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36755090378.4747 - val_loss: 37836006947.0685\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36749910679.3625 - val_loss: 37830795544.5479\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36744732220.9837 - val_loss: 37825564503.6712\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36739562358.2382 - val_loss: 37820348331.8356\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36734393114.9820 - val_loss: 37815105507.9452\n",
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36729206982.3068 - val_loss: 37809914360.9863\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36724067048.0891 - val_loss: 37804649654.3562\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36718885762.5227 - val_loss: 37799498289.0959\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36713746523.2562 - val_loss: 37794242952.7671\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36708551731.7703 - val_loss: 37789052030.2466\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36703393546.3102 - val_loss: 37783822392.1096\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36698241883.9143 - val_loss: 37778583383.6712\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 36693065987.7292 - val_loss: 37773407947.3973\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36687915295.8081 - val_loss: 37768155809.3151\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36682738381.7652 - val_loss: 37762989687.2329\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36677585582.1765 - val_loss: 37757770036.6027\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36672410975.8629 - val_loss: 37752545392.2192\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36667245460.0720 - val_loss: 37747282312.7671\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36662063354.9546 - val_loss: 37742092231.8904\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 36656903969.1243 - val_loss: 37736917749.4795\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36651756141.2442 - val_loss: 37731698996.6027\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36646592547.0985 - val_loss: 37726504707.5069\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36641443396.0034 - val_loss: 37721255262.6849\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 36636271746.7421 - val_loss: 37716065013.4795\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36631113720.9803 - val_loss: 37710830830.4658\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36625949259.9006 - val_loss: 37705623411.7260\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 36620790837.5253 - val_loss: 37700415936.8767\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36615629305.4190 - val_loss: 37695241847.2329\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36610491094.5398 - val_loss: 37690007383.6712\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36605325087.3693 - val_loss: 37684822352.6575\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36600165927.9246 - val_loss: 37679626885.2603\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36595020141.0248 - val_loss: 37674372671.1233\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36589855451.8046 - val_loss: 37669180345.8630\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36584700900.7986 - val_loss: 37664016524.2740\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36579564657.1928 - val_loss: 37658818026.9589\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36574417766.4439 - val_loss: 37653606400.0000\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36569262629.2922 - val_loss: 37648405882.7397\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36564100437.3333 - val_loss: 37643202952.7671\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36558933061.3196 - val_loss: 37638020166.1370\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36553779938.8243 - val_loss: 37632784355.9452\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36548625498.3787 - val_loss: 37627556569.4247\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36543486457.4190 - val_loss: 37622326650.7397\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36538324284.7644 - val_loss: 37617155759.3425\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36533164733.9709 - val_loss: 37611957767.0137\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36528019554.2759 - val_loss: 37606744961.7534\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36522864875.1602 - val_loss: 37601571096.5479\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36517725995.6538 - val_loss: 37596363790.0274\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36512575803.8869 - val_loss: 37591126857.6438\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36507421358.1765 - val_loss: 37585962026.0822\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36502278017.6452 - val_loss: 37580745237.0411\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36497121678.3685 - val_loss: 37575553865.6438\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36491978123.7361 - val_loss: 37570356939.3973\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 263us/step - loss: 36486824550.2245 - val_loss: 37565189021.8082\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36481680878.0120 - val_loss: 37559959327.5616\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36476527276.4216 - val_loss: 37554780973.5890\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36471390760.8021 - val_loss: 37549559695.7808\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36466234447.8492 - val_loss: 37544397894.1370\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36461102354.6461 - val_loss: 37539197208.5479\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36455948382.7661 - val_loss: 37533989397.0411\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36450784237.5733 - val_loss: 37528814577.9726\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36445656654.5330 - val_loss: 37523602277.6986\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36440516439.5270 - val_loss: 37518405856.4384\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36435368501.0865 - val_loss: 37513224248.1096\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36430228265.0214 - val_loss: 37508043088.6575\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36425086474.0908 - val_loss: 37502852615.0137\n",
      "Epoch 259/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 247us/step - loss: 36419954521.2819 - val_loss: 37497654173.8082\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36414807691.9554 - val_loss: 37492467964.4931\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36409664979.6881 - val_loss: 37487280240.2192\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36404526596.8260 - val_loss: 37482079274.0822\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36399381996.2571 - val_loss: 37476889922.6301\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36394242837.7172 - val_loss: 37471721556.1644\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36389099426.9889 - val_loss: 37466531755.8356\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36383964840.0343 - val_loss: 37461280178.8493\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36378782161.0557 - val_loss: 37456092005.6986\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36373650673.3025 - val_loss: 37450890815.1233\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36368516725.1414 - val_loss: 37445692093.3699\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36363371208.5004 - val_loss: 37440529618.4110\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36358231825.3299 - val_loss: 37435357997.5890\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36353113188.9083 - val_loss: 37430156582.5753\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36347975021.9023 - val_loss: 37424993883.1781\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36342831395.3179 - val_loss: 37419799032.9863\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36337682840.8980 - val_loss: 37414648228.8219\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 36332569081.4190 - val_loss: 37409404619.3973\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36327413933.7378 - val_loss: 37404236926.2466\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36322267120.2057 - val_loss: 37399056215.6712\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 36317144593.1105 - val_loss: 37393850255.7808\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36312007049.1037 - val_loss: 37388702537.6438\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36306888967.2391 - val_loss: 37383502300.9315\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 36301742188.8055 - val_loss: 37378355256.1096\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36296630752.8500 - val_loss: 37373142001.9726\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36291493768.6650 - val_loss: 37367950350.0274\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36286346165.4156 - val_loss: 37362801958.5753\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36281229922.7147 - val_loss: 37357618105.8630\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36276085910.0463 - val_loss: 37352403505.0959\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36270953609.7618 - val_loss: 37347220718.4658\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36265826140.7918 - val_loss: 37342053923.0685\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36260716063.1500 - val_loss: 37336876579.0685\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36255573484.2571 - val_loss: 37331739409.5342\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36250453226.2828 - val_loss: 37326506965.9178\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36245300365.2716 - val_loss: 37321396167.8904\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 36240192421.6213 - val_loss: 37316161760.4384\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36235063878.6358 - val_loss: 37310972128.4384\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36229932043.4070 - val_loss: 37305803930.3014\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36224804581.4567 - val_loss: 37300599317.0411\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36219663954.0428 - val_loss: 37295469890.6301\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36214559337.7344 - val_loss: 37290272739.9452\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36209424078.6427 - val_loss: 37285109535.5616\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36204313581.5733 - val_loss: 37279950932.1644\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36199197549.4636 - val_loss: 37274787615.5616\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36194080402.9752 - val_loss: 37269625308.9315\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36188970758.8003 - val_loss: 37264434105.8630\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36183820054.5947 - val_loss: 37259303052.2740\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36178706607.4927 - val_loss: 37254086824.3288\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36173576018.2622 - val_loss: 37248916325.6986\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36168465096.5004 - val_loss: 37243720633.8630\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36163333740.3668 - val_loss: 37238573140.1644\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36158227646.4096 - val_loss: 37233417454.4658\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36153112465.4396 - val_loss: 37228241513.2055\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36147989039.8218 - val_loss: 37223084032.0000\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36142873855.3419 - val_loss: 37217873976.1096\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36137743181.8749 - val_loss: 37212752966.1370\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36132629762.8518 - val_loss: 37207592679.4521\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36127516654.4507 - val_loss: 37202380323.0685\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36122391075.5373 - val_loss: 37197232604.9315\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36117283753.1311 - val_loss: 37192073103.7808\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36112177306.4336 - val_loss: 37186916295.8904\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36107069568.1097 - val_loss: 37181765435.6164\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36101961087.4516 - val_loss: 37176594488.1096\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36096839739.6675 - val_loss: 37171463378.4110\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36091744656.1234 - val_loss: 37166295685.2603\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36086628704.7404 - val_loss: 37161114750.2466\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36081514371.4002 - val_loss: 37155955641.8630\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36076391449.4464 - val_loss: 37150796589.5890\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36071294519.7189 - val_loss: 37145628391.4521\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36066187707.9966 - val_loss: 37140466758.1370\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36061061009.4396 - val_loss: 37135335480.1096\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36055966892.8603 - val_loss: 37130164083.7260\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36050849756.9015 - val_loss: 37125011147.3973\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36045751152.9734 - val_loss: 37119852039.0137\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36040645327.5201 - val_loss: 37114682045.3699\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36035530141.2853 - val_loss: 37109530680.1096\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 36030408038.8826 - val_loss: 37104350923.3973\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36025288042.3925 - val_loss: 37099204103.0137\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36020182773.2511 - val_loss: 37094054140.4931\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36015077832.2811 - val_loss: 37088886840.1096\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36009970546.7284 - val_loss: 37083726609.5342\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36004866179.6195 - val_loss: 37078559589.6986\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35999757114.5707 - val_loss: 37073407719.4521\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 35994657141.7995 - val_loss: 37068249340.4931\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35989544349.2853 - val_loss: 37063137420.2740\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35984452171.9006 - val_loss: 37057950481.5342\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 35979335090.3445 - val_loss: 37052804166.1370\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35974242051.2905 - val_loss: 37047664696.1096\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35969122398.7661 - val_loss: 37042488025.4247\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35964023993.1448 - val_loss: 37037292277.4795\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 35958906844.9015 - val_loss: 37032160494.4658\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 35953812694.9786 - val_loss: 37027038362.3014\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35948723669.4430 - val_loss: 37021897208.9863\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35943625869.2716 - val_loss: 37016731311.3425\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35938517441.2614 - val_loss: 37011590943.5616\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35933419521.7549 - val_loss: 37006443225.4247\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35928322862.2862 - val_loss: 37001298424.9863\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35923230899.0026 - val_loss: 36996116592.2192\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 35918111632.5621 - val_loss: 36990960008.7671\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35912997932.7506 - val_loss: 36985822502.5753\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35907907494.4987 - val_loss: 36980661149.8082\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35902813186.6324 - val_loss: 36975506361.8630\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35897707810.4404 - val_loss: 36970397808.2192\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35892624390.1422 - val_loss: 36965209578.9589\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35887507780.6615 - val_loss: 36960097097.6438\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35882401549.8200 - val_loss: 36954953531.6164\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35877304241.0283 - val_loss: 36949805589.0411\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35872211302.0051 - val_loss: 36944665614.0274\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35867123887.4927 - val_loss: 36939525863.4521\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35862041369.2271 - val_loss: 36934352671.5616\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35856934506.6118 - val_loss: 36929213145.4247\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35851843320.7609 - val_loss: 36924054766.4658\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35846749421.7926 - val_loss: 36918922534.5753\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35841644092.9837 - val_loss: 36913796194.1918\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35836550499.3727 - val_loss: 36908629623.2329\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35831462735.6298 - val_loss: 36903483700.6027\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 35826379034.5433 - val_loss: 36898340527.3425\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35821295631.7943 - val_loss: 36893202852.8219\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35816205391.8492 - val_loss: 36888106587.1781\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35811131095.4173 - val_loss: 36882930870.3562\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35806033077.6350 - val_loss: 36877802958.9041\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 35800944176.6992 - val_loss: 36872693283.0685\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35795864071.4584 - val_loss: 36867565203.2877\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35790777920.4936 - val_loss: 36862427977.6438\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35785704459.4070 - val_loss: 36857280652.2740\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35780618383.9040 - val_loss: 36852153245.8082\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35775528967.0197 - val_loss: 36847042784.4384\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35770445692.3805 - val_loss: 36841861176.1096\n",
      "Epoch 387/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 251us/step - loss: 35765347483.3111 - val_loss: 36836729617.5342\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35760273351.8423 - val_loss: 36831582572.7123\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35755194403.0985 - val_loss: 36826471101.3699\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35750124121.9400 - val_loss: 36821362547.7260\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35745035900.1611 - val_loss: 36816217578.9589\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 35739942648.7609 - val_loss: 36811075527.8904\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35734834733.1894 - val_loss: 36805947392.0000\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35729741021.9983 - val_loss: 36800814711.2329\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35724663213.9572 - val_loss: 36795629904.6575\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 35719579098.7078 - val_loss: 36790486058.0822\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35714489130.7764 - val_loss: 36785385023.1233\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35709415539.8252 - val_loss: 36780254811.1781\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35704327802.4062 - val_loss: 36775135260.0548\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35699258725.1277 - val_loss: 36769954100.6027\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 35694158269.7515 - val_loss: 36764865185.3151\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35689083655.6778 - val_loss: 36759717467.1781\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35683996683.4070 - val_loss: 36754556226.6301\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35678904488.4730 - val_loss: 36749473258.9589\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35673836708.0857 - val_loss: 36744308595.7260\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 233us/step - loss: 35668745008.0411 - val_loss: 36739190503.4521\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35663668275.7703 - val_loss: 36734087672.9863\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 35658611556.6889 - val_loss: 36728949942.3562\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 35653520755.1671 - val_loss: 36723832691.7260\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35648457407.7258 - val_loss: 36718686881.3151\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 35643369354.4199 - val_loss: 36713566881.3151\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35638291914.9135 - val_loss: 36708435547.1781\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 35633221717.9914 - val_loss: 36703280254.2466\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35628150201.3642 - val_loss: 36698189094.5753\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35623083852.9974 - val_loss: 36693123801.4247\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35618030750.8209 - val_loss: 36687977822.6849\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35612937648.5895 - val_loss: 36682865678.0274\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35607880897.9194 - val_loss: 36677730696.7671\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35602806747.1465 - val_loss: 36672601887.5616\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35597713997.6555 - val_loss: 36667485141.9178\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35592657273.3093 - val_loss: 36662336638.2466\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35587587899.4482 - val_loss: 36657225166.9041\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35582514327.8012 - val_loss: 36652158471.0137\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35577454069.0317 - val_loss: 36647006937.4247\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35572397202.5364 - val_loss: 36641883570.8493\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35567322527.0403 - val_loss: 36636799817.6438\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35562259679.7532 - val_loss: 36631674655.5616\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35557192122.2416 - val_loss: 36626551906.1918\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35552137461.6898 - val_loss: 36621415353.8630\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35547059857.2202 - val_loss: 36616313196.7123\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35542006239.0951 - val_loss: 36611180010.9589\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35536926546.2622 - val_loss: 36606071906.1918\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35531850363.2836 - val_loss: 36601003022.0274\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35526801437.8338 - val_loss: 36595833589.4795\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35521726370.9889 - val_loss: 36590710952.3288\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35516652451.8663 - val_loss: 36585583938.6301\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35511587179.2699 - val_loss: 36580473308.9315\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35506514967.6915 - val_loss: 36575421874.8493\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35501458917.2374 - val_loss: 36570268209.0959\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35496384141.7104 - val_loss: 36565162909.8082\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35491323591.6230 - val_loss: 36560048184.1096\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35486271693.3265 - val_loss: 36554930933.4795\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35481207020.9152 - val_loss: 36549830066.8493\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35476159750.3616 - val_loss: 36544724038.1370\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35471099326.6290 - val_loss: 36539629343.5616\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35466039962.8723 - val_loss: 36534491164.0548\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35460967370.4747 - val_loss: 36529409935.7808\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35455926868.6752 - val_loss: 36524283819.8356\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35450855652.1405 - val_loss: 36519173919.5616\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35445799019.0506 - val_loss: 36514044829.8082\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 35440737542.3616 - val_loss: 36508967473.0959\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35435686112.6307 - val_loss: 36503862342.1370\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35430633749.2785 - val_loss: 36498744362.0822\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 35425568773.2648 - val_loss: 36493643776.0000\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 35420499499.4344 - val_loss: 36488551606.3562\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35415457114.5981 - val_loss: 36483418196.1644\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35410388198.7729 - val_loss: 36478330403.0685\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35405338076.4627 - val_loss: 36473246369.3151\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35400300394.8312 - val_loss: 36468112622.4658\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 35395242656.1371 - val_loss: 36463035882.9589\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35390190262.9512 - val_loss: 36457930471.4521\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35385126452.2091 - val_loss: 36452814848.0000\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35380059610.7078 - val_loss: 36447673021.3699\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35375006833.1928 - val_loss: 36442592690.8493\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 35369961266.6735 - val_loss: 36437487784.3288\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 35364904931.9212 - val_loss: 36432412223.1233\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35359863742.1902 - val_loss: 36427293401.4247\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35354813912.9529 - val_loss: 36422182098.4110\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35349770341.7858 - val_loss: 36417079885.1507\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35344718038.1011 - val_loss: 36412013357.5890\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35339678250.5570 - val_loss: 36406943126.7945\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35334641802.6392 - val_loss: 36401811736.5479\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35329583965.6692 - val_loss: 36396711150.4658\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35324544636.1611 - val_loss: 36391580265.2055\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35319490112.4936 - val_loss: 36386519516.9315\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35314464320.4936 - val_loss: 36381420501.9178\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35309412906.5570 - val_loss: 36376320084.1644\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 35304359018.1731 - val_loss: 36371239304.7671\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35299298892.7781 - val_loss: 36366150108.9315\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35294251901.2579 - val_loss: 36361055414.3562\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35289203860.2913 - val_loss: 36355958419.2877\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35284155377.0831 - val_loss: 36350857384.3288\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35279121379.0437 - val_loss: 36345714603.8356\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 35274053514.4199 - val_loss: 36340652733.3699\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35268997516.6135 - val_loss: 36335577396.6027\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35263954041.9674 - val_loss: 36330432371.7260\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35258905032.2811 - val_loss: 36325316074.9589\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35253852137.1859 - val_loss: 36320239111.0137\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35248827009.4259 - val_loss: 36315141218.1918\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35243785880.2399 - val_loss: 36310123057.0959\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35238769633.2888 - val_loss: 36305036835.0685\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35233713653.0317 - val_loss: 36299925027.0685\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35228673009.0831 - val_loss: 36294811872.4384\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 35223626945.9194 - val_loss: 36289734010.7397\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35218598988.3393 - val_loss: 36284617657.8630\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35213552433.7961 - val_loss: 36279574752.4384\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35208533231.5476 - val_loss: 36274484378.3014\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35203491476.7301 - val_loss: 36269369877.0411\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35198437047.8286 - val_loss: 36264309970.4110\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35193409967.7121 - val_loss: 36259218025.2055\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0005)\n",
    "NN_5000E_Adam_Sig.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = NN_5000E_Adam_Sig.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 512)               169472    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,220,609\n",
      "Trainable params: 1,220,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_4H = Sequential()\n",
    "NN_5000E_Adam_4H.add(Dense(512,input_dim = 330,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(1))\n",
    "NN_5000E_Adam_4H.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 1s 564us/step - loss: 36522277088.6307 - val_loss: 31151011924.1644\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 20591323523.4002 - val_loss: 10548069663.5616\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 14787735020.2571 - val_loss: 9263632727.6712\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 12293208674.7147 - val_loss: 7920128925.8082\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 9516187921.4396 - val_loss: 5965733312.8767\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 6733085789.2853 - val_loss: 4627476713.2055\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 4693404259.3728 - val_loss: 4056714520.5479\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 3240308142.8346 - val_loss: 3607858887.8904\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 2409489225.2956 - val_loss: 3407602991.3425\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 2126030431.5338 - val_loss: 3345038679.6712\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 1923661741.2991 - val_loss: 3390656392.7671\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1856447810.9066 - val_loss: 3235776711.8904\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1840494072.1028 - val_loss: 3213287765.9178\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1782692888.1302 - val_loss: 3306326983.8904\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1750170984.5004 - val_loss: 3425477902.0274\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 417us/step - loss: 1759172968.4730 - val_loss: 3297372075.8356\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1739661912.5141 - val_loss: 3182036495.7808\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1700264707.7292 - val_loss: 3237325718.7945\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1689910471.8423 - val_loss: 3233075962.7397\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1677819253.6898 - val_loss: 3279000328.7671\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1672347774.7935 - val_loss: 3259854074.7397\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1654582417.7686 - val_loss: 3256749399.6712\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1636965410.5501 - val_loss: 3173431262.6849\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1677840963.1260 - val_loss: 3256274823.0137\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1653054868.7301 - val_loss: 3350165290.0822\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 1s 455us/step - loss: 1613606679.4722 - val_loss: 3368744814.4658\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1626568677.2922 - val_loss: 3261663114.5205\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1598763332.2228 - val_loss: 3400512554.0822\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1600926150.6358 - val_loss: 3133992418.1918\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1589578693.8680 - val_loss: 3169283257.8630\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 1558678889.7344 - val_loss: 3343467656.7671\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 1577731098.2416 - val_loss: 3109704747.8356\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1572402602.5570 - val_loss: 3108685816.9863\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1556857881.7755 - val_loss: 3194043000.9863\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1531136180.0171 - val_loss: 3077415045.2603\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1559198172.5724 - val_loss: 3153962674.8493\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1611650858.1731 - val_loss: 3321784872.3288\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1597638399.6710 - val_loss: 3112523512.9863\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1533095930.1868 - val_loss: 3112745910.3562\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1507373505.8098 - val_loss: 3048931233.3151\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 1505514252.2296 - val_loss: 3165863196.0548\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1541081999.5750 - val_loss: 3124507476.1644\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1483151867.5030 - val_loss: 3368232248.1096\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1506677220.6341 - val_loss: 3230176112.2192\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1464773561.9949 - val_loss: 3109345416.7671\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1473269284.3599 - val_loss: 3029772579.0685\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1474278194.2896 - val_loss: 3350865646.4658\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1468301756.8740 - val_loss: 3040980655.3425\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1444248563.2219 - val_loss: 3273193668.3836\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 1441196837.0180 - val_loss: 3289186696.7671\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1465705852.3805 - val_loss: 3054998640.2192\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1445793033.4327 - val_loss: 3009686117.6986\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1448398886.1697 - val_loss: 3376718620.0548\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1442741500.4901 - val_loss: 3039573184.8767\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 1419594656.2468 - val_loss: 3103301109.4795\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1412442016.7952 - val_loss: 3072513343.1233\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1392019265.3710 - val_loss: 2961614932.1644\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1446132969.9263 - val_loss: 2986880175.3425\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1397039893.0591 - val_loss: 2973036999.8904\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 1436366527.5613 - val_loss: 3078874971.1781\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1374469245.4773 - val_loss: 2989103146.0822\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1401208453.1551 - val_loss: 3078489358.0274\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1404507123.8252 - val_loss: 2962517556.6027\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1373522788.4696 - val_loss: 3419958615.6712\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 1s 492us/step - loss: 1412739832.3222 - val_loss: 3066478935.6712\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1385106021.9777 - val_loss: 3052150110.6849\n",
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1385407327.5338 - val_loss: 3518912722.4110\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1387324834.6598 - val_loss: 3015115821.5890\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1360344916.7575 - val_loss: 3181765951.1233\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 414us/step - loss: 1356330097.4670 - val_loss: 2938336606.6849\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1418422828.9152 - val_loss: 3055152359.4521\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 408us/step - loss: 1389436894.1354 - val_loss: 3155283982.0274\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 1375963933.8338 - val_loss: 3211531123.7260\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1515055199.0951 - val_loss: 2916702348.2740\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1378849899.9554 - val_loss: 2923911280.2192\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 413us/step - loss: 1347275776.6581 - val_loss: 2924796110.9041\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 1422471533.0797 - val_loss: 2990133830.1370\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1365481410.7969 - val_loss: 2926225425.5342\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1343566834.6187 - val_loss: 3069888519.0137\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1397603578.7901 - val_loss: 2906680456.7671\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1361736407.0883 - val_loss: 3027433657.8630\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1297740856.8706 - val_loss: 2982782909.3699\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1337794199.0883 - val_loss: 2964935013.6986\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1344212505.8852 - val_loss: 2905368421.6986\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1337217342.8483 - val_loss: 3134342459.6164\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1304985361.0009 - val_loss: 3129265088.8767\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1320477071.2459 - val_loss: 2910153934.9041\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 1357981463.9657 - val_loss: 3150098537.2055\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1373008714.5844 - val_loss: 3004727278.4658\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1327203684.3599 - val_loss: 2886679432.7671\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1354050444.8329 - val_loss: 2916040125.3699\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1443282470.4987 - val_loss: 2858416924.0548\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1274284851.5510 - val_loss: 3335619359.5616\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1332225653.0865 - val_loss: 2850390545.5342\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1297488776.7198 - val_loss: 3218708767.5616\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1288654257.4670 - val_loss: 2883222969.8630\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1315380586.3925 - val_loss: 2911319446.7945\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1277727962.2691 - val_loss: 3091833638.5753\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1351574174.6015 - val_loss: 2860897763.9452\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1283803369.4053 - val_loss: 2917407512.5479\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1269597817.5287 - val_loss: 2836882137.4247\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1277439467.7635 - val_loss: 3185674250.5205\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 1328414660.8260 - val_loss: 2819263295.1233\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1267059175.5955 - val_loss: 3268857722.7397\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1286872024.7335 - val_loss: 2884893134.9041\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1269414652.7644 - val_loss: 2836079342.4658\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1277289148.8740 - val_loss: 3226905796.3836\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1337045110.6769 - val_loss: 2912508700.0548\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1257782190.2314 - val_loss: 2805235824.2192\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 411us/step - loss: 1253752236.4216 - val_loss: 3031019754.9589\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1265631103.5613 - val_loss: 2966189385.6438\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1254794863.3282 - val_loss: 3039772307.2877\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 1273395907.7841 - val_loss: 3015003164.0548\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1253965045.6350 - val_loss: 2833160090.3014\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1248249266.6598 - val_loss: 2793115721.6438\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1235316038.3068 - val_loss: 2814190970.7397\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1242431011.5921 - val_loss: 2917721915.6164\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1215784993.4533 - val_loss: 2791357678.4658\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1227567860.0583 - val_loss: 2799876092.4932\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1204748677.7035 - val_loss: 2772784054.3562\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1241430347.6812 - val_loss: 2829291463.8904\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1203923034.9820 - val_loss: 2980978354.8493\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 1227110923.5167 - val_loss: 2726641320.3288\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1224827167.3693 - val_loss: 2758921773.5890\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1216114306.7695 - val_loss: 2849170474.0822\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1208462259.2219 - val_loss: 2765550206.2466\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1178334835.6058 - val_loss: 3064930535.4521\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1242277517.9846 - val_loss: 2776714061.1507\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1168476084.4284 - val_loss: 2790439487.1233\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1180584411.1465 - val_loss: 2937083875.9452\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1196678562.0566 - val_loss: 2905820500.1644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1228621927.9246 - val_loss: 2713756279.2329\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 410us/step - loss: 1170014581.2511 - val_loss: 2879766373.6986\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1151639312.4524 - val_loss: 2739778556.4932\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1161729214.9032 - val_loss: 2823046936.5479\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1136940189.0660 - val_loss: 2735173772.2740\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1374524349.3402 - val_loss: 2740995797.9178\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1256290006.7592 - val_loss: 2676047721.2055\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1219013163.5441 - val_loss: 3056892068.8219\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1175807710.8209 - val_loss: 2801659167.5616\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1141791732.3188 - val_loss: 2757831231.1233\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 1127327780.1680 - val_loss: 2737458295.2329\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1209805676.1474 - val_loss: 3516409617.5342\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1162274225.1380 - val_loss: 2675946969.4247\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1152418114.4130 - val_loss: 2801360590.9041\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1134924492.1200 - val_loss: 2961354320.6575\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1133271640.2674 - val_loss: 2716638965.4795\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1167033444.5793 - val_loss: 2647884600.1096\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1125002850.0566 - val_loss: 2801871847.4521\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1130881244.7918 - val_loss: 2723554237.3699\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1104355664.7266 - val_loss: 2765979483.1781\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 1s 455us/step - loss: 1124271407.1637 - val_loss: 2821367613.3699\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 1114570797.5733 - val_loss: 3038661454.9041\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1147617141.0317 - val_loss: 2604635970.6301\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1111880157.6692 - val_loss: 3401820396.7123\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1170500358.4713 - val_loss: 2696339080.7671\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1086342238.9032 - val_loss: 2697793108.1644\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1108077620.6478 - val_loss: 2832108351.1233\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1104968992.8775 - val_loss: 2569963520.0000\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1103239603.8800 - val_loss: 2687750305.3151\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1120879070.4370 - val_loss: 2821650165.4795\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1129409480.7198 - val_loss: 2820961201.0959\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1197193376.5210 - val_loss: 2704260963.9452\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1163431229.3128 - val_loss: 2805474561.7534\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1068764773.4019 - val_loss: 2606075844.3836\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1049542469.6761 - val_loss: 2823917047.2329\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1102754081.4259 - val_loss: 2579630518.3562\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1079847800.1851 - val_loss: 2649085930.9589\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1125941684.7849 - val_loss: 2754391592.3288\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1065836692.9494 - val_loss: 2620755960.9863\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1030397758.6290 - val_loss: 2701324843.8356\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1029007056.8912 - val_loss: 2569067470.9041\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1057597332.0720 - val_loss: 2654585500.0548\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1112998012.1611 - val_loss: 2549296867.9452\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1046072091.4207 - val_loss: 2772417765.6986\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1060361687.9109 - val_loss: 2562131704.9863\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1083178665.1585 - val_loss: 2557425727.1233\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1038767162.1868 - val_loss: 2552461753.8630\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1058269719.6915 - val_loss: 2641547583.1233\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1017577372.6547 - val_loss: 2596697991.0137\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1033077963.7087 - val_loss: 2538548981.4795\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1006965434.2965 - val_loss: 2590699842.6301\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1018358084.7164 - val_loss: 2573801826.1918\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1003363456.4936 - val_loss: 2688863068.9315\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 412us/step - loss: 1014420952.3496 - val_loss: 2551687518.6849\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 1018725788.8466 - val_loss: 2528942500.8219\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1052294238.1080 - val_loss: 2584050958.0274\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1031049205.5253 - val_loss: 2567224334.0274\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1025497806.8620 - val_loss: 2614238016.8767\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1003398974.9580 - val_loss: 2518541908.1644\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1057644201.5150 - val_loss: 2615751445.0411\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 981876905.2408 - val_loss: 2585163630.4658\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 1s 451us/step - loss: 1028487203.5373 - val_loss: 2502089194.9589\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1019378153.5698 - val_loss: 2575126538.5205\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 1s 446us/step - loss: 1022293632.3839 - val_loss: 2593028273.0959\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 976082292.1817 - val_loss: 2573779561.2055\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1023460718.6701 - val_loss: 2541961191.4521\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 980955752.5278 - val_loss: 2521318952.3288\n",
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 965736293.1277 - val_loss: 2500116050.4110\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 970083816.7472 - val_loss: 2444630431.5616\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1067123853.9023 - val_loss: 2674072440.9863\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 983837678.5878 - val_loss: 2897060513.3151\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1115245671.2665 - val_loss: 2622692625.5342\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 975434417.5767 - val_loss: 2688465379.9452\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 983308236.1200 - val_loss: 2503049694.6849\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 951119468.0925 - val_loss: 2766748196.8219\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 997028387.5373 - val_loss: 2981307234.1918\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1028715966.8209 - val_loss: 2464263634.4110\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 958872795.0368 - val_loss: 2495568682.0822\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 985832051.2768 - val_loss: 2447847767.6712\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 957202617.5287 - val_loss: 2474049039.7808\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 977453249.3710 - val_loss: 2503193084.4932\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 924376434.8380 - val_loss: 2492283058.8493\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 945514387.5784 - val_loss: 2546149814.3562\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 955767238.1422 - val_loss: 2794607095.2329\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 1s 457us/step - loss: 931387452.5450 - val_loss: 2565829263.7808\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 922580138.2965 - val_loss: 2511601230.9041\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1012442514.9203 - val_loss: 2570016177.0959\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 920826590.2177 - val_loss: 2515948172.2740\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 910687443.0848 - val_loss: 2527533694.2466\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 946499936.6307 - val_loss: 2582226616.1096\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 944178932.6478 - val_loss: 2501055575.6712\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 948697753.0077 - val_loss: 2560802458.3014\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 900709444.2228 - val_loss: 2679332709.6986\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 911544116.8535 - val_loss: 2415079790.4658\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 908810597.2922 - val_loss: 2644379144.7671\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 906620429.2716 - val_loss: 2421240640.8767\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 894227365.2374 - val_loss: 2449338320.6575\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 865456966.4165 - val_loss: 2567168552.3288\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 918058290.1251 - val_loss: 2475005134.9041\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 893168682.9957 - val_loss: 2458523854.9041\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 959278870.4850 - val_loss: 2403692305.5342\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 890078937.2819 - val_loss: 2483831921.9726\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 898361193.2134 - val_loss: 2510960057.8630\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 1s 452us/step - loss: 970780942.3685 - val_loss: 2408832478.6849\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 943779464.6650 - val_loss: 2549861414.5753\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 984472130.1114 - val_loss: 2493290639.7808\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 994768016.8912 - val_loss: 2533802001.5342\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 878162922.3925 - val_loss: 2454324343.2329\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 877589596.6821 - val_loss: 2380361147.6164\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 911282183.6778 - val_loss: 2781867688.3288\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 961077939.0848 - val_loss: 2482427535.7808\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 901307280.8363 - val_loss: 2374933109.4795\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 839346405.7858 - val_loss: 2508189897.6438\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 846065842.2348 - val_loss: 2416079793.0959\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 876929966.2588 - val_loss: 2405825460.6027\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 881971499.5990 - val_loss: 2461471552.8767\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1026497828.0308 - val_loss: 2580763316.6027\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 940188732.6547 - val_loss: 2563567966.6849\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1033726922.0360 - val_loss: 2387850588.9315\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 871534964.1817 - val_loss: 2429442842.3014\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 909385648.9186 - val_loss: 2562697219.5068\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 866050949.8132 - val_loss: 2507730016.4384\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 840345883.2014 - val_loss: 2462088819.7260\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 887160160.9049 - val_loss: 2387740046.0274\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 885163780.9632 - val_loss: 2568999688.7671\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 881363563.9829 - val_loss: 2421478305.3151\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 831474608.9460 - val_loss: 2423309382.1370\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 832505091.6744 - val_loss: 2431839032.1096\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 846284255.7806 - val_loss: 2414436655.3425\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 821298057.5424 - val_loss: 2397715456.0000\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 876104139.5716 - val_loss: 2587999116.2740\n",
      "Epoch 263/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 426us/step - loss: 809903481.3093 - val_loss: 2481664671.5616\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 817842361.8578 - val_loss: 2401059868.0548\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 827466964.1542 - val_loss: 2538248945.9726\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 820705424.0686 - val_loss: 2347676878.9041\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 825997942.4850 - val_loss: 2434173759.1233\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 895748078.3959 - val_loss: 2428477632.8767\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 822585974.5124 - val_loss: 2452980557.1507\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 801043098.4884 - val_loss: 2391332821.9178\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 886114413.7652 - val_loss: 2459997108.6027\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 839023465.8440 - val_loss: 2453516280.9863\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 808831226.3513 - val_loss: 2446906611.7260\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 806534559.4516 - val_loss: 2489390183.4521\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 806042001.1654 - val_loss: 2429343568.6575\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 814260592.5347 - val_loss: 2389523440.2192\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 787634547.3865 - val_loss: 2407609998.0274\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 784523957.5527 - val_loss: 2423837594.3014\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 795087403.8732 - val_loss: 2404417618.4110\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 789063277.5733 - val_loss: 2452517842.4110\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 839851849.6247 - val_loss: 2455173042.8493\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 841785062.7181 - val_loss: 2529570693.2603\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 914266457.2819 - val_loss: 2575414896.2192\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 831771568.6992 - val_loss: 2453789255.8904\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 790015239.2391 - val_loss: 2360814688.4384\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 788207536.3702 - val_loss: 2400364689.5342\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 842039451.2014 - val_loss: 2380899142.1370\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 417us/step - loss: 803941542.1148 - val_loss: 2440020427.3973\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 769553763.8663 - val_loss: 2453919742.2466\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 800665857.0831 - val_loss: 2360788118.7945\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 866723271.1842 - val_loss: 2336134550.7945\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 786412193.4533 - val_loss: 2469033186.1918\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 774387014.5810 - val_loss: 2501145656.1096\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 759279214.9443 - val_loss: 2388313345.7534\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 768833315.2356 - val_loss: 2368530628.3836\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 417us/step - loss: 745396218.1868 - val_loss: 2483869504.8767\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 740539419.3659 - val_loss: 2339098997.4795\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 414us/step - loss: 770241684.8398 - val_loss: 2390654704.2192\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 747615910.5261 - val_loss: 2397503480.9863\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 793495796.1817 - val_loss: 2334234552.1096\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 801730959.5201 - val_loss: 2345727675.6164\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 747542744.1851 - val_loss: 2498170655.5616\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 1s 480us/step - loss: 790751367.3488 - val_loss: 2389159960.5479\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 743665199.4379 - val_loss: 2553739458.6301\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 762377482.5844 - val_loss: 2390248981.0411\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 750283890.0428 - val_loss: 2323665294.0274\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 787193796.1131 - val_loss: 2705346779.1781\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 815940878.4233 - val_loss: 2566856213.0411\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 415us/step - loss: 784603573.2237 - val_loss: 2365077779.2877\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 724795739.8320 - val_loss: 2409498073.4247\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 765429001.7069 - val_loss: 2467798670.0274\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 772722542.5056 - val_loss: 2446749320.7671\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 851150413.6555 - val_loss: 2352762944.8767\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 764232299.9143 - val_loss: 2384771352.5479\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 742819693.8201 - val_loss: 2428034414.4658\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 715300996.8260 - val_loss: 2491498972.9315\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 733766702.9991 - val_loss: 2359009779.7260\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 694370764.8329 - val_loss: 2477721403.6164\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 742151629.6281 - val_loss: 2688429918.6849\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 749009446.8826 - val_loss: 2313789971.2877\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 706460612.0034 - val_loss: 2383975294.2466\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 727507675.3111 - val_loss: 2582766311.4521\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 821800171.7087 - val_loss: 2506657118.6849\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 819613654.5947 - val_loss: 2310187758.4658\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 745200026.4336 - val_loss: 2436599951.7808\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 752391363.0163 - val_loss: 2345429190.1370\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 738840697.9400 - val_loss: 2316779960.1096\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 732201030.4713 - val_loss: 2504809985.7534\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 709335316.8398 - val_loss: 2438000248.9863\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 751711049.6247 - val_loss: 2343722653.8082\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 701395589.7035 - val_loss: 2370005360.2192\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 677976394.5844 - val_loss: 2420657094.1370\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 700541107.2219 - val_loss: 2480075974.1370\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 665851731.3316 - val_loss: 2451428702.6849\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 678998788.0308 - val_loss: 2371011647.1233\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 698614768.1508 - val_loss: 2317950434.1918\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 675414115.5647 - val_loss: 2542269987.0685\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 729897242.7626 - val_loss: 2656059246.4658\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 711171766.2382 - val_loss: 2323855822.9041\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 687465815.4447 - val_loss: 2380172303.7808\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 667565321.4602 - val_loss: 2360235961.8630\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 672494633.0763 - val_loss: 2280649303.6712\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 653599729.9606 - val_loss: 2542033481.6438\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 652296081.4944 - val_loss: 2357716036.3836\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 698488538.9820 - val_loss: 2469881154.6301\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 699070963.6058 - val_loss: 2367242375.0137\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 666605859.0985 - val_loss: 2313763520.8767\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 641228013.9023 - val_loss: 2376246275.5068\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 648157152.9871 - val_loss: 2290733813.4795\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 648029400.2399 - val_loss: 2482713542.1370\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 676783162.9546 - val_loss: 2253589304.1096\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 687731260.4353 - val_loss: 2551187357.8082\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 693525812.6478 - val_loss: 2263384802.1918\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 1s 457us/step - loss: 649249442.8243 - val_loss: 2436736422.5753\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 630722665.3505 - val_loss: 2339113026.6301\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 631198143.6984 - val_loss: 2449198979.5068\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 660726545.2202 - val_loss: 2276557955.5068\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 635180405.9914 - val_loss: 2411087189.9178\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 415us/step - loss: 646835009.1791 - val_loss: 2376843516.4932\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 635925468.1063 - val_loss: 2278746971.1781\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 730552981.5801 - val_loss: 2407490644.1644\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 660039041.1791 - val_loss: 2309187825.9726\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 624311635.0300 - val_loss: 2276247792.2192\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 651175797.7584 - val_loss: 2285672430.4658\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 625971637.5253 - val_loss: 2301243157.0411\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 651141655.4173 - val_loss: 2284701497.8630\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 610031888.0137 - val_loss: 2266322361.8630\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 614614797.6007 - val_loss: 2308266020.8219\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 634982278.9649 - val_loss: 2225863843.0685\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 607783560.9940 - val_loss: 2366102224.6575\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 608315344.6307 - val_loss: 2311168033.3151\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 637371135.3967 - val_loss: 2220983750.1370\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 704354636.6684 - val_loss: 2210336911.7808\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 662216000.7129 - val_loss: 2249227704.1096\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 671174157.4910 - val_loss: 2508978645.9178\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 681831503.8492 - val_loss: 2270667134.2466\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 801104037.2374 - val_loss: 2259342562.1918\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 600770757.0180 - val_loss: 2209300872.7671\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 606772524.9152 - val_loss: 2260240164.8219\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 598738966.7044 - val_loss: 2365470788.3836\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 581708820.7849 - val_loss: 2277087009.3151\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 589641918.3548 - val_loss: 2193358307.9452\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 594558691.2631 - val_loss: 2206415970.1918\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 656423462.5536 - val_loss: 2602849858.6301\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 639719073.6727 - val_loss: 2368870259.7260\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 646062660.7164 - val_loss: 2277231956.1644\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 612187533.1620 - val_loss: 2401172110.0274\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 590911884.9974 - val_loss: 2243274266.3014\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 584990705.1928 - val_loss: 2282490723.9452\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 599397960.3907 - val_loss: 2252540701.8082\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 615689960.3633 - val_loss: 2325972318.6849\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 577663039.8081 - val_loss: 2354439639.6712\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 593966307.3728 - val_loss: 2181883719.8904\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 552265748.2913 - val_loss: 2190654048.4384\n",
      "Epoch 395/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 419us/step - loss: 637857781.4156 - val_loss: 2214793082.7397\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 583126219.6264 - val_loss: 2182806457.8630\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 628089033.9263 - val_loss: 2403850236.4932\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 599288422.6358 - val_loss: 2366905063.4521\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 626103832.1302 - val_loss: 2407291341.1507\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 570548071.0746 - val_loss: 2214153749.0411\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 600729730.2211 - val_loss: 2782370617.8630\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 616668776.5553 - val_loss: 2386182559.5616\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 627390685.6144 - val_loss: 2224731155.2877\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 557893881.9400 - val_loss: 2292105396.6027\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 545144828.6547 - val_loss: 2192441014.3562\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 558283691.4344 - val_loss: 2155195441.0959\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 544811436.0377 - val_loss: 2201508706.1918\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 550698000.5621 - val_loss: 2233732707.9452\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 591090273.2065 - val_loss: 2172412410.7397\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 562197502.7935 - val_loss: 2129002520.5479\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 624774251.9280 - val_loss: 2283998595.5068\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 1s 458us/step - loss: 597099676.4353 - val_loss: 2149478075.6164\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 1s 467us/step - loss: 546849668.2228 - val_loss: 2303420338.8493\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 1s 472us/step - loss: 527472307.1945 - val_loss: 2259343105.7534\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 1s 469us/step - loss: 522604062.8483 - val_loss: 2153799485.3699\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 1s 465us/step - loss: 533793090.4267 - val_loss: 2258412345.8630\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 1s 475us/step - loss: 519035235.7841 - val_loss: 2166524107.3973\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 1s 461us/step - loss: 542402578.3171 - val_loss: 2244855183.7808\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 1s 469us/step - loss: 569013728.3016 - val_loss: 2332655921.0959\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 1s 459us/step - loss: 538103436.0925 - val_loss: 2310023907.9452\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 1s 462us/step - loss: 576505642.6941 - val_loss: 2210883284.1644\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 1s 459us/step - loss: 542632405.9366 - val_loss: 2129097684.1644\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 554214285.9023 - val_loss: 2197056727.6712\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 515045332.5656 - val_loss: 2158640757.4795\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 584308170.8997 - val_loss: 2138831093.4795\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 508733226.8860 - val_loss: 2285831792.2192\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 589167336.1440 - val_loss: 2079473627.1781\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 522222990.7524 - val_loss: 2168033316.8219\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 571651492.4696 - val_loss: 2291274958.9041\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 603015320.4593 - val_loss: 2229901936.2192\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 524343174.9786 - val_loss: 2101377988.3836\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 539728373.8543 - val_loss: 2122909390.9041\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 511874972.2228 - val_loss: 2186072272.6575\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 562759148.3119 - val_loss: 2117983684.3836\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 519693291.8183 - val_loss: 2158459200.8767\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 493881214.0805 - val_loss: 2166377168.6575\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 502078934.1834 - val_loss: 2098739897.8630\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 529026566.9923 - val_loss: 2395801175.6712\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 530601616.3016 - val_loss: 2187134965.4795\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 487350926.5330 - val_loss: 2291397042.8493\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 486936561.5767 - val_loss: 2138136269.1507\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 413us/step - loss: 493159076.7986 - val_loss: 2202595319.2329\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 531578425.1997 - val_loss: 2168301138.4110\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 415us/step - loss: 487667499.3796 - val_loss: 2122696525.1507\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 518301955.1260 - val_loss: 2126776128.8767\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 498038780.9837 - val_loss: 2188954511.7808\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 500780114.0017 - val_loss: 2085714360.1096\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 517685615.8355 - val_loss: 2329906233.8630\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 1s 447us/step - loss: 551063976.7061 - val_loss: 2312218701.1507\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 490981971.3590 - val_loss: 2148451112.3288\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 528969558.9237 - val_loss: 2170177642.9589\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 523604603.7361 - val_loss: 2373947397.2603\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 499428918.2382 - val_loss: 2314304887.2329\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 493898900.0171 - val_loss: 2212574897.0959\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 492465896.5827 - val_loss: 2152268365.1507\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 497642579.1397 - val_loss: 2111886814.6849\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 454758872.5141 - val_loss: 2128725640.7671\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 472131492.2502 - val_loss: 2209085664.4384\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 466152114.4267 - val_loss: 2141905867.3973\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 469449370.2416 - val_loss: 2094584314.7397\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 471798480.6718 - val_loss: 2066637643.3973\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 474862000.7266 - val_loss: 2130776227.0685\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 488551699.7429 - val_loss: 2306299700.6027\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 475858674.9751 - val_loss: 2053234994.8493\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 504993967.1088 - val_loss: 2120908228.3836\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 486189729.8509 - val_loss: 2164837248.0000\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 466126397.9434 - val_loss: 2137085981.8082\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 443284154.0771 - val_loss: 2146301299.7260\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 455077509.4567 - val_loss: 2115810872.1096\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 466901363.0643 - val_loss: 2240322144.4384\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 493800170.4473 - val_loss: 2065612896.4384\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 455589020.4353 - val_loss: 2319207637.9178\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 445601691.5030 - val_loss: 2126078651.6164\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 441519543.4996 - val_loss: 2352376386.6301\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 431745276.0788 - val_loss: 2092260366.0274\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 430217983.4516 - val_loss: 2081458086.5753\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 468114833.5561 - val_loss: 2017135800.1096\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 453311539.0574 - val_loss: 2151033261.5890\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 523021385.1585 - val_loss: 2098038508.7123\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 465422913.1928 - val_loss: 2426776540.9315\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 550481967.5201 - val_loss: 2151403662.0274\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 470275424.1371 - val_loss: 2125608796.9315\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 434303994.8175 - val_loss: 2263325247.1233\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 488932887.2734 - val_loss: 2025614132.6027\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 453335345.7686 - val_loss: 2259035151.7808\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 446069468.8055 - val_loss: 2068731272.7671\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 436329514.4747 - val_loss: 2057267622.5753\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 431849700.5244 - val_loss: 2130841768.3288\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 457425081.9949 - val_loss: 2139316381.8082\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 448494086.4987 - val_loss: 2002187051.8356\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 448693023.7532 - val_loss: 2117843424.4384\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 455686421.2374 - val_loss: 2070145285.2603\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 416661227.8183 - val_loss: 2164533696.8767\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 495962642.2622 - val_loss: 2005678027.3973\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 426483906.4953 - val_loss: 2140407115.3973\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 429844164.6067 - val_loss: 2062513083.6164\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 414us/step - loss: 433833588.0994 - val_loss: 2207829184.8767\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 607598219.4070 - val_loss: 1982930601.2055\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 430218705.5081 - val_loss: 1972770870.3562\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 399483474.8518 - val_loss: 2035059394.6301\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_5000E_Adam_4H.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_5000E_Adam_4H.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 32)                10592     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,737\n",
      "Trainable params: 12,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_32 = Sequential()\n",
    "NN_5000E_Adam_32.add(Dense(32,input_dim = 330,activation = 'relu'))\n",
    "NN_5000E_Adam_32.add(Dense(32,activation = 'relu'))\n",
    "NN_5000E_Adam_32.add(Dense(32,activation = 'relu'))\n",
    "NN_5000E_Adam_32.add(Dense(1))\n",
    "NN_5000E_Adam_32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 38706398633.5698 - val_loss: 39777723770.7397\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 38636421785.1174 - val_loss: 39698240722.4110\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 38542318570.0634 - val_loss: 39589462240.4384\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 38415337147.3385 - val_loss: 39443433948.9315\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 38248806998.4302 - val_loss: 39249987275.3973\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 38010765052.2708 - val_loss: 38963096646.1370\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 37672731139.9486 - val_loss: 38570601149.3699\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 37220937980.7095 - val_loss: 38052890708.1644\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 36598502590.4096 - val_loss: 37321046773.4795\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 35805219818.0634 - val_loss: 36460606842.7397\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 34872381573.3745 - val_loss: 35419379571.7260\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 33742581523.0848 - val_loss: 34171426282.9589\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 32438129647.3282 - val_loss: 32714959409.0959\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 30893797028.5244 - val_loss: 30992828584.3288\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 29121939647.2871 - val_loss: 29113267073.7534\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 27253701934.7249 - val_loss: 27067186218.0822\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 25301668003.2082 - val_loss: 24952807003.1781\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 23338820777.3505 - val_loss: 22845223921.9726\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 21412865454.8346 - val_loss: 20753600960.8767\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 19594083869.3950 - val_loss: 18759517422.4658\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 17935180062.9306 - val_loss: 16933127967.5616\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 16475649507.4824 - val_loss: 15357747775.1233\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 15268909963.2973 - val_loss: 13884133572.3836\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 14237559091.1123 - val_loss: 12753926494.6849\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 13452300094.9580 - val_loss: 11853913466.7397\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 12808024805.4567 - val_loss: 11034024693.4795\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 12319047574.2656 - val_loss: 10391662409.6438\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 11900296191.1225 - val_loss: 9917871608.9863\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 11603727163.0094 - val_loss: 9430912631.2329\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 11313423176.6101 - val_loss: 9087031239.8904\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 11068894077.2579 - val_loss: 8854256345.4247\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 10853735214.2862 - val_loss: 8725884857.8630\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 10660931209.3231 - val_loss: 8466526853.2603\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 10486329650.2348 - val_loss: 8288456963.5068\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 10292422791.5681 - val_loss: 8086953212.4932\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 10128356060.6821 - val_loss: 7959175459.0685\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 9976599358.5193 - val_loss: 7823500351.1233\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 9809923101.8338 - val_loss: 7669834352.2192\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 9703149746.1251 - val_loss: 7552567930.7397\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 9534539733.4430 - val_loss: 7462937049.4247\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 9383754433.9195 - val_loss: 7324561692.0548\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 9245272851.7429 - val_loss: 7246285809.9726\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 9122447823.5201 - val_loss: 7161663526.5753\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 8975761924.6067 - val_loss: 7036752208.6575\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 8848736644.7164 - val_loss: 6965342043.1781\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 8735802418.6735 - val_loss: 6901014952.3288\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 8607325949.3676 - val_loss: 6790326380.7123\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 8477249712.1508 - val_loss: 6724546279.4521\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 8336319042.6872 - val_loss: 6644477012.1644\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 8235009435.5304 - val_loss: 6559246392.1096\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 8093321635.6470 - val_loss: 6502898835.2877\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 7982328565.4704 - val_loss: 6442004297.6438\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 7857201790.7935 - val_loss: 6377018550.3562\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 7735639634.9203 - val_loss: 6300772453.6986\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 7639975582.8209 - val_loss: 6226937982.2466\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 7512915052.8055 - val_loss: 6183156981.4795\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 7406318823.7601 - val_loss: 6127344668.0548\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 7282975436.4490 - val_loss: 6049111706.3014\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 7178417607.1842 - val_loss: 6007947253.4795\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 7063475700.1542 - val_loss: 5945050960.6575\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 6953709088.4662 - val_loss: 5879241931.3973\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 6847420000.0823 - val_loss: 5820818323.2877\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 6753905585.6864 - val_loss: 5772550256.2192\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 6623230864.3428 - val_loss: 5707564978.8493\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 6525066252.7232 - val_loss: 5657811810.1918\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 6425273773.0797 - val_loss: 5607142968.1096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 6317624499.0026 - val_loss: 5556530887.8904\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 6214892520.7472 - val_loss: 5502116827.1781\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 6116913004.8055 - val_loss: 5452863030.3562\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 6025396548.6615 - val_loss: 5398700131.9452\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 5953267253.3059 - val_loss: 5374689651.7260\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 5810141920.4113 - val_loss: 5317422718.2466\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 5734094331.1740 - val_loss: 5277504990.6849\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5636116918.2931 - val_loss: 5215588639.5616\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 5530374620.9015 - val_loss: 5179111062.7945\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 5449547373.6829 - val_loss: 5137831530.9589\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5341673511.0471 - val_loss: 5091163600.6575\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5265072582.7455 - val_loss: 5050199011.9452\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 5163992398.3136 - val_loss: 5004535776.4384\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5080508594.6187 - val_loss: 4963202773.9178\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4997120554.1183 - val_loss: 4926502570.0822\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4913954565.2648 - val_loss: 4888727716.8219\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4815495078.9374 - val_loss: 4849002865.9726\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4736895983.3282 - val_loss: 4811981357.5890\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 4650270387.4413 - val_loss: 4771804938.5205\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 4571201740.9974 - val_loss: 4734898526.6849\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4488653437.4773 - val_loss: 4699172348.4932\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 4418284794.2965 - val_loss: 4663280436.6027\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 4328052583.3213 - val_loss: 4628027493.6986\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 4247456842.1731 - val_loss: 4593235662.9041\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 4164169424.1782 - val_loss: 4560153840.2192\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 4087768613.5116 - val_loss: 4527197715.2877\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 4018893261.1071 - val_loss: 4496667085.1507\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 3943134419.2494 - val_loss: 4462239614.2466\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 3875217084.6547 - val_loss: 4430877392.6575\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 3805339698.2348 - val_loss: 4400140181.0411\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 3734887477.0865 - val_loss: 4373305508.8219\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 3670403773.6418 - val_loss: 4341492283.6164\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 3607399924.5930 - val_loss: 4312472546.1918\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3540711237.5390 - val_loss: 4283318605.1507\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3472353100.2296 - val_loss: 4259193168.6575\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 3411745480.3907 - val_loss: 4232893983.5616\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3361530084.5793 - val_loss: 4200075370.9589\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3285369875.8526 - val_loss: 4180581216.4384\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 3222169827.7018 - val_loss: 4155386408.3288\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3173382167.3076 - val_loss: 4120383561.6438\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 3103955209.4327 - val_loss: 4099355448.1096\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 3050930032.0960 - val_loss: 4077304104.3288\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2990750478.3685 - val_loss: 4043948764.9315\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2940906004.4010 - val_loss: 4018824591.7808\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 2882607095.2254 - val_loss: 4009177217.7534\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2830177688.1851 - val_loss: 3965381930.0822\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2774880382.2451 - val_loss: 3952398397.3699\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 2730119532.8055 - val_loss: 3926675943.4521\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2679935106.1388 - val_loss: 3909341378.6301\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2640372572.1337 - val_loss: 3888944827.6164\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2588194538.3925 - val_loss: 3874660306.4110\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 2547997006.2039 - val_loss: 3858030118.5753\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2508642872.2674 - val_loss: 3837574517.4795\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2469528089.4464 - val_loss: 3817418906.3014\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 2427632317.7515 - val_loss: 3798146256.6575\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 2396896891.3933 - val_loss: 3799322247.0137\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 2362590387.0026 - val_loss: 3763367294.2466\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 2318120108.4216 - val_loss: 3753669028.8219\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2286852517.0728 - val_loss: 3738668835.0685\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2254323046.0051 - val_loss: 3722019175.4521\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2222012521.0763 - val_loss: 3713085522.4110\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2191895375.5201 - val_loss: 3695300153.8630\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2167147673.6658 - val_loss: 3681623038.2466\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2139802497.3162 - val_loss: 3664703047.8904\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2113456239.4379 - val_loss: 3662482817.7534\n",
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 2085059142.3068 - val_loss: 3639073692.0548\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 52us/step - loss: 2062470155.5167 - val_loss: 3629449210.7397\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2039034682.6804 - val_loss: 3611442688.0000\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 2017470409.5973 - val_loss: 3597317581.1507\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1994395819.6538 - val_loss: 3595217275.6164\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1974552403.7978 - val_loss: 3586392843.3973\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - ETA: 0s - loss: 1971996256.00 - 0s 55us/step - loss: 1957795174.6632 - val_loss: 3578731761.9726\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1939415341.8475 - val_loss: 3568260553.6438\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1922744864.3290 - val_loss: 3552882700.2740\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1905478207.0677 - val_loss: 3553967503.7808\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1890238106.9820 - val_loss: 3538871984.2192\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1877667966.3822 - val_loss: 3536281866.5205\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1864408960.0000 - val_loss: 3516147306.9589\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1850358573.8475 - val_loss: 3512954774.7945\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1837450442.4747 - val_loss: 3505507644.4932\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1829624468.3462 - val_loss: 3491177894.5753\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1815536968.9392 - val_loss: 3500005283.0685\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1807436928.2194 - val_loss: 3495819360.4384\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1795641924.9906 - val_loss: 3479796967.4521\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1787179761.4122 - val_loss: 3471683278.0274\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1780303638.4302 - val_loss: 3469492081.9726\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1771548038.4713 - val_loss: 3463687374.9041\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1764629664.7952 - val_loss: 3461038145.7534\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1757342492.6272 - val_loss: 3456917567.1233\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 1751547446.5124 - val_loss: 3453712243.7260\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1746295093.9640 - val_loss: 3448716894.6849\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1739427787.5716 - val_loss: 3440058282.9589\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1734687408.1508 - val_loss: 3435727543.2329\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1730488431.8766 - val_loss: 3425369769.2055\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1727635935.7532 - val_loss: 3436082506.5205\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1722694381.6829 - val_loss: 3414917388.2740\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1718910985.8166 - val_loss: 3423651845.2603\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1716679538.6735 - val_loss: 3415767932.4932\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1711951244.3942 - val_loss: 3426715008.8767\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1707524151.4996 - val_loss: 3415765361.0959\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1705556316.1337 - val_loss: 3410250505.6438\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1702480068.5518 - val_loss: 3415080218.3014\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1702778834.8106 - val_loss: 3395565839.7808\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1697179953.7961 - val_loss: 3407484970.9589\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1694956684.9426 - val_loss: 3413392680.3288\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1692443871.9177 - val_loss: 3410999443.2877\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1692344402.0428 - val_loss: 3384032043.8356\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1687718659.7292 - val_loss: 3402425109.9178\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1684760729.3368 - val_loss: 3401716743.0137\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 1681913474.6324 - val_loss: 3391241259.8356\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 53us/step - loss: 1681247660.8603 - val_loss: 3390127026.8493\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1679080424.4182 - val_loss: 3398176941.5890\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 53us/step - loss: 1680472277.2648 - val_loss: 3392955113.2055\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1673838683.5853 - val_loss: 3386586562.6301\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1672412963.8663 - val_loss: 3384779902.2466\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1674562137.9949 - val_loss: 3386585903.3425\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 1670759076.8535 - val_loss: 3361329436.0548\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1666544476.0788 - val_loss: 3383822623.5616\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 63us/step - loss: 1666571945.1311 - val_loss: 3396075585.7534\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1664615394.1662 - val_loss: 3384505978.7397\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1666060725.7995 - val_loss: 3386037789.8082\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1661833314.7695 - val_loss: 3399095325.8082\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1659161578.7215 - val_loss: 3369155649.7534\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1659652241.6590 - val_loss: 3366439484.4932\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1655639092.5381 - val_loss: 3383871337.2055\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1655291298.1114 - val_loss: 3378685585.5342\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1653014404.9906 - val_loss: 3394723612.0548\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1651405891.8389 - val_loss: 3376431242.5205\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1650283011.6195 - val_loss: 3382085351.4521\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1647595548.0788 - val_loss: 3367060853.4795\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1645953143.8835 - val_loss: 3363220888.5479\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1646748258.7147 - val_loss: 3364070696.3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1642946155.7635 - val_loss: 3378011390.2466\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1643436666.3239 - val_loss: 3388817851.6164\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1641126022.1422 - val_loss: 3370086361.4247\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1640486235.0368 - val_loss: 3368569985.7534\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1640567324.9015 - val_loss: 3365025458.8493\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1636792076.9152 - val_loss: 3356467892.6027\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1633960917.0043 - val_loss: 3374617394.8493\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1637340350.7798 - val_loss: 3348085214.6849\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1630970737.5219 - val_loss: 3367064267.3973\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1631393451.5441 - val_loss: 3367508408.1096\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1632107544.2399 - val_loss: 3358085837.1507\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1629815965.2853 - val_loss: 3360494528.8767\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1630682019.4276 - val_loss: 3344180395.8356\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1624866225.4670 - val_loss: 3376202373.2603\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1624640587.6812 - val_loss: 3369888859.1781\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1623777854.1902 - val_loss: 3369012103.0137\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1621197153.0420 - val_loss: 3353191995.6164\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1620273572.5244 - val_loss: 3350942855.0137\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1620673721.1448 - val_loss: 3344478467.5068\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1618500240.5073 - val_loss: 3357727873.7534\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1619188639.9177 - val_loss: 3358749336.5479\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1617198681.3093 - val_loss: 3338672303.3425\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1614507859.2768 - val_loss: 3354863195.1781\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1613196823.1431 - val_loss: 3362721052.0548\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1612524431.9040 - val_loss: 3349652231.0137\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1610517926.9374 - val_loss: 3338402808.9863\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1611139156.2365 - val_loss: 3354705194.0822\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1609206515.0574 - val_loss: 3348781113.8630\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1606634112.7129 - val_loss: 3352287654.5753\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1608319920.2605 - val_loss: 3366012263.4521\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1610905307.0368 - val_loss: 3346298688.8767\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1603822376.4182 - val_loss: 3356487969.3151\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1603342626.7695 - val_loss: 3338179043.9452\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1601124574.5193 - val_loss: 3338147429.6986\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1599499721.4876 - val_loss: 3341876406.3562\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1600386606.7249 - val_loss: 3361123033.4247\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1598714451.1397 - val_loss: 3335292542.2466\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1599877571.1260 - val_loss: 3325158875.1781\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1600471715.9760 - val_loss: 3368318337.7534\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1594547448.1028 - val_loss: 3327266288.2192\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1594258169.6932 - val_loss: 3330957715.2877\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1592677826.7969 - val_loss: 3351511466.0822\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1591808407.5818 - val_loss: 3329061919.5616\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1593877713.9332 - val_loss: 3352627929.4247\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1590893470.5467 - val_loss: 3312453696.8767\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1591728951.3899 - val_loss: 3349289852.4932\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1589881132.4764 - val_loss: 3330775594.0822\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1588658104.3770 - val_loss: 3320039490.6301\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1586497934.1491 - val_loss: 3331378533.6986\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1585370064.3428 - val_loss: 3342122816.8767\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1584601782.9512 - val_loss: 3329291313.0959\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1582765797.2374 - val_loss: 3339787418.3014\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1583465219.6195 - val_loss: 3319001585.9726\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1584579680.6855 - val_loss: 3333327921.0959\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1582124058.7626 - val_loss: 3325263484.4932\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1582069529.6658 - val_loss: 3357233393.9726\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1583155360.7952 - val_loss: 3328286083.5068\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1577047561.6521 - val_loss: 3339482282.0822\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1576756690.4816 - val_loss: 3342498053.2603\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1576185572.5244 - val_loss: 3324439883.3973\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1575592102.7181 - val_loss: 3334397031.4521\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1574631051.4070 - val_loss: 3341170154.9589\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1576035657.3779 - val_loss: 3306831628.2740\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1572128399.4105 - val_loss: 3337726525.3699\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1570996126.9306 - val_loss: 3317617194.0822\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1571672843.1877 - val_loss: 3326764831.5616\n",
      "Epoch 265/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 60us/step - loss: 1570279104.1097 - val_loss: 3324334576.2192\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1568328393.8166 - val_loss: 3333199596.7123\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1569793750.1560 - val_loss: 3337802536.3288\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1566240392.4456 - val_loss: 3336586560.8767\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1568994391.8423 - val_loss: 3313692328.3288\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1567468620.2296 - val_loss: 3312366188.7123\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1564970378.9683 - val_loss: 3312891207.8904\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1566892999.8423 - val_loss: 3317635817.2055\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1561978648.1851 - val_loss: 3338508084.6027\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1564405482.8312 - val_loss: 3323758125.5890\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1565620191.0951 - val_loss: 3338893227.8356\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1561465066.2828 - val_loss: 3313342681.4247\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1568981902.2039 - val_loss: 3330112536.5479\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1560113636.5244 - val_loss: 3294286199.2329\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1558503755.4619 - val_loss: 3318952651.3973\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1556752483.4824 - val_loss: 3303615663.3425\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1559867775.0129 - val_loss: 3317163043.0685\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1555163925.9640 - val_loss: 3310872523.3973\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1554209150.0257 - val_loss: 3302283106.1918\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1554028558.4781 - val_loss: 3298381620.6027\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1554974751.5887 - val_loss: 3307001003.8356\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1557085587.0848 - val_loss: 3302131852.2740\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1552921951.6435 - val_loss: 3312779860.1644\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1553246008.8706 - val_loss: 3301375992.9863\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1549916051.1945 - val_loss: 3301833533.3699\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1550725861.1277 - val_loss: 3305065864.7671\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1548908395.4893 - val_loss: 3305409518.4658\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1547857614.6427 - val_loss: 3300498784.4384\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1548983276.5861 - val_loss: 3314609166.0274\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1547381346.6050 - val_loss: 3294355559.4521\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1546236545.2614 - val_loss: 3310683700.6027\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1545287706.8723 - val_loss: 3297308366.9041\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1544330124.4490 - val_loss: 3294957266.4110\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1545272419.3728 - val_loss: 3303228505.4247\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1546049333.8543 - val_loss: 3294140303.7808\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1542332286.6838 - val_loss: 3306862090.5205\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1543987471.2734 - val_loss: 3297484133.6986\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1542748617.1585 - val_loss: 3326367018.0822\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1542180393.5698 - val_loss: 3288134673.5342\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1539066629.2648 - val_loss: 3276072177.9726\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1542246092.4490 - val_loss: 3305773776.6575\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1539218996.3188 - val_loss: 3279851672.5479\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1537352052.4833 - val_loss: 3308955306.0822\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1537320515.6744 - val_loss: 3291054977.7534\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1539481241.2271 - val_loss: 3305520399.7808\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1536118347.0231 - val_loss: 3288447644.0548\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1534592269.2168 - val_loss: 3284422559.5616\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1537395395.0985 - val_loss: 3270059772.4932\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1531610300.2159 - val_loss: 3312667781.2603\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1533855421.4225 - val_loss: 3305355228.9315\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1535321952.3016 - val_loss: 3298388953.4247\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1538843926.9237 - val_loss: 3288880587.3973\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1532028982.4027 - val_loss: 3284776916.1644\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1531159296.2742 - val_loss: 3282761985.7534\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1531414114.0291 - val_loss: 3274923909.2603\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1529698755.1808 - val_loss: 3292283521.7534\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1529307895.2254 - val_loss: 3278986851.9452\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1529553500.1885 - val_loss: 3279407056.6575\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1529385230.9169 - val_loss: 3287744361.2055\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1529842700.9426 - val_loss: 3311285730.1918\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1525120427.3248 - val_loss: 3271882197.9178\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1526953513.1859 - val_loss: 3286710014.2466\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1524520400.3702 - val_loss: 3291868356.3836\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1524830274.1388 - val_loss: 3299909972.1644\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1525849500.7369 - val_loss: 3275655671.2329\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1522915575.0060 - val_loss: 3283981357.5890\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1521478965.2511 - val_loss: 3273719729.0959\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1525159656.9666 - val_loss: 3279379178.9589\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1523251915.5716 - val_loss: 3289551121.5342\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1520413531.9143 - val_loss: 3284330848.4384\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1521950434.2759 - val_loss: 3275888320.8767\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1519656317.6967 - val_loss: 3271421639.8904\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 1517919038.8483 - val_loss: 3275567707.1781\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1520154150.8278 - val_loss: 3277380253.8082\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1520201249.6727 - val_loss: 3269832893.3699\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1521117311.0129 - val_loss: 3273244414.2466\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1519509877.4704 - val_loss: 3266068923.6164\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1516237902.8895 - val_loss: 3281168811.8356\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1516469935.1637 - val_loss: 3282818002.4110\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1516114923.9280 - val_loss: 3283774890.0822\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1512970700.0651 - val_loss: 3274600013.1507\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1517373470.7112 - val_loss: 3294449583.3425\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1525440964.6615 - val_loss: 3250548818.4110\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1513587383.3899 - val_loss: 3260082093.5890\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1510653332.6204 - val_loss: 3282825591.2329\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1512875770.1868 - val_loss: 3263326285.1507\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 53us/step - loss: 1514542504.8021 - val_loss: 3268497383.4521\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1511196022.0189 - val_loss: 3301923797.9178\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1512069283.8663 - val_loss: 3261537280.0000\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1510187746.2759 - val_loss: 3270397724.0548\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1510388869.7309 - val_loss: 3270987470.9041\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1508622600.9392 - val_loss: 3250580078.4658\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1512035835.7224 - val_loss: 3248653578.5205\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1509030281.7618 - val_loss: 3275158685.8082\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1506152789.1962 - val_loss: 3267227232.4384\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1506064821.1962 - val_loss: 3271577901.5890\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1504672111.4379 - val_loss: 3249588017.0959\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1505568442.0771 - val_loss: 3269657901.5890\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1504521214.1354 - val_loss: 3257113496.5479\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1507002240.2194 - val_loss: 3253819136.0000\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1504684521.4053 - val_loss: 3248412649.2055\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1502914994.3719 - val_loss: 3253278495.5616\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1501579723.3522 - val_loss: 3267931127.2329\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1502474516.6204 - val_loss: 3271842645.9178\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1503408805.9503 - val_loss: 3287094917.2603\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1500035197.1482 - val_loss: 3248584376.1096\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1501230360.4045 - val_loss: 3270496112.2192\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1500860263.1020 - val_loss: 3249280582.1370\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1497934593.0968 - val_loss: 3270298659.0685\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1499776000.3290 - val_loss: 3256270998.7945\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1497232771.8389 - val_loss: 3263570677.4795\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1498850310.2519 - val_loss: 3262840221.8082\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1496281235.4139 - val_loss: 3241033584.2192\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1497488735.3145 - val_loss: 3245896626.8493\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1497479608.9254 - val_loss: 3262576419.0685\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1499584348.5176 - val_loss: 3262936691.7260\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1503448550.2245 - val_loss: 3256721062.5753\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1498932223.4516 - val_loss: 3257401715.7260\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1497135519.4790 - val_loss: 3265295628.2740\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1494110699.8183 - val_loss: 3256117975.6712\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1497377424.5621 - val_loss: 3256655160.1096\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1495445215.1500 - val_loss: 3263472839.8904\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1494065969.7961 - val_loss: 3259768546.1918\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1492116730.6804 - val_loss: 3246462972.4932\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1492950162.5364 - val_loss: 3254962712.5479\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1490286117.2922 - val_loss: 3264291832.9863\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1494607285.1414 - val_loss: 3229488913.5342\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1490344395.7361 - val_loss: 3267381917.8082\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1490028983.4996 - val_loss: 3251606594.6301\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1488713414.8003 - val_loss: 3241056436.6027\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1488494062.5056 - val_loss: 3250394033.0959\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1488652402.5090 - val_loss: 3253074198.7945\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 58us/step - loss: 1486969053.2305 - val_loss: 3231215961.4247\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1485900301.8201 - val_loss: 3240620473.8630\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1490707363.6470 - val_loss: 3247976372.6027\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1488909484.6410 - val_loss: 3274217538.6301\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1495862845.0934 - val_loss: 3234048885.4795\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1485780904.8569 - val_loss: 3238229065.6438\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1487108986.9546 - val_loss: 3245870991.7808\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1490039268.5793 - val_loss: 3216480452.3836\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1480992769.5356 - val_loss: 3258522483.7260\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1484978125.7104 - val_loss: 3243279184.6575\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1486306557.5321 - val_loss: 3250113153.7534\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1482138049.2614 - val_loss: 3218907947.8356\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1482320573.5870 - val_loss: 3224521633.3151\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1479427623.9246 - val_loss: 3247877453.1507\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1485569207.0608 - val_loss: 3228562154.9589\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1485710774.4027 - val_loss: 3276820548.3836\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1481066139.6401 - val_loss: 3236017283.5068\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1479683953.0831 - val_loss: 3231855323.1781\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1478071504.3976 - val_loss: 3243903042.6301\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1482730487.5544 - val_loss: 3248476898.1918\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1478934622.2177 - val_loss: 3222848634.7397\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1479707065.6384 - val_loss: 3217185758.6849\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1479066300.7644 - val_loss: 3234499347.2877\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1479548089.2545 - val_loss: 3219090284.7123\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1475982091.5167 - val_loss: 3255215668.6027\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1475937192.6924 - val_loss: 3239381507.5068\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1474830663.6778 - val_loss: 3232866833.5342\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1475361120.4662 - val_loss: 3245886232.5479\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1481581588.8398 - val_loss: 3225789113.8630\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1475648315.6675 - val_loss: 3253279921.0959\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1474589375.3967 - val_loss: 3225022891.8356\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1473099330.3582 - val_loss: 3230469945.8630\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1475045690.4610 - val_loss: 3217910180.8219\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1473758648.5416 - val_loss: 3231167386.3014\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1471057962.3376 - val_loss: 3233282987.8356\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1473022559.8629 - val_loss: 3220702179.9452\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1471570520.0754 - val_loss: 3220594302.2466\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1469428385.5081 - val_loss: 3233011775.1233\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1472438270.4644 - val_loss: 3222713854.2466\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1470033951.4790 - val_loss: 3241057792.0000\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1470120003.8389 - val_loss: 3212787189.4795\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1474844036.6067 - val_loss: 3229537560.5479\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1472377732.7164 - val_loss: 3205435207.8904\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 1468226268.7918 - val_loss: 3222889137.0959\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1467326716.3805 - val_loss: 3222296444.4932\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1466337237.7172 - val_loss: 3225696613.6986\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1467379493.6213 - val_loss: 3219830079.1233\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1466157290.6118 - val_loss: 3229535917.5890\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1466948386.5501 - val_loss: 3210768203.3973\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1464894467.7292 - val_loss: 3216125906.4110\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1465758578.5090 - val_loss: 3234856998.5753\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1465763920.6170 - val_loss: 3208414521.8630\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1464103865.1448 - val_loss: 3228899152.6575\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1465476036.1131 - val_loss: 3222152968.7671\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1463218183.1294 - val_loss: 3212645158.5753\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1463579299.9760 - val_loss: 3207096763.6164\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1463970871.1705 - val_loss: 3214238935.6712\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1463704969.7618 - val_loss: 3194173436.4932\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1460909807.2185 - val_loss: 3242586289.0959\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1462218206.6564 - val_loss: 3195322410.0822\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1460174671.4105 - val_loss: 3231075243.8356\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1460402533.1277 - val_loss: 3217991138.1918\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1462218801.4670 - val_loss: 3196542492.0548\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1461586358.9512 - val_loss: 3225326230.7945\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 52us/step - loss: 1457585468.8740 - val_loss: 3198147790.9041\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1457424939.3248 - val_loss: 3217194671.3425\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1458521830.3342 - val_loss: 3205390181.6986\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1456691644.4901 - val_loss: 3213111255.6712\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1460065529.7481 - val_loss: 3195573916.0548\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1456073851.9966 - val_loss: 3228109978.3014\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1456722377.5150 - val_loss: 3195960516.3836\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1456343124.6752 - val_loss: 3195000917.9178\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1455973650.8655 - val_loss: 3208429326.0274\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1455483148.7232 - val_loss: 3213158640.2192\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1457038310.1148 - val_loss: 3220148371.2877\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1454206584.1028 - val_loss: 3198300805.2603\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1454387145.5424 - val_loss: 3200413061.2603\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1452827117.5184 - val_loss: 3202696868.8219\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1454262713.9674 - val_loss: 3208549428.6027\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1454195866.1045 - val_loss: 3188978384.6575\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1454608477.0111 - val_loss: 3186756976.2192\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1451465321.1311 - val_loss: 3222083682.1918\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1453027177.6247 - val_loss: 3206809184.4384\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1450490898.7009 - val_loss: 3207057283.5068\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1450616565.7446 - val_loss: 3212872234.0822\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1450770115.0163 - val_loss: 3200044149.4795\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1453509328.5621 - val_loss: 3210151893.9178\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1452355236.9632 - val_loss: 3179999398.5753\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1449791432.3907 - val_loss: 3220240220.9315\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1447590060.8603 - val_loss: 3194245600.4384\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1447647414.8415 - val_loss: 3199469266.4110\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1449048873.8989 - val_loss: 3193018480.2192\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1446570931.8800 - val_loss: 3194738570.5205\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1448366469.0454 - val_loss: 3198471087.3425\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1449191750.9649 - val_loss: 3209340421.2603\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1450744712.4456 - val_loss: 3208845601.3151\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1446352680.2536 - val_loss: 3176652184.5479\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1446298082.1388 - val_loss: 3200123697.0959\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1444904612.8535 - val_loss: 3206175114.5205\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1446772943.5201 - val_loss: 3188714450.4110\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1446655241.8715 - val_loss: 3194649866.5205\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1443726001.1380 - val_loss: 3210666197.9178\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 1443403879.5407 - val_loss: 3194751077.6986\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1448296398.5330 - val_loss: 3211983658.0822\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_5000E_Adam_32.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_5000E_Adam_32.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 128)               42368     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 75,521\n",
      "Trainable params: 75,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_128 = Sequential()\n",
    "NN_5000E_Adam_128.add(Dense(128,input_dim = 330,activation = 'relu'))\n",
    "NN_5000E_Adam_128.add(Dense(128,activation = 'relu'))\n",
    "NN_5000E_Adam_128.add(Dense(128,activation = 'relu'))\n",
    "NN_5000E_Adam_128.add(Dense(1))\n",
    "NN_5000E_Adam_128.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 187us/step - loss: 38690821717.5527 - val_loss: 39492075295.5616\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 38067167105.6452 - val_loss: 38787898438.1370\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 37190807258.9272 - val_loss: 37712415982.4658\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 35843968595.7978 - val_loss: 36016919930.7397\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 33685374161.7138 - val_loss: 33240283023.7808\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 30315134535.5133 - val_loss: 29067547633.9726\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 25726978767.5201 - val_loss: 23658073130.0822\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 20687751560.2262 - val_loss: 17790178893.1507\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 16332400330.2554 - val_loss: 13507428772.8219\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 13912655913.2408 - val_loss: 10726015200.4384\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 12373337261.7378 - val_loss: 9575326229.0411\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 11631635721.4327 - val_loss: 8843370446.9041\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 10945010183.0197 - val_loss: 7988676488.7671\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 10465418717.9983 - val_loss: 7637861512.7671\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 9926815894.7044 - val_loss: 7271128025.4247\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 9455436974.6153 - val_loss: 6929785249.3151\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 9050618438.8552 - val_loss: 6756696190.2466\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 8458948733.6967 - val_loss: 6462015224.9863\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 8060035825.3025 - val_loss: 6213013760.0000\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 7721604976.3153 - val_loss: 6058501624.9863\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 7373885489.5767 - val_loss: 5837274918.5753\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 7046762151.7052 - val_loss: 5713586190.0274\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 6714713529.8029 - val_loss: 5560544222.6849\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 6362998518.3479 - val_loss: 5411602556.4932\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 6114681818.9272 - val_loss: 5261357199.7808\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 5900298889.7618 - val_loss: 5140693509.2603\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 5516017395.4961 - val_loss: 5024565849.4247\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 5262466807.8835 - val_loss: 4911693916.9315\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 5033685816.8158 - val_loss: 4801774953.2055\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 4785897392.1508 - val_loss: 4711544777.6438\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 4565737980.9289 - val_loss: 4624816354.1918\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 4373846530.4130 - val_loss: 4538253781.9178\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 4170533894.2519 - val_loss: 4471793390.4658\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 4027080239.1637 - val_loss: 4381940913.0959\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 3817368406.1011 - val_loss: 4327780955.1781\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 3671174179.9760 - val_loss: 4255320768.8767\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 3509813862.4439 - val_loss: 4198651961.8630\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 3364312493.4087 - val_loss: 4145323935.5616\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 3233605219.0437 - val_loss: 4083228373.9178\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 3113194758.7455 - val_loss: 4038278405.2603\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 3000024144.7266 - val_loss: 3987423161.8630\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2897044877.8201 - val_loss: 3940047659.8356\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 2794538529.1243 - val_loss: 3898660660.6027\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 2693925538.3308 - val_loss: 3870363409.5342\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 2615841646.3410 - val_loss: 3825830321.0959\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 2525107162.5981 - val_loss: 3806016874.9589\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2449423670.2931 - val_loss: 3767683345.5342\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 2376894208.9323 - val_loss: 3727610050.6301\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 2313798758.7729 - val_loss: 3701340980.6027\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2247536923.4207 - val_loss: 3680367987.7260\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2197614061.7926 - val_loss: 3645921213.3699\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2149296620.4764 - val_loss: 3624097872.6575\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 2101034082.4953 - val_loss: 3586967536.2192\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 2060009782.6221 - val_loss: 3564261879.2329\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2017589945.1448 - val_loss: 3563128867.0685\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1986802820.9906 - val_loss: 3533260000.4384\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1953185707.7635 - val_loss: 3501930897.5342\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1921736022.6495 - val_loss: 3493290848.4384\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1891507887.7395 - val_loss: 3492703905.3151\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1872829801.2408 - val_loss: 3479570442.5205\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1850119845.6213 - val_loss: 3461361009.9726\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1833115860.6752 - val_loss: 3449006435.9452\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1815890317.2716 - val_loss: 3434561678.0274\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1798590653.0934 - val_loss: 3440421863.4521\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1790863390.4919 - val_loss: 3453983845.6986\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1780702719.2871 - val_loss: 3410398958.4658\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 77us/step - loss: 1765073418.6941 - val_loss: 3411879232.8767\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1758582463.3967 - val_loss: 3396047347.7260\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1744228315.3111 - val_loss: 3391115974.1370\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1740766425.8303 - val_loss: 3383109579.3973\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1731204590.8895 - val_loss: 3370837176.1096\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1725354745.0900 - val_loss: 3366179091.2877\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1718979936.6307 - val_loss: 3377862207.1233\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1712693046.8415 - val_loss: 3371860983.2329\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1708490303.5613 - val_loss: 3378068564.1644\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1704787961.7481 - val_loss: 3339882573.1507\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1703973682.1251 - val_loss: 3357356445.8082\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1695298964.4010 - val_loss: 3339705628.0548\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1689236585.5150 - val_loss: 3365038067.7260\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1690931133.9709 - val_loss: 3380680388.3836\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1688811651.5099 - val_loss: 3322751175.8904\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1685057484.7781 - val_loss: 3350533686.3562\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1679914373.8680 - val_loss: 3327800735.5616\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1679541471.7532 - val_loss: 3319176711.0137\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1669090301.0386 - val_loss: 3360757882.7397\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1668658837.2785 - val_loss: 3353300683.3973\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1664280498.7832 - val_loss: 3345946013.8082\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1661486543.7395 - val_loss: 3336027839.1233\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1659604884.7301 - val_loss: 3359719420.4932\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1656503828.5107 - val_loss: 3338508133.6986\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1660222528.0000 - val_loss: 3330688746.9589\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1655388307.1945 - val_loss: 3313850185.6438\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1648768505.3093 - val_loss: 3330929718.3562\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1644764600.8158 - val_loss: 3338596937.6438\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 88us/step - loss: 1646409940.0720 - val_loss: 3339107941.6986\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1643141319.1294 - val_loss: 3353732111.7808\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1636680942.8895 - val_loss: 3323005729.3151\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1635330841.0077 - val_loss: 3323178573.1507\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1634361155.8389 - val_loss: 3359353712.2192\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1631797359.3282 - val_loss: 3318075213.1507\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1629113475.6744 - val_loss: 3319366305.3151\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1627805146.2691 - val_loss: 3334105887.5616\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1629693944.9803 - val_loss: 3318363120.2192\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1626486403.6195 - val_loss: 3317063436.2740\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1618069643.4893 - val_loss: 3336611662.9041\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1619008606.3273 - val_loss: 3334277624.9863\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1617880009.8166 - val_loss: 3302084320.4384\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1610036048.9460 - val_loss: 3332106103.2329\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1617190670.5878 - val_loss: 3321647861.4795\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1608856921.8303 - val_loss: 3330656031.5616\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1607572440.8432 - val_loss: 3327140579.9452\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1607331026.5638 - val_loss: 3349558008.9863\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1610473889.6727 - val_loss: 3352170232.9863\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1606755967.5064 - val_loss: 3293851223.6712\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1602693199.9589 - val_loss: 3327297080.1096\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1597508780.4764 - val_loss: 3291412015.3425\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1595430501.3470 - val_loss: 3300611426.1918\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1596271285.1962 - val_loss: 3320798600.7671\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1604448752.0411 - val_loss: 3320680824.9863\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1590719620.1680 - val_loss: 3327045374.2466\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1587118316.5861 - val_loss: 3304098256.6575\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1587905175.7464 - val_loss: 3322614454.3562\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1590611202.5775 - val_loss: 3312616069.2603\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1599297664.5484 - val_loss: 3298919811.5068\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1585494978.3582 - val_loss: 3293800064.0000\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1579680696.3222 - val_loss: 3337476565.9178\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1579820378.9820 - val_loss: 3313943704.5479\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1575912695.0608 - val_loss: 3332444913.9726\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1571208151.8560 - val_loss: 3292374017.7534\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1570281117.3950 - val_loss: 3293225699.9452\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1567917123.9486 - val_loss: 3292520125.3699\n",
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1566880886.2382 - val_loss: 3258921528.1096\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1570674846.0531 - val_loss: 3255703068.0548\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1563468971.9829 - val_loss: 3270459430.5753\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1562383891.7978 - val_loss: 3266234839.6712\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1558576079.4653 - val_loss: 3267359046.1370\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1557170502.5261 - val_loss: 3324256352.4384\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1558175326.0531 - val_loss: 3227761960.3288\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1556239595.2151 - val_loss: 3343159089.0959\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1555928486.4987 - val_loss: 3265898780.0548\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1549365186.9066 - val_loss: 3272611441.9726\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1550740153.1448 - val_loss: 3262053619.7260\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1545835599.7395 - val_loss: 3290108261.6986\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1555280816.2605 - val_loss: 3227780208.2192\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1546191682.9066 - val_loss: 3286265459.7260\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1543843507.4413 - val_loss: 3246309409.3151\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1546611775.6161 - val_loss: 3236711758.9041\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1543095150.5604 - val_loss: 3265072373.4795\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1544141717.0591 - val_loss: 3252649612.2740\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1549295673.8578 - val_loss: 3208628355.5068\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1537496262.7455 - val_loss: 3269559927.2329\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1538829363.6058 - val_loss: 3275735843.0685\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1534752111.5476 - val_loss: 3240437475.9452\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1528171347.5784 - val_loss: 3299476245.0411\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1527832808.3085 - val_loss: 3271722033.0959\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1526442370.9614 - val_loss: 3302241050.3014\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1521467822.2862 - val_loss: 3237932151.2329\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1525087722.7763 - val_loss: 3292718020.3836\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1521724569.7207 - val_loss: 3232061354.0822\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1524208277.4430 - val_loss: 3227814670.0274\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1521896209.0831 - val_loss: 3244831744.0000\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1515204024.4867 - val_loss: 3241420286.2466\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1514870550.0463 - val_loss: 3225552178.8493\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1518505196.0377 - val_loss: 3297948612.3836\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1519946437.4841 - val_loss: 3275245373.3699\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1511770031.0540 - val_loss: 3264484257.3151\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1508949058.9066 - val_loss: 3265124969.2055\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1508174864.7815 - val_loss: 3237168403.2877\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1506134690.1114 - val_loss: 3192860545.7534\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1509281838.2862 - val_loss: 3215589863.4521\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1502696444.3256 - val_loss: 3241339698.8493\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1518082064.6718 - val_loss: 3235630469.2603\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1505001920.1645 - val_loss: 3217396911.3425\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1502174259.7704 - val_loss: 3263088120.9863\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1498949032.4730 - val_loss: 3163575196.0548\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1500360093.6144 - val_loss: 3210644658.8493\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1499736509.9709 - val_loss: 3279094901.4795\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1489219477.8269 - val_loss: 3204941343.5616\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1492210820.3873 - val_loss: 3206655747.5068\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1494217260.6958 - val_loss: 3257974954.0822\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1487787094.4302 - val_loss: 3234007706.3014\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1485465137.1380 - val_loss: 3218581318.1370\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1483763452.8192 - val_loss: 3198049711.3425\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1481178088.3085 - val_loss: 3204465613.1507\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1482555226.7078 - val_loss: 3261255616.8767\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1482571524.7712 - val_loss: 3194029340.0548\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1478707517.3676 - val_loss: 3224623731.7260\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1480306651.1465 - val_loss: 3186029978.3014\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1478709230.3410 - val_loss: 3159731394.6301\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1479275049.2408 - val_loss: 3250761664.8767\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1482342482.8106 - val_loss: 3267631095.2329\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1471410374.4439 - val_loss: 3162824761.8630\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1471130591.6710 - val_loss: 3203604450.1918\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1470992428.7506 - val_loss: 3156579708.4932\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1465524189.7789 - val_loss: 3270903902.6849\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1479179382.5673 - val_loss: 3171777918.2466\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1465199777.5630 - val_loss: 3167468505.4247\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1468787796.8946 - val_loss: 3259688109.5890\n",
      "Epoch 199/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 76us/step - loss: 1459983632.4524 - val_loss: 3164350236.0548\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1459066925.9023 - val_loss: 3197279603.7260\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1459778316.4490 - val_loss: 3218539085.1507\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1455076517.3470 - val_loss: 3173961978.7397\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1469992295.7601 - val_loss: 3283399466.0822\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1460079372.9426 - val_loss: 3190675250.8493\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1453765550.6427 - val_loss: 3171887473.9726\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1452141877.0865 - val_loss: 3207173442.6301\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 73us/step - loss: 1450308726.2382 - val_loss: 3140537363.2877\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1455467347.7429 - val_loss: 3158695727.3425\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1457097786.8997 - val_loss: 3236488868.8219\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1445272320.2194 - val_loss: 3151867181.5890\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1448726323.8800 - val_loss: 3129861691.6164\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1444694799.0814 - val_loss: 3184161118.6849\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1441820524.3668 - val_loss: 3201284160.8767\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1444807998.5741 - val_loss: 3143746782.6849\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1441310125.6829 - val_loss: 3185047536.2192\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1436894381.1894 - val_loss: 3125299704.9863\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1440056680.4730 - val_loss: 3172982231.6712\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1438581463.6367 - val_loss: 3212501116.4932\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1431788792.8158 - val_loss: 3129181317.2603\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1438056743.3762 - val_loss: 3123997243.6164\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1428388586.1731 - val_loss: 3216173487.3425\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1429461352.5278 - val_loss: 3115027247.3425\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1438753438.6015 - val_loss: 3166862371.0685\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1427598430.6564 - val_loss: 3152806924.2740\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1429185400.9803 - val_loss: 3148019068.4932\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1431519465.1859 - val_loss: 3119732390.5753\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1441836262.3342 - val_loss: 3086973923.9452\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1420599451.0643 - val_loss: 3214283218.4110\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1422123318.1834 - val_loss: 3119011876.8219\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1421013158.7181 - val_loss: 3123354688.8767\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1425210077.2305 - val_loss: 3094355014.1370\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1430701472.5758 - val_loss: 3144101454.9041\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1420990268.7644 - val_loss: 3125365507.5068\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1415673226.4747 - val_loss: 3117257035.3973\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1413499840.6033 - val_loss: 3131148209.0959\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1414287335.1568 - val_loss: 3151103903.5616\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1410445226.8312 - val_loss: 3162833341.3699\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1411216462.7524 - val_loss: 3115704568.9863\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1408967594.5570 - val_loss: 3227565624.1096\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1417664557.2991 - val_loss: 3155346859.8356\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1412325827.9486 - val_loss: 3121459501.5890\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1414032368.9734 - val_loss: 3254407434.5205\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1408156286.3548 - val_loss: 3089405517.1507\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1407466827.2425 - val_loss: 3110096759.2329\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1423163069.5321 - val_loss: 3061301426.8493\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1398782519.2802 - val_loss: 3228339350.7945\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1407763370.0086 - val_loss: 3123664224.4384\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1404778553.9126 - val_loss: 3122049402.7397\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1402358232.0754 - val_loss: 3097738089.2055\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1395852786.7284 - val_loss: 3121076334.4658\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1392871868.3256 - val_loss: 3106225716.6027\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1396617313.1791 - val_loss: 3104958130.8493\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1402070312.3633 - val_loss: 3217251138.6301\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1396243119.6024 - val_loss: 3119514148.8219\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1392088711.2391 - val_loss: 3108879517.8082\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1393486394.5707 - val_loss: 3202783826.4110\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1407678321.7961 - val_loss: 3086451079.0137\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1394161979.2288 - val_loss: 3044552004.3836\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1383245954.0840 - val_loss: 3163632622.4658\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1394473553.8783 - val_loss: 3175485191.0137\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1392112947.6607 - val_loss: 3116972316.0548\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1381544879.6572 - val_loss: 3075198071.2329\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1387125735.4310 - val_loss: 3122764186.3014\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1388404438.6495 - val_loss: 3135612026.7397\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1380272853.8817 - val_loss: 3126829838.0274\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1385263274.1183 - val_loss: 3088221492.6027\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1379664126.6290 - val_loss: 3028957592.5479\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1385406710.7318 - val_loss: 3115687194.3014\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1374478207.6161 - val_loss: 3038248479.5616\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1381103564.4490 - val_loss: 3060688012.2740\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1376695011.8663 - val_loss: 3045779603.2877\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1371959017.6247 - val_loss: 3077756873.6438\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1382485112.2125 - val_loss: 3040809156.3836\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1375180540.3805 - val_loss: 3061841941.0411\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1381865727.2322 - val_loss: 3099671299.5068\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1375198702.1765 - val_loss: 3029189533.8082\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1371808962.6872 - val_loss: 3086926825.2055\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1365922099.9075 - val_loss: 3010399744.0000\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1371698134.3205 - val_loss: 3078232810.9589\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1361941808.5895 - val_loss: 3045185141.4795\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1363065444.6341 - val_loss: 3083774732.2740\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1363375714.9340 - val_loss: 3082717962.5205\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1362878025.3779 - val_loss: 3060259324.4932\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1361047722.5021 - val_loss: 3080823986.8493\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1365214802.8106 - val_loss: 3106728784.6575\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1362335999.2871 - val_loss: 3081102300.9315\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1357606438.4987 - val_loss: 3061972636.0548\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1359639439.8492 - val_loss: 3090892526.4658\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1353645182.7935 - val_loss: 3084135718.5753\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1354111368.7746 - val_loss: 3073343224.9863\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1354369310.4919 - val_loss: 3113130583.6712\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1354708760.7883 - val_loss: 3015799327.5616\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1362132809.2682 - val_loss: 3126767104.0000\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1357877152.2468 - val_loss: 3057918451.7260\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1352642612.2091 - val_loss: 3166298865.9726\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1360120600.2399 - val_loss: 3050691349.0411\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1346302341.7035 - val_loss: 3017302001.9726\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1341032707.5167 - val_loss: 3101190776.9863\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1343770026.8312 - val_loss: 3016246910.2466\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1345521843.2219 - val_loss: 3070906932.6027\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1345454852.4422 - val_loss: 3087084437.0411\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1347558890.1731 - val_loss: 3026717827.5068\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1336329178.9272 - val_loss: 3026385308.0548\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1333413089.1243 - val_loss: 3061588067.9452\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1334000292.6889 - val_loss: 3001451218.4110\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1343190219.7361 - val_loss: 3088089372.0548\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1337866588.3530 - val_loss: 3053700671.1233\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1338517866.2828 - val_loss: 3043050360.9863\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1340747394.4953 - val_loss: 2988757214.6849\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1330004558.4233 - val_loss: 3081226380.2740\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1332904437.0865 - val_loss: 3025844413.3699\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1328083447.8835 - val_loss: 3135792119.2329\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1337228534.7318 - val_loss: 3028174865.5342\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1330163055.6572 - val_loss: 2984515168.4384\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1349279007.9177 - val_loss: 3102535371.3973\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1341520307.9897 - val_loss: 3098223803.6164\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1325636692.8946 - val_loss: 3060485095.4521\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1325307057.6864 - val_loss: 3037452056.5479\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1324753657.4739 - val_loss: 2951958235.1781\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1316794146.3582 - val_loss: 3109231149.5890\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1324125157.4567 - val_loss: 2990095093.4795\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1332797153.5630 - val_loss: 2964578309.2603\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1320622854.6358 - val_loss: 3006426827.3973\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1328548622.5330 - val_loss: 3057665690.3014\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1322097556.8946 - val_loss: 2994948404.6027\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1316253060.9357 - val_loss: 3069805233.0959\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1321836734.7386 - val_loss: 3024580665.8630\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1310083821.0249 - val_loss: 2984159621.2603\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1312768183.1979 - val_loss: 2956205994.0822\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1316037687.3350 - val_loss: 3075665685.0411\n",
      "Epoch 331/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 75us/step - loss: 1318095657.1585 - val_loss: 3072681147.6164\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1310741673.5698 - val_loss: 2991538752.8767\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1304317506.9066 - val_loss: 3077951677.3699\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1308258751.0677 - val_loss: 3022073233.5342\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1305852619.5716 - val_loss: 2977221335.6712\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1301489061.0180 - val_loss: 3007367178.5205\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1307269222.9923 - val_loss: 2974396563.2877\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 83us/step - loss: 1311471675.5578 - val_loss: 2942342524.4932\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1313529684.9494 - val_loss: 2958171923.2877\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1316290481.0283 - val_loss: 3017989852.9315\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1302944480.9597 - val_loss: 2998342955.8356\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1306564665.9949 - val_loss: 2908773049.8630\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1290980283.5578 - val_loss: 3123779135.1233\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1308742390.5124 - val_loss: 2935492285.3699\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1297373274.5433 - val_loss: 3005975734.3562\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1292808827.4482 - val_loss: 2978460682.5205\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1293763687.2117 - val_loss: 2950951525.6986\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1295543918.9991 - val_loss: 2993230232.5479\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1291041840.2605 - val_loss: 2999319848.3288\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1296490945.7001 - val_loss: 3029567340.7123\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1291783669.4704 - val_loss: 3000805719.6712\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1297742858.6392 - val_loss: 2913414792.7671\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1291499661.4910 - val_loss: 2970454889.2055\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1284413575.5681 - val_loss: 3019385172.1644\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1282621840.0137 - val_loss: 2978971733.9178\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1286484404.8946 - val_loss: 2917488555.8356\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1288201781.7446 - val_loss: 3014019899.6164\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1280858378.0908 - val_loss: 2941398133.4795\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1283020353.5904 - val_loss: 2960164555.3973\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1281955410.4267 - val_loss: 2927155918.9041\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1285965594.1594 - val_loss: 2958851633.0959\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1278120962.8518 - val_loss: 2974425696.4384\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1278688157.1757 - val_loss: 3002447468.7123\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1282968973.2716 - val_loss: 2908233389.5890\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1270675301.3470 - val_loss: 3042337748.1644\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1275843168.0823 - val_loss: 2916194828.2740\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1275324840.4319 - val_loss: 2984403573.4795\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1277021424.4250 - val_loss: 2966258325.0411\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1272731812.1954 - val_loss: 2982504847.7808\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1278813451.0780 - val_loss: 3013950954.9589\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1274804810.5844 - val_loss: 2907470065.9726\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1291251532.9974 - val_loss: 2910986667.8356\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1267836301.7104 - val_loss: 2943999021.5890\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1265071365.7584 - val_loss: 2990992748.7123\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1265105959.4310 - val_loss: 2928455160.9863\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1266203766.3479 - val_loss: 3033253621.4795\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1265334329.6110 - val_loss: 2946811788.2740\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1264504616.7472 - val_loss: 2947762929.9726\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1263734859.0231 - val_loss: 3080744739.0685\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1276775422.0531 - val_loss: 2946494548.1644\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1258152897.3710 - val_loss: 2991523173.6986\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1259881912.5690 - val_loss: 2951391042.6301\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1259461874.3445 - val_loss: 2968980723.7260\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1256595616.4662 - val_loss: 2917681232.6575\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1258324576.4662 - val_loss: 2987782436.8219\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1260789038.5604 - val_loss: 3012164927.1233\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1258888362.7763 - val_loss: 2899332646.5753\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1247820202.4473 - val_loss: 2984323282.4110\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1254597403.7498 - val_loss: 2971140271.3425\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1266001405.7515 - val_loss: 2992339782.1370\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1253539477.9914 - val_loss: 3001430494.6849\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1253713981.4225 - val_loss: 2976922604.7123\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1247123562.8312 - val_loss: 2948899569.9726\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1252074047.9452 - val_loss: 2891783206.5753\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1256808549.8406 - val_loss: 2901894648.9863\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1251228532.2639 - val_loss: 2918974471.0137\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1265526109.1757 - val_loss: 2994700487.8904\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1249477696.9323 - val_loss: 2923473774.4658\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1263228350.1902 - val_loss: 2841196572.0548\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1253857367.0060 - val_loss: 3006037844.1644\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1257952915.9623 - val_loss: 2984407609.8630\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1238504395.4619 - val_loss: 2906402198.7945\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1241806175.0951 - val_loss: 2927156844.7123\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1243983053.4087 - val_loss: 2927357115.6164\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1237128931.4824 - val_loss: 2926511282.8493\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1233665872.1234 - val_loss: 2997228852.6027\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1238511880.0891 - val_loss: 2904200455.0137\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1235477104.9734 - val_loss: 2920015459.9452\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1234618984.7746 - val_loss: 2934325523.2877\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1240989484.4216 - val_loss: 3020602385.5342\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1236627842.3033 - val_loss: 2869569036.2740\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1226983362.6324 - val_loss: 3001148559.7808\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1224360613.1551 - val_loss: 2843035321.8630\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1249337101.1620 - val_loss: 2879421618.8493\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1232299887.9314 - val_loss: 2968840810.9589\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1232177238.3205 - val_loss: 3011360599.6712\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1232062966.0189 - val_loss: 2900132120.5479\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1226646518.8963 - val_loss: 2925581280.4384\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1227932505.0626 - val_loss: 2856132620.2740\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1231789232.5895 - val_loss: 2927484873.6438\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1219139068.9563 - val_loss: 2877281948.0548\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1224683678.2725 - val_loss: 2996278755.9452\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1229758011.5578 - val_loss: 2897336609.3151\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1214978020.7986 - val_loss: 2977064635.6164\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1223068926.4507 - val_loss: 2916446541.1507\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1221659319.0608 - val_loss: 2912431905.3151\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1217538875.2562 - val_loss: 2869864540.9315\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1217142348.9152 - val_loss: 2930624545.3151\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1222312499.9897 - val_loss: 2867450592.4384\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1209137791.8903 - val_loss: 2937896367.3425\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1215404177.8235 - val_loss: 2858437553.0959\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1218568101.6761 - val_loss: 2907695489.7534\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1216943899.8595 - val_loss: 2931543352.1096\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1219653819.9966 - val_loss: 2864201936.6575\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1224183054.5330 - val_loss: 2963660305.5342\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1209737774.9169 - val_loss: 2867739323.6164\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1219377380.8809 - val_loss: 2924813738.0822\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1221337511.5407 - val_loss: 2903357704.7671\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1204105478.4713 - val_loss: 2853729185.3151\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1218430043.1465 - val_loss: 2856052581.6986\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1211704926.3273 - val_loss: 2870568556.7123\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1209801040.0411 - val_loss: 2869554891.3973\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1203635385.9126 - val_loss: 2876667469.1507\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1199343002.5433 - val_loss: 2954341232.2192\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1216482433.9743 - val_loss: 2825529194.9589\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1199646071.3350 - val_loss: 2913399094.3562\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1210088481.2888 - val_loss: 2925798184.3288\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1199240831.1774 - val_loss: 2845141917.8082\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1199894547.7155 - val_loss: 2878760011.3973\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1195495279.7121 - val_loss: 2953202502.1370\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1200559536.6992 - val_loss: 2889595114.9589\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1197830189.2442 - val_loss: 2844562000.6575\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1189756686.9169 - val_loss: 2963656702.2466\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1199854526.4096 - val_loss: 2889451176.3288\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1191452364.0103 - val_loss: 2902736387.5068\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1190887908.1954 - val_loss: 2897406898.8493\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1196029999.8766 - val_loss: 2829632066.6301\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1197924567.9657 - val_loss: 2974215927.2329\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1198655793.3573 - val_loss: 2865318622.6849\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1185361183.9314 - val_loss: 2905817503.5616\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1193657399.2528 - val_loss: 2888334469.2603\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1184990142.3548 - val_loss: 2863090505.6438\n",
      "Epoch 463/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 78us/step - loss: 1188486239.4790 - val_loss: 2842957252.3836\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1188595814.2245 - val_loss: 2892383512.5479\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1190590372.5244 - val_loss: 2827251631.3425\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1198473161.7069 - val_loss: 2877694178.1918\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1182912518.0326 - val_loss: 2909647379.2877\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1181008338.1525 - val_loss: 2894570378.5205\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1180401603.0163 - val_loss: 2903558436.8219\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1187769878.9237 - val_loss: 2938208680.3288\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1170512352.6855 - val_loss: 2805979788.2740\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1187661252.0583 - val_loss: 2866597125.2603\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1173947475.3042 - val_loss: 2886037191.8904\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1167690929.0283 - val_loss: 2787052300.2740\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1190646753.1243 - val_loss: 2852057607.0137\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1175442687.8081 - val_loss: 2914026687.1233\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1173763821.4636 - val_loss: 2841390635.8356\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1172965924.4970 - val_loss: 2829201697.3151\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1171807578.5159 - val_loss: 2849873688.5479\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1168474988.3668 - val_loss: 2839483623.4521\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1177092422.4987 - val_loss: 2769246840.9863\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1206495235.0163 - val_loss: 2831547923.2877\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1170958137.0900 - val_loss: 2852846818.1918\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1170009483.6264 - val_loss: 2905960951.2329\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1168014905.3368 - val_loss: 2824092203.8356\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1160493215.6984 - val_loss: 2959308070.5753\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1175736961.6452 - val_loss: 2825483765.4795\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1178672316.4901 - val_loss: 2936679169.7534\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1169963932.7095 - val_loss: 2838038087.8904\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1166746205.2853 - val_loss: 2896109929.2055\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1162822181.2922 - val_loss: 2814668561.5342\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1156284524.8877 - val_loss: 2876832206.9041\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1179239352.5416 - val_loss: 2936843397.2603\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1160361078.4850 - val_loss: 2809424945.0959\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1153918058.1183 - val_loss: 2836176119.2329\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1157276527.2734 - val_loss: 2823545093.2603\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1159490228.2091 - val_loss: 2866422954.0822\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1159725948.1611 - val_loss: 2816017116.9315\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1158251411.9623 - val_loss: 2893251059.7260\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1163715978.4199 - val_loss: 2830939967.1233\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_5000E_Adam_128.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_5000E_Adam_128.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 512)               169472    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 432,641\n",
      "Trainable params: 432,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_500E_1H = Sequential()\n",
    "NN_500E_1H.add(Dense(512,input_dim = 330,activation = 'relu'))\n",
    "NN_500E_1H.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_1H.add(Dense(1))\n",
    "NN_500E_1H.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 274us/step - loss: 37315028674.3582 - val_loss: 37191868359.8904\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 34670875241.7344 - val_loss: 34077835656.7671\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 30991707826.5638 - val_loss: 29510343750.1370\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 181us/step - loss: 26036271552.3839 - val_loss: 23620228306.4110\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 20506454550.3753 - val_loss: 17563180817.5342\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 16197170727.9246 - val_loss: 13097517168.2192\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 13490555313.4670 - val_loss: 10542575125.0411\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 11950503666.6187 - val_loss: 9014504882.8493\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 10914384783.6847 - val_loss: 7763919468.7123\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 10067454718.4644 - val_loss: 7220150450.8493\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 9136653946.1320 - val_loss: 6749834225.9726\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 182us/step - loss: 8366443936.5758 - val_loss: 6327585886.6849\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 180us/step - loss: 7717556697.8303 - val_loss: 6000714986.9589\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 7186532844.9152 - val_loss: 5706583201.3151\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 6515859842.7421 - val_loss: 5414860233.6438\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 5978292068.9083 - val_loss: 5178900264.3288\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 5542993590.0737 - val_loss: 4983877091.9452\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 5032659067.4207 - val_loss: 4799289342.2466\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 4665423896.8980 - val_loss: 4645329190.5753\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 4353698071.9109 - val_loss: 4495737047.6712\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 4054433321.0214 - val_loss: 4392187577.8630\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 3749640709.5938 - val_loss: 4272741889.7534\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 3536665930.6941 - val_loss: 4174349447.0137\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 3297821516.3393 - val_loss: 4096969671.8904\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 3110048505.9674 - val_loss: 4052673525.4795\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 2935378196.4010 - val_loss: 3961064462.0274\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 2779042569.0488 - val_loss: 3905559702.7945\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 180us/step - loss: 2652014658.9066 - val_loss: 3842299432.3288\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 2531300596.7027 - val_loss: 3798827141.2603\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 2430988893.7241 - val_loss: 3745671564.2740\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 2338622697.1859 - val_loss: 3720047421.3699\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 2260526976.9871 - val_loss: 3665621954.6301\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 2187125384.8843 - val_loss: 3631143564.2740\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 2132022882.8243 - val_loss: 3617098813.3699\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 2076032572.4353 - val_loss: 3576360938.9589\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 2024760738.1114 - val_loss: 3555087389.8082\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1986936308.3736 - val_loss: 3552991342.4658\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1949047595.7635 - val_loss: 3503962915.0685\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 182us/step - loss: 1917269984.5210 - val_loss: 3509827701.4795\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1887271394.7147 - val_loss: 3503179740.9315\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1862797180.8192 - val_loss: 3470618148.8219\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1838407799.1157 - val_loss: 3456655587.9452\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1821753861.9229 - val_loss: 3451772394.9589\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1806544170.7763 - val_loss: 3431800449.7534\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1787762029.2442 - val_loss: 3431017857.7534\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1772413779.3042 - val_loss: 3415600562.8493\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1758605655.5270 - val_loss: 3421956329.2055\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1754524641.2888 - val_loss: 3403857846.3562\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1743829941.3059 - val_loss: 3417888590.9041\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1733097519.4927 - val_loss: 3393846142.2466\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1722630336.8226 - val_loss: 3380560606.6849\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1714408404.2365 - val_loss: 3376155770.7397\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1709978285.9023 - val_loss: 3375490689.7534\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1704343698.6461 - val_loss: 3366487829.0411\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1702087523.7018 - val_loss: 3356079570.4110\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1691812845.7378 - val_loss: 3360359315.2877\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1685336782.4781 - val_loss: 3375123894.3562\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1686588384.9049 - val_loss: 3359355602.4110\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1681345133.0249 - val_loss: 3333076779.8356\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1675188586.1731 - val_loss: 3370475821.5890\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1667915564.4764 - val_loss: 3356171025.5342\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1667027711.8903 - val_loss: 3388621645.1507\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1657937805.7104 - val_loss: 3342111945.6438\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1663353738.2005 - val_loss: 3348182754.1918\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1653331115.1054 - val_loss: 3366345747.2877\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1655902098.5364 - val_loss: 3379900068.8219\n",
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1648449816.3496 - val_loss: 3355527729.0959\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1643874144.0823 - val_loss: 3337687820.2740\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1640827852.7781 - val_loss: 3353097636.8219\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1639270160.3976 - val_loss: 3337490688.0000\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1634624033.5630 - val_loss: 3343139952.2192\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1631566120.5827 - val_loss: 3341120948.6027\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1635436093.4225 - val_loss: 3305758039.6712\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1622641392.2057 - val_loss: 3348590930.4110\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1625013358.5604 - val_loss: 3349232969.6438\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1622145831.3762 - val_loss: 3320904789.9178\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1617142902.3479 - val_loss: 3345644933.2603\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1614403354.1045 - val_loss: 3372705511.4521\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1612069741.7378 - val_loss: 3293882510.0274\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1608827022.4781 - val_loss: 3325956581.6986\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1607301779.5236 - val_loss: 3352520817.9726\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1604683141.8132 - val_loss: 3338757952.8767\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1604178378.1183 - val_loss: 3336159556.3836\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1599913849.7481 - val_loss: 3297213664.4384\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1594738597.1825 - val_loss: 3366801141.4795\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1591738433.1517 - val_loss: 3329352926.6849\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1591458210.8792 - val_loss: 3318909029.6986\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1590982046.3822 - val_loss: 3308497811.2877\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1588919537.9057 - val_loss: 3323234640.6575\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1589014372.7301 - val_loss: 3307856873.2055\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1581768065.7549 - val_loss: 3309634752.8767\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1584813957.7035 - val_loss: 3298508785.9726\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1579001397.8543 - val_loss: 3272298894.0274\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1572653457.8783 - val_loss: 3333281609.6438\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1574142408.7198 - val_loss: 3285592177.9726\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1576762191.9589 - val_loss: 3314581454.9041\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1572271445.0591 - val_loss: 3342638886.5753\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1564947625.2956 - val_loss: 3285431837.8082\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1565575662.2314 - val_loss: 3279120340.1644\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1560303332.5793 - val_loss: 3278397857.3151\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1567668225.2065 - val_loss: 3365976786.4110\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1557275402.5296 - val_loss: 3265063264.4384\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1558877992.6924 - val_loss: 3354253952.0000\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1552700832.3565 - val_loss: 3266758706.8493\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1553089813.0591 - val_loss: 3320114624.8767\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1546484455.9794 - val_loss: 3285040417.3151\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1546580004.8535 - val_loss: 3326570366.2466\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1544759771.4207 - val_loss: 3273808839.8904\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1549289305.0626 - val_loss: 3277493314.6301\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1539036359.8423 - val_loss: 3260876477.3699\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1542860901.2374 - val_loss: 3282846078.2466\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1534709017.4464 - val_loss: 3269854865.5342\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1534751264.8500 - val_loss: 3296201457.9726\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1528887551.3419 - val_loss: 3261128533.9178\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1528385593.4739 - val_loss: 3274377991.0137\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1533527148.8055 - val_loss: 3271168042.0822\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1522830240.1919 - val_loss: 3245230220.2740\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1523892837.2374 - val_loss: 3237864956.4932\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1522261966.0394 - val_loss: 3284612741.2603\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1522785384.0891 - val_loss: 3262917102.4658\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1519165144.1165 - val_loss: 3298911586.1918\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1529299856.2879 - val_loss: 3278141496.1096\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1522754574.2588 - val_loss: 3263848749.5890\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1513285360.0960 - val_loss: 3278229623.2329\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1514650125.2716 - val_loss: 3201067363.9452\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1511519014.7181 - val_loss: 3299334919.0137\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1509639659.4619 - val_loss: 3265399362.6301\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1504056791.2528 - val_loss: 3230316549.2603\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1500614732.2296 - val_loss: 3269410938.7397\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1498183694.9169 - val_loss: 3235858393.4247\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1500985989.1003 - val_loss: 3227157684.6027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1499194769.6590 - val_loss: 3228848405.0411\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1494834233.9126 - val_loss: 3203703075.0685\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1493585565.2853 - val_loss: 3217558794.5205\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1496104212.6204 - val_loss: 3188617789.3699\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1494191274.8312 - val_loss: 3298419843.5068\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1487912581.0454 - val_loss: 3203506900.1644\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1495450916.8535 - val_loss: 3238017388.7123\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1487215100.0514 - val_loss: 3229990629.6986\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1484912450.1937 - val_loss: 3185915483.1781\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1481396838.9374 - val_loss: 3199070316.7123\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1480322579.9349 - val_loss: 3209328999.4521\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1477870024.2811 - val_loss: 3218827420.0548\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1476194067.0848 - val_loss: 3214861838.0274\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1476328917.3333 - val_loss: 3222851275.3973\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1473758148.3325 - val_loss: 3204257483.3973\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1473763765.9092 - val_loss: 3167591762.4110\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1468890786.3719 - val_loss: 3249749333.9178\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1466296010.4747 - val_loss: 3184588740.3836\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1479339953.0831 - val_loss: 3257148735.1233\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1459738126.9717 - val_loss: 3155807203.9452\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1468020457.8989 - val_loss: 3230487455.5616\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1467576966.5810 - val_loss: 3208534713.8630\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1459418274.2211 - val_loss: 3203149312.0000\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1467769116.6272 - val_loss: 3221557642.5205\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1458138971.2836 - val_loss: 3126444642.1918\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1457023278.3136 - val_loss: 3189015031.2329\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1452216593.3299 - val_loss: 3221237842.4110\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1463663629.9297 - val_loss: 3234237727.5616\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1456292125.1757 - val_loss: 3201688453.2603\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1448378137.9126 - val_loss: 3173139971.5068\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1450147078.8003 - val_loss: 3207035353.4247\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1449656834.3582 - val_loss: 3195080924.9315\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1446053675.9829 - val_loss: 3176446840.9863\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1448684265.8440 - val_loss: 3207829398.7945\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1441257053.0111 - val_loss: 3134001546.5205\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1442477649.6041 - val_loss: 3180028721.0959\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1438727716.0857 - val_loss: 3181341624.1096\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1437855169.5904 - val_loss: 3183206371.9452\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1436665262.8346 - val_loss: 3190770396.9315\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1438779932.7918 - val_loss: 3175425834.0822\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1437544903.6230 - val_loss: 3147711533.5890\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1431401924.8535 - val_loss: 3160866731.8356\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1430647362.7421 - val_loss: 3162103627.3973\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1438471647.8629 - val_loss: 3080320263.0137\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1427229249.3162 - val_loss: 3151083239.4521\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1429403859.0848 - val_loss: 3099396525.5890\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1434836707.4824 - val_loss: 3152029385.6438\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1427304010.8038 - val_loss: 3138980679.8904\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1430047138.6598 - val_loss: 3183665506.1918\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1424476048.1234 - val_loss: 3104112087.6712\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1418332255.8629 - val_loss: 3247054963.7260\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1420282364.0514 - val_loss: 3087104087.6712\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1421974227.2494 - val_loss: 3136672613.6986\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1415499252.9220 - val_loss: 3144995750.5753\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1416346929.0831 - val_loss: 3119313169.5342\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1413025078.6221 - val_loss: 3118817448.3288\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1409323811.7566 - val_loss: 3143995865.4247\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1408828644.5793 - val_loss: 3125622117.6986\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1416309831.0746 - val_loss: 3177310183.4521\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1409511477.0865 - val_loss: 3128015984.2192\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1405851288.4045 - val_loss: 3170470042.3014\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1404473893.7309 - val_loss: 3084152369.0959\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1406922148.5518 - val_loss: 3083450050.6301\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1403853702.1971 - val_loss: 3111937008.2192\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1400905794.6324 - val_loss: 3103091245.5890\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1396826188.5587 - val_loss: 3098127440.6575\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1397424325.6487 - val_loss: 3104948420.3836\n",
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1394368216.9529 - val_loss: 3130412942.0274\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1399225521.6864 - val_loss: 3037778556.4932\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1401163106.7969 - val_loss: 3152657278.2466\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1395044963.8115 - val_loss: 3071270368.4384\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1391320107.8732 - val_loss: 3112499575.2329\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1388457493.6898 - val_loss: 3100581118.2466\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1389107352.6787 - val_loss: 3101615866.7397\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1387113397.1962 - val_loss: 3123269554.8493\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1388549245.8063 - val_loss: 3093736062.2466\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1389706529.5630 - val_loss: 3162695736.1096\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1386238145.4259 - val_loss: 3087381155.0685\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1382074242.3033 - val_loss: 3093986693.2603\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1381908823.4173 - val_loss: 3075597987.0685\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1387165900.0925 - val_loss: 3031338225.9726\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1395195432.2536 - val_loss: 3107067234.1918\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1380676580.8535 - val_loss: 3108179841.7534\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1391910216.2262 - val_loss: 3155716341.4795\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1375506934.5398 - val_loss: 3053554724.8219\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1386063965.3402 - val_loss: 3121331087.7808\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1374614086.5261 - val_loss: 3055813235.7260\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1378252734.2999 - val_loss: 3108765056.0000\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1375893148.4627 - val_loss: 3070547697.9726\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1380307815.1020 - val_loss: 3127550990.0274\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1367932035.1808 - val_loss: 3066910870.7945\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1365885398.6495 - val_loss: 3068394497.7534\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1378019126.0463 - val_loss: 3034665966.4658\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1373355156.4010 - val_loss: 3073423796.6027\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1364225923.1808 - val_loss: 3095787670.7945\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1367264075.2973 - val_loss: 3144637317.2603\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1364112170.0086 - val_loss: 3010497653.4795\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1369643733.2237 - val_loss: 3005151482.7397\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1359913242.5433 - val_loss: 3032340928.8767\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1364319435.5716 - val_loss: 3063132154.7397\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1361290383.4653 - val_loss: 3044027251.7260\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1355220255.2048 - val_loss: 3037863781.6986\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1351072918.0463 - val_loss: 3065345930.5205\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1363206585.8303 - val_loss: 3063245441.7534\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1351144082.3171 - val_loss: 3052489652.6027\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1368458748.2708 - val_loss: 3102289928.7671\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1362870192.8089 - val_loss: 3016290407.4521\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1347241756.6821 - val_loss: 3036717748.6027\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1345427874.3856 - val_loss: 2997531214.9041\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1342691961.4739 - val_loss: 3122720469.9178\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1359942157.8201 - val_loss: 3077138090.0822\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1344189502.6290 - val_loss: 3064088216.5479\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1339096164.8260 - val_loss: 2997879210.0822\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1346550806.9512 - val_loss: 2995009921.7534\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1345517716.8946 - val_loss: 3138842161.0959\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1347399486.0257 - val_loss: 3054075013.2603\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1339446634.5021 - val_loss: 2960078034.4110\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1344987320.2125 - val_loss: 3038697237.0411\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1339969448.3085 - val_loss: 3088044126.6849\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1336182689.5630 - val_loss: 2975944120.1096\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1341513526.5124 - val_loss: 3030476545.7534\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1341409879.4722 - val_loss: 3064799081.2055\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1333578372.9357 - val_loss: 3016648251.6164\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1335303471.1088 - val_loss: 3038118813.8082\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1325362177.8646 - val_loss: 2981165999.3425\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1330154972.5724 - val_loss: 3070005761.7534\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1332356265.3505 - val_loss: 3014303875.5068\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1323182665.6795 - val_loss: 2995376853.9178\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1325399349.3608 - val_loss: 3039482706.4110\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1323853158.4439 - val_loss: 2972400126.2466\n",
      "Epoch 262/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 175us/step - loss: 1331742575.9040 - val_loss: 2983347953.9726\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1322969068.1474 - val_loss: 2999233437.8082\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1325709812.3736 - val_loss: 2948871111.8904\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1319454295.1705 - val_loss: 3027924883.2877\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1319151775.4790 - val_loss: 2978417109.9178\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1316628719.1637 - val_loss: 2983793849.8630\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1324831273.9537 - val_loss: 3020292837.6986\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1319023766.7592 - val_loss: 2923802243.5068\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1314177399.4996 - val_loss: 3019780404.6027\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1317192751.3830 - val_loss: 2940756032.8767\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1317408587.3522 - val_loss: 2965475229.8082\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1311329999.6572 - val_loss: 3033924343.2329\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1313409653.5801 - val_loss: 3009352467.2877\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1312649425.7138 - val_loss: 3073956322.1918\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1320635826.8380 - val_loss: 2988136314.7397\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1304978835.4687 - val_loss: 2914882991.3425\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1310697930.4747 - val_loss: 3021648403.2877\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1306578379.7909 - val_loss: 2992599409.9726\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1304749285.2374 - val_loss: 2997729367.6712\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1305360851.1397 - val_loss: 3054258596.8219\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1302241478.8826 - val_loss: 2973233916.4932\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1301173095.1568 - val_loss: 3034198931.2877\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1296778896.0686 - val_loss: 2893460650.0822\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 182us/step - loss: 1304081349.7584 - val_loss: 3044056584.7671\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 190us/step - loss: 1307546485.8543 - val_loss: 2904137501.8082\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 191us/step - loss: 1301135578.3787 - val_loss: 2934102077.3699\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 186us/step - loss: 1302715198.9854 - val_loss: 2940728409.4247\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1292220500.2365 - val_loss: 2975911737.8630\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1294294835.8800 - val_loss: 3026599834.3014\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1295697770.1183 - val_loss: 2921296426.0822\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1288709267.6332 - val_loss: 2981139771.6164\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1292714257.2202 - val_loss: 2983060436.1644\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1292512649.4327 - val_loss: 2971155313.9726\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1298626690.9614 - val_loss: 2937800774.1370\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1289446185.3505 - val_loss: 2925798617.4247\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1283740500.5656 - val_loss: 2992090240.0000\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1286734162.8106 - val_loss: 2966588396.7123\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1287027812.9083 - val_loss: 2953481577.2055\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1292886082.2485 - val_loss: 2926363100.9315\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1280676051.1397 - val_loss: 2985889136.2192\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1305777941.4979 - val_loss: 3059140937.6438\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1293633796.1680 - val_loss: 2981515888.2192\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1279606451.7704 - val_loss: 2876981006.0274\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1273897293.4636 - val_loss: 3064936339.2877\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1282833342.1354 - val_loss: 2915763689.2055\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1275759232.9871 - val_loss: 2917104559.3425\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1271161642.8312 - val_loss: 2913697653.4795\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1280673440.5758 - val_loss: 2882323215.7808\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1267589126.1971 - val_loss: 2960895763.2877\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1271020445.3950 - val_loss: 2900114602.0822\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1270690873.6932 - val_loss: 2924173790.6849\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1269123571.9349 - val_loss: 2898760111.3425\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1274095231.1225 - val_loss: 2882125105.0959\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1272002406.4439 - val_loss: 2878777275.6164\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1268666860.3668 - val_loss: 2903444699.1781\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1267449707.3796 - val_loss: 2874975317.9178\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1266572173.9846 - val_loss: 2880182734.9041\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1265637773.1620 - val_loss: 2948092503.6712\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1263553088.0548 - val_loss: 2879617700.8219\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1261248178.7832 - val_loss: 2963113410.6301\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1255773775.4105 - val_loss: 2865518763.8356\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1267417911.9109 - val_loss: 2931513121.3151\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1258263824.0686 - val_loss: 2938248986.3014\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1254945253.8406 - val_loss: 2879494678.7945\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1260733119.3419 - val_loss: 2949298361.8630\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1253375998.2451 - val_loss: 2905074081.3151\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1263154684.3805 - val_loss: 2955825062.5753\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1254281836.3668 - val_loss: 2912716538.7397\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1250946054.2519 - val_loss: 2922847789.5890\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1254447664.4799 - val_loss: 2961930769.5342\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1253670352.5073 - val_loss: 2863370872.9863\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1248950893.0249 - val_loss: 2853797747.7260\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1254028187.8046 - val_loss: 3003580763.1781\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1256037022.2725 - val_loss: 2908640815.3425\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1259318683.4756 - val_loss: 2842734609.5342\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1264734909.4773 - val_loss: 2839478536.7671\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1242764779.2699 - val_loss: 2888942840.9863\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1242306391.0883 - val_loss: 2834431638.7945\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1242636591.6024 - val_loss: 2915873255.4521\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1243641610.7489 - val_loss: 2855332581.6986\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1240285475.7566 - val_loss: 2848123176.3288\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1242308943.1911 - val_loss: 2898200674.1918\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1238495101.0386 - val_loss: 2900130461.8082\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1241199578.6530 - val_loss: 2832159284.6027\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1235823598.3136 - val_loss: 2844795453.3699\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1245936585.2682 - val_loss: 2846639542.3562\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1238163026.3719 - val_loss: 2824456528.6575\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1239989174.0737 - val_loss: 2857422416.6575\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1235677960.9940 - val_loss: 2928574348.2740\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1232735735.9931 - val_loss: 2869492679.8904\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1240868395.9829 - val_loss: 2827326483.2877\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1228010216.3085 - val_loss: 2863481084.4932\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1227188903.5955 - val_loss: 2865763142.1370\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1225744499.7155 - val_loss: 2821229688.9863\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1231328016.5347 - val_loss: 2918150119.4521\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1244080364.2571 - val_loss: 2813549192.7671\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1235131676.9563 - val_loss: 2853226131.2877\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1222747815.5955 - val_loss: 2837830068.6027\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1218564204.7506 - val_loss: 2907235177.2055\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1230142663.8423 - val_loss: 2840624029.8082\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1216783149.7378 - val_loss: 2835447369.6438\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1220425965.0249 - val_loss: 2807666619.6164\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1220695535.3282 - val_loss: 2838438296.5479\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1220780544.6033 - val_loss: 2878642477.5890\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1217090689.3710 - val_loss: 2855982250.0822\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1213924091.1740 - val_loss: 2843535907.0685\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1219327932.9837 - val_loss: 2774986350.4658\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1218864004.4970 - val_loss: 2793300429.1507\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1212229949.6967 - val_loss: 2831930737.9726\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1211667747.0985 - val_loss: 2852982601.6438\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1213747994.9272 - val_loss: 2805749505.7534\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1207991295.5613 - val_loss: 2794501393.5342\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1205371672.1302 - val_loss: 2821225438.6849\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1204352147.0300 - val_loss: 2906593716.6027\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1208957601.1243 - val_loss: 2819471949.1507\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1203188959.4790 - val_loss: 2779140890.3014\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1219976639.7258 - val_loss: 2748292750.0274\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1216865482.5844 - val_loss: 2767301097.2055\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1203462797.0523 - val_loss: 2845217637.6986\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1198210961.2202 - val_loss: 2815441457.0959\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1198454603.4619 - val_loss: 2870674684.4932\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1198387118.6153 - val_loss: 2794913521.9726\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1199062104.4319 - val_loss: 2783874996.6027\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1199923650.4679 - val_loss: 2854769905.9726\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1192066503.7875 - val_loss: 2778701944.9863\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1194436834.6050 - val_loss: 2785146143.5616\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1210357915.2014 - val_loss: 2770981518.0274\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1196198321.7412 - val_loss: 2809237987.9452\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1190510725.4841 - val_loss: 2769142254.4658\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1195550978.4679 - val_loss: 2754038622.6849\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 167us/step - loss: 1194996301.6555 - val_loss: 2860333389.1507\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1187779177.9537 - val_loss: 2812116793.8630\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1186398046.2725 - val_loss: 2826688878.4658\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1189064424.7198 - val_loss: 2785884032.0000\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1188443536.5621 - val_loss: 2836600046.4658\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1185510655.2322 - val_loss: 2776230901.4795\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1187375549.3128 - val_loss: 2809821203.2877\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1180257150.9580 - val_loss: 2774626161.9726\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1181016472.2399 - val_loss: 2853340526.4658\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1180045217.2888 - val_loss: 2741655937.7534\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1181592951.2802 - val_loss: 2819270608.6575\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1175301321.3231 - val_loss: 2775820109.1507\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1180646243.5921 - val_loss: 2746420702.6849\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1177251809.6178 - val_loss: 2782180085.4795\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1171888328.3907 - val_loss: 2781179200.8767\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1174505434.5433 - val_loss: 2800674368.8767\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1177882332.7918 - val_loss: 2814586997.4795\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1173492407.3762 - val_loss: 2766070936.5479\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1179875054.5604 - val_loss: 2744561909.4795\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1174554510.1491 - val_loss: 2736749667.9452\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1171589981.0111 - val_loss: 2745053953.7534\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1166020395.8458 - val_loss: 2809257733.2603\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1166949792.9049 - val_loss: 2779700383.5616\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1174294388.8123 - val_loss: 2768061720.5479\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1167368243.9349 - val_loss: 2892913295.7808\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 164us/step - loss: 1172740965.9503 - val_loss: 2753840739.9452\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1165217020.1611 - val_loss: 2703913491.2877\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1173349306.7901 - val_loss: 2793832118.3562\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1162344795.3659 - val_loss: 2772192119.2329\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1163219127.9383 - val_loss: 2833508399.3425\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1167402177.4533 - val_loss: 2785251086.0274\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1157276278.1285 - val_loss: 2774923884.7123\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1160454854.9649 - val_loss: 2750381678.4658\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1153314921.6247 - val_loss: 2761940039.8904\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1162512417.1791 - val_loss: 2757364306.4110\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1154348778.0360 - val_loss: 2735159820.2740\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1156092964.2502 - val_loss: 2756811258.7397\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1160378365.3676 - val_loss: 2696211238.5753\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1164571595.9829 - val_loss: 2830658128.6575\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1158279900.3805 - val_loss: 2781469382.1370\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1148499736.2399 - val_loss: 2717235117.5890\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1150633840.3702 - val_loss: 2773992328.7671\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1146513381.3745 - val_loss: 2717910899.7260\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1146350182.5536 - val_loss: 2709895185.5342\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1152961658.0223 - val_loss: 2772352322.6301\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1145670806.8963 - val_loss: 2758604992.8767\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1142615517.6692 - val_loss: 2734153098.5205\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1157493938.3376 - val_loss: 2800605462.7945\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1138191524.1954 - val_loss: 2688267379.7260\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1146616604.4079 - val_loss: 2715024245.4795\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1150861615.5201 - val_loss: 2837676724.6027\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1145849471.0129 - val_loss: 2718175209.2055\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1145488372.3736 - val_loss: 2652436608.0000\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1164997228.4216 - val_loss: 2775186291.7260\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1135284835.9760 - val_loss: 2751465933.1507\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1137593442.4953 - val_loss: 2735837638.1370\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1132006251.7361 - val_loss: 2708203386.7397\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1138115343.6847 - val_loss: 2683439391.5616\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1139765243.2836 - val_loss: 2728276522.0822\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1132071740.3256 - val_loss: 2718741155.0685\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1130127707.8046 - val_loss: 2712132436.1644\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1145174902.0463 - val_loss: 2658424153.4247\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1134574207.0129 - val_loss: 2705778723.0685\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1130904981.0043 - val_loss: 2710782251.8356\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1139274133.6624 - val_loss: 2794782509.5890\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1151135686.8003 - val_loss: 2754844763.1781\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1131579318.6221 - val_loss: 2659550974.2466\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1116771861.9914 - val_loss: 2782558399.1233\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1130127745.1517 - val_loss: 2713946674.8493\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1120211946.1731 - val_loss: 2700679653.6986\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1128732453.6761 - val_loss: 2766594328.5479\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1123788964.7986 - val_loss: 2709428238.0274\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1119661451.6812 - val_loss: 2770372464.2192\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1126740613.4841 - val_loss: 2699937313.3151\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1122872063.1225 - val_loss: 2657749944.1096\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1124363109.5664 - val_loss: 2656557343.5616\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1122707196.8740 - val_loss: 2668460640.4384\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1118526203.0643 - val_loss: 2660293842.4110\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1115871453.8338 - val_loss: 2689540097.7534\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1115483877.8955 - val_loss: 2736692207.3425\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1111898191.9589 - val_loss: 2651729849.8630\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1101995607.3076 - val_loss: 2825248943.3425\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1116079575.1979 - val_loss: 2649945166.9041\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1120136125.4773 - val_loss: 2676112843.3973\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1104416788.1817 - val_loss: 2765490617.8630\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1119598747.3111 - val_loss: 2676602434.6301\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1115953851.3933 - val_loss: 2684436091.6164\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1111801332.9220 - val_loss: 2747725692.4932\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1108185286.5261 - val_loss: 2711726090.5205\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1104744101.6213 - val_loss: 2700173410.1918\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1103761732.6615 - val_loss: 2675744471.6712\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1100812274.1799 - val_loss: 2721587943.4521\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1101844595.9349 - val_loss: 2725707306.0822\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1104896594.8655 - val_loss: 2631405182.2466\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1102339366.6084 - val_loss: 2681277388.2740\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1102172249.3642 - val_loss: 2615350475.3973\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1092507479.1842 - val_loss: 2779674129.5342\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1111013321.9263 - val_loss: 2667322356.6027\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1097216608.3016 - val_loss: 2713862548.1644\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1102458905.1174 - val_loss: 2683880463.7808\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1099997836.5587 - val_loss: 2662023864.1096\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1097724972.0925 - val_loss: 2626973273.4247\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1087307654.2519 - val_loss: 2726045308.4932\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1097397482.9409 - val_loss: 2700623652.8219\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1094706161.6315 - val_loss: 2691211351.6712\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1094648169.7344 - val_loss: 2704708844.7123\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1089752727.3625 - val_loss: 2664464984.5479\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1093873680.2879 - val_loss: 2682409014.3562\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1085849150.7386 - val_loss: 2679263491.5068\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_500E_1H.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_500E_1H.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_14 (Dense)             (None, 512)               207872    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 733,697\n",
      "Trainable params: 733,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_500E_Adam_LReLU = Sequential()\n",
    "NN_500E_Adam_LReLU.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam_LReLU.add(LeakyReLU(alpha=0.1))\n",
    "NN_500E_Adam_LReLU.add(Dense(512))\n",
    "NN_500E_Adam_LReLU.add(LeakyReLU(alpha=0.1))\n",
    "NN_500E_Adam_LReLU.add(Dense(512))\n",
    "NN_500E_Adam_LReLU.add(LeakyReLU(alpha=0.1))\n",
    "NN_500E_Adam_LReLU.add(Dense(1))\n",
    "NN_500E_Adam_LReLU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 38245584896.0000 - val_loss: 37254803456.0000\n",
      "Epoch 2/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37333716992.0000 - val_loss: 36323143680.0000\n",
      "Epoch 3/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36372570112.0000 - val_loss: 35196166144.0000\n",
      "Epoch 4/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35220848640.0000 - val_loss: 33773645824.0000\n",
      "Epoch 5/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 33784530944.0000 - val_loss: 31998197760.0000\n",
      "Epoch 6/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 32020578304.0000 - val_loss: 29846003712.0000\n",
      "Epoch 7/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 29928282112.0000 - val_loss: 27285381120.0000\n",
      "Epoch 8/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 27512322048.0000 - val_loss: 24343758848.0000\n",
      "Epoch 9/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 24852373504.0000 - val_loss: 21087752192.0000\n",
      "Epoch 10/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 22087641088.0000 - val_loss: 17639696384.0000\n",
      "Epoch 11/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 19437475840.0000 - val_loss: 14210404352.0000\n",
      "Epoch 12/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 17230594048.0000 - val_loss: 11123844096.0000\n",
      "Epoch 13/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 15899992064.0000 - val_loss: 8794222592.0000\n",
      "Epoch 14/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 15846248448.0000 - val_loss: 7537210880.0000\n",
      "Epoch 15/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 16920820736.0000 - val_loss: 7096946176.0000\n",
      "Epoch 16/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 18037354496.0000 - val_loss: 6923218432.0000\n",
      "Epoch 17/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 18246801408.0000 - val_loss: 6767812608.0000\n",
      "Epoch 18/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 17492234240.0000 - val_loss: 6673272832.0000\n",
      "Epoch 19/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 16206667776.0000 - val_loss: 6753823744.0000\n",
      "Epoch 20/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 14864447488.0000 - val_loss: 7068017152.0000\n",
      "Epoch 21/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 13780614144.0000 - val_loss: 7584955392.0000\n",
      "Epoch 22/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 13074006016.0000 - val_loss: 8205256192.0000\n",
      "Epoch 23/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 12706995200.0000 - val_loss: 8805063680.0000\n",
      "Epoch 24/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 12556496896.0000 - val_loss: 9272997888.0000\n",
      "Epoch 25/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 12485606400.0000 - val_loss: 9531101184.0000\n",
      "Epoch 26/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 12381608960.0000 - val_loss: 9540639744.0000\n",
      "Epoch 27/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 12171963392.0000 - val_loss: 9295109120.0000\n",
      "Epoch 28/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 11822780416.0000 - val_loss: 8825161728.0000\n",
      "Epoch 29/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 11347643392.0000 - val_loss: 8173995520.0000\n",
      "Epoch 30/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 10772825088.0000 - val_loss: 7405380096.0000\n",
      "Epoch 31/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 10149587968.0000 - val_loss: 6597360128.0000\n",
      "Epoch 32/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 9540234240.0000 - val_loss: 5829893632.0000\n",
      "Epoch 33/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 9003666432.0000 - val_loss: 5170713600.0000\n",
      "Epoch 34/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 8577271296.0000 - val_loss: 4658115072.0000\n",
      "Epoch 35/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 8252436480.0000 - val_loss: 4292124160.0000\n",
      "Epoch 36/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 7981312000.0000 - val_loss: 4039406592.0000\n",
      "Epoch 37/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 7692416512.0000 - val_loss: 3854928896.0000\n",
      "Epoch 38/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 7328158720.0000 - val_loss: 3704931584.0000\n",
      "Epoch 39/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 6870992896.0000 - val_loss: 3578017024.0000\n",
      "Epoch 40/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 6343961600.0000 - val_loss: 3479609344.0000\n",
      "Epoch 41/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 5802855936.0000 - val_loss: 3416891392.0000\n",
      "Epoch 42/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 5301803520.0000 - val_loss: 3382376192.0000\n",
      "Epoch 43/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 4876340736.0000 - val_loss: 3354803968.0000\n",
      "Epoch 44/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 4512703488.0000 - val_loss: 3303295744.0000\n",
      "Epoch 45/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 4187160320.0000 - val_loss: 3200834304.0000\n",
      "Epoch 46/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 3866463744.0000 - val_loss: 3039404032.0000\n",
      "Epoch 47/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 3531767552.0000 - val_loss: 2834339072.0000\n",
      "Epoch 48/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 3189538816.0000 - val_loss: 2624199936.0000\n",
      "Epoch 49/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2867101184.0000 - val_loss: 2460362240.0000\n",
      "Epoch 50/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2607122688.0000 - val_loss: 2382590976.0000\n",
      "Epoch 51/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2445614592.0000 - val_loss: 2395075072.0000\n",
      "Epoch 52/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2390119424.0000 - val_loss: 2457013760.0000\n",
      "Epoch 53/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2410978560.0000 - val_loss: 2509463040.0000\n",
      "Epoch 54/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2461100032.0000 - val_loss: 2519219200.0000\n",
      "Epoch 55/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2512960512.0000 - val_loss: 2496525824.0000\n",
      "Epoch 56/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2562505216.0000 - val_loss: 2470545920.0000\n",
      "Epoch 57/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2609594880.0000 - val_loss: 2454800640.0000\n",
      "Epoch 58/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2636296448.0000 - val_loss: 2443104000.0000\n",
      "Epoch 59/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2626064128.0000 - val_loss: 2427904000.0000\n",
      "Epoch 60/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2582484480.0000 - val_loss: 2410084608.0000\n",
      "Epoch 61/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2524137472.0000 - val_loss: 2393604096.0000\n",
      "Epoch 62/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2464287744.0000 - val_loss: 2378059520.0000\n",
      "Epoch 63/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2409921024.0000 - val_loss: 2359163904.0000\n",
      "Epoch 64/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2361489664.0000 - val_loss: 2332589056.0000\n",
      "Epoch 65/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2316348928.0000 - val_loss: 2298953984.0000\n",
      "Epoch 66/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 2274261248.0000 - val_loss: 2263546368.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2237722624.0000 - val_loss: 2232261888.0000\n",
      "Epoch 68/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2209837312.0000 - val_loss: 2208127232.0000\n",
      "Epoch 69/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2191059456.0000 - val_loss: 2190148608.0000\n",
      "Epoch 70/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 2178732032.0000 - val_loss: 2175620352.0000\n",
      "Epoch 71/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2169381376.0000 - val_loss: 2162599680.0000\n",
      "Epoch 72/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2161066240.0000 - val_loss: 2150736128.0000\n",
      "Epoch 73/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2153761024.0000 - val_loss: 2140318208.0000\n",
      "Epoch 74/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2147771392.0000 - val_loss: 2131057280.0000\n",
      "Epoch 75/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2142540928.0000 - val_loss: 2121916032.0000\n",
      "Epoch 76/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2136465024.0000 - val_loss: 2111934848.0000\n",
      "Epoch 77/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2128172800.0000 - val_loss: 2100984960.0000\n",
      "Epoch 78/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2117349248.0000 - val_loss: 2089803904.0000\n",
      "Epoch 79/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2104902400.0000 - val_loss: 2079029760.0000\n",
      "Epoch 80/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2091864704.0000 - val_loss: 2068790400.0000\n",
      "Epoch 81/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2078539776.0000 - val_loss: 2058811136.0000\n",
      "Epoch 82/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2064672000.0000 - val_loss: 2048974720.0000\n",
      "Epoch 83/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2050251008.0000 - val_loss: 2039611904.0000\n",
      "Epoch 84/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2035826048.0000 - val_loss: 2031198976.0000\n",
      "Epoch 85/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2021930240.0000 - val_loss: 2023852416.0000\n",
      "Epoch 86/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 2009234688.0000 - val_loss: 2016829312.0000\n",
      "Epoch 87/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1997475968.0000 - val_loss: 2009220864.0000\n",
      "Epoch 88/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1986250240.0000 - val_loss: 2000583296.0000\n",
      "Epoch 89/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1975387904.0000 - val_loss: 1991190400.0000\n",
      "Epoch 90/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 1965048960.0000 - val_loss: 1981682560.0000\n",
      "Epoch 91/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1955470592.0000 - val_loss: 1972597632.0000\n",
      "Epoch 92/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1946555008.0000 - val_loss: 1964131456.0000\n",
      "Epoch 93/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1937910144.0000 - val_loss: 1956345856.0000\n",
      "Epoch 94/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1929195392.0000 - val_loss: 1949208576.0000\n",
      "Epoch 95/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1920408448.0000 - val_loss: 1942495616.0000\n",
      "Epoch 96/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1911693440.0000 - val_loss: 1935679872.0000\n",
      "Epoch 97/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1903068416.0000 - val_loss: 1928131968.0000\n",
      "Epoch 98/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1894366464.0000 - val_loss: 1919520896.0000\n",
      "Epoch 99/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1885452032.0000 - val_loss: 1910044288.0000\n",
      "Epoch 100/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1876409472.0000 - val_loss: 1900254080.0000\n",
      "Epoch 101/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1867436672.0000 - val_loss: 1890744064.0000\n",
      "Epoch 102/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1858635904.0000 - val_loss: 1881869568.0000\n",
      "Epoch 103/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1849949952.0000 - val_loss: 1873673984.0000\n",
      "Epoch 104/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1841340672.0000 - val_loss: 1866011520.0000\n",
      "Epoch 105/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1832787200.0000 - val_loss: 1858699776.0000\n",
      "Epoch 106/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1824276224.0000 - val_loss: 1851435904.0000\n",
      "Epoch 107/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1815990528.0000 - val_loss: 1843905152.0000\n",
      "Epoch 108/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1808007296.0000 - val_loss: 1835910528.0000\n",
      "Epoch 109/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1800257792.0000 - val_loss: 1827703040.0000\n",
      "Epoch 110/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1792665088.0000 - val_loss: 1819703040.0000\n",
      "Epoch 111/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1785251840.0000 - val_loss: 1812177664.0000\n",
      "Epoch 112/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1777934080.0000 - val_loss: 1805235200.0000\n",
      "Epoch 113/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1770595072.0000 - val_loss: 1798794880.0000\n",
      "Epoch 114/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1763191424.0000 - val_loss: 1792708096.0000\n",
      "Epoch 115/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1755831424.0000 - val_loss: 1786679040.0000\n",
      "Epoch 116/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1748532224.0000 - val_loss: 1780395776.0000\n",
      "Epoch 117/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1741271424.0000 - val_loss: 1773794688.0000\n",
      "Epoch 118/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1734054528.0000 - val_loss: 1767072256.0000\n",
      "Epoch 119/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1726946432.0000 - val_loss: 1760519680.0000\n",
      "Epoch 120/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1719971968.0000 - val_loss: 1754379136.0000\n",
      "Epoch 121/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1713108736.0000 - val_loss: 1748708992.0000\n",
      "Epoch 122/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1706342528.0000 - val_loss: 1743409280.0000\n",
      "Epoch 123/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1699693440.0000 - val_loss: 1738176512.0000\n",
      "Epoch 124/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1693173888.0000 - val_loss: 1732704128.0000\n",
      "Epoch 125/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1686753280.0000 - val_loss: 1726868352.0000\n",
      "Epoch 126/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1680409984.0000 - val_loss: 1720843008.0000\n",
      "Epoch 127/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1674159488.0000 - val_loss: 1714972928.0000\n",
      "Epoch 128/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1668008576.0000 - val_loss: 1709474688.0000\n",
      "Epoch 129/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1661936256.0000 - val_loss: 1704350336.0000\n",
      "Epoch 130/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1655949824.0000 - val_loss: 1699420032.0000\n",
      "Epoch 131/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1650075008.0000 - val_loss: 1694441088.0000\n",
      "Epoch 132/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1644318080.0000 - val_loss: 1689260672.0000\n",
      "Epoch 133/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1638665216.0000 - val_loss: 1683966208.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 134/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1633137408.0000 - val_loss: 1678828032.0000\n",
      "Epoch 135/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1627756544.0000 - val_loss: 1674073472.0000\n",
      "Epoch 136/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1622494720.0000 - val_loss: 1669742976.0000\n",
      "Epoch 137/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1617348992.0000 - val_loss: 1665675648.0000\n",
      "Epoch 138/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1612327168.0000 - val_loss: 1661669632.0000\n",
      "Epoch 139/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1607428608.0000 - val_loss: 1657675648.0000\n",
      "Epoch 140/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1602641024.0000 - val_loss: 1653738880.0000\n",
      "Epoch 141/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1597957120.0000 - val_loss: 1650013568.0000\n",
      "Epoch 142/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1593386624.0000 - val_loss: 1646559360.0000\n",
      "Epoch 143/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1588921984.0000 - val_loss: 1643391616.0000\n",
      "Epoch 144/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1584575872.0000 - val_loss: 1640374528.0000\n",
      "Epoch 145/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1580347392.0000 - val_loss: 1637497344.0000\n",
      "Epoch 146/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1576259200.0000 - val_loss: 1634576128.0000\n",
      "Epoch 147/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1572290560.0000 - val_loss: 1631792384.0000\n",
      "Epoch 148/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1568429952.0000 - val_loss: 1629283072.0000\n",
      "Epoch 149/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1564726784.0000 - val_loss: 1626785152.0000\n",
      "Epoch 150/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1561108608.0000 - val_loss: 1624495488.0000\n",
      "Epoch 151/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1557524736.0000 - val_loss: 1622207488.0000\n",
      "Epoch 152/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1553980928.0000 - val_loss: 1620157696.0000\n",
      "Epoch 153/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1550665600.0000 - val_loss: 1618886144.0000\n",
      "Epoch 154/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1547676672.0000 - val_loss: 1617337344.0000\n",
      "Epoch 155/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1544768000.0000 - val_loss: 1614696704.0000\n",
      "Epoch 156/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1541849856.0000 - val_loss: 1611906432.0000\n",
      "Epoch 157/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1539075840.0000 - val_loss: 1610170880.0000\n",
      "Epoch 158/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1536423040.0000 - val_loss: 1609506304.0000\n",
      "Epoch 159/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1533852160.0000 - val_loss: 1609111936.0000\n",
      "Epoch 160/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1531445760.0000 - val_loss: 1607960320.0000\n",
      "Epoch 161/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1529150080.0000 - val_loss: 1606225280.0000\n",
      "Epoch 162/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1526983680.0000 - val_loss: 1604917120.0000\n",
      "Epoch 163/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1524917632.0000 - val_loss: 1604635776.0000\n",
      "Epoch 164/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1522860544.0000 - val_loss: 1604928128.0000\n",
      "Epoch 165/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1520902400.0000 - val_loss: 1604528256.0000\n",
      "Epoch 166/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1519007232.0000 - val_loss: 1602973056.0000\n",
      "Epoch 167/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1517114496.0000 - val_loss: 1601868672.0000\n",
      "Epoch 168/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1515284352.0000 - val_loss: 1601670528.0000\n",
      "Epoch 169/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1513461120.0000 - val_loss: 1601915136.0000\n",
      "Epoch 170/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1511808128.0000 - val_loss: 1601496960.0000\n",
      "Epoch 171/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1510275456.0000 - val_loss: 1600666240.0000\n",
      "Epoch 172/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1508842368.0000 - val_loss: 1600422400.0000\n",
      "Epoch 173/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1507409536.0000 - val_loss: 1600846208.0000\n",
      "Epoch 174/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1505940864.0000 - val_loss: 1601050624.0000\n",
      "Epoch 175/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1504506880.0000 - val_loss: 1600354944.0000\n",
      "Epoch 176/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1503102848.0000 - val_loss: 1599329280.0000\n",
      "Epoch 177/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1501778688.0000 - val_loss: 1598855040.0000\n",
      "Epoch 178/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1500487040.0000 - val_loss: 1598834048.0000\n",
      "Epoch 179/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1499217664.0000 - val_loss: 1598225792.0000\n",
      "Epoch 180/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 1497948672.0000 - val_loss: 1596701952.0000\n",
      "Epoch 181/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1496664832.0000 - val_loss: 1595555712.0000\n",
      "Epoch 182/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1495439616.0000 - val_loss: 1595000960.0000\n",
      "Epoch 183/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1494228224.0000 - val_loss: 1595000064.0000\n",
      "Epoch 184/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1492973312.0000 - val_loss: 1594939264.0000\n",
      "Epoch 185/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1491599104.0000 - val_loss: 1594086784.0000\n",
      "Epoch 186/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1490168320.0000 - val_loss: 1593109120.0000\n",
      "Epoch 187/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1488911616.0000 - val_loss: 1592741632.0000\n",
      "Epoch 188/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1487740288.0000 - val_loss: 1592093824.0000\n",
      "Epoch 189/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1486503552.0000 - val_loss: 1591193600.0000\n",
      "Epoch 190/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1485163264.0000 - val_loss: 1590415360.0000\n",
      "Epoch 191/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1483791744.0000 - val_loss: 1589765504.0000\n",
      "Epoch 192/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1482431232.0000 - val_loss: 1589191296.0000\n",
      "Epoch 193/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1481130624.0000 - val_loss: 1588151168.0000\n",
      "Epoch 194/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1479797120.0000 - val_loss: 1587126016.0000\n",
      "Epoch 195/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1478444288.0000 - val_loss: 1586549760.0000\n",
      "Epoch 196/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1477116416.0000 - val_loss: 1586105856.0000\n",
      "Epoch 197/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1475819648.0000 - val_loss: 1585740416.0000\n",
      "Epoch 198/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1474608384.0000 - val_loss: 1585184256.0000\n",
      "Epoch 199/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1473532800.0000 - val_loss: 1584711040.0000\n",
      "Epoch 200/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 1472556032.0000 - val_loss: 1584004992.0000\n",
      "Epoch 201/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1471558272.0000 - val_loss: 1583675136.0000\n",
      "Epoch 202/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1470487296.0000 - val_loss: 1583817344.0000\n",
      "Epoch 203/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1469393152.0000 - val_loss: 1583861248.0000\n",
      "Epoch 204/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1468300416.0000 - val_loss: 1583636992.0000\n",
      "Epoch 205/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1467182720.0000 - val_loss: 1583411968.0000\n",
      "Epoch 206/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1466065792.0000 - val_loss: 1583702528.0000\n",
      "Epoch 207/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1464896768.0000 - val_loss: 1583798016.0000\n",
      "Epoch 208/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1463707648.0000 - val_loss: 1583377408.0000\n",
      "Epoch 209/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1462529152.0000 - val_loss: 1582947584.0000\n",
      "Epoch 210/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1461369600.0000 - val_loss: 1582990080.0000\n",
      "Epoch 211/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1460242304.0000 - val_loss: 1583243776.0000\n",
      "Epoch 212/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1459127168.0000 - val_loss: 1583105152.0000\n",
      "Epoch 213/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1458007936.0000 - val_loss: 1583032704.0000\n",
      "Epoch 214/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1456897408.0000 - val_loss: 1583315584.0000\n",
      "Epoch 215/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1455792640.0000 - val_loss: 1583485824.0000\n",
      "Epoch 216/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1454703872.0000 - val_loss: 1583332096.0000\n",
      "Epoch 217/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1453611136.0000 - val_loss: 1583171072.0000\n",
      "Epoch 218/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1452514176.0000 - val_loss: 1583265792.0000\n",
      "Epoch 219/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1451441024.0000 - val_loss: 1583352960.0000\n",
      "Epoch 220/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1450346752.0000 - val_loss: 1582418688.0000\n",
      "Epoch 221/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1449220736.0000 - val_loss: 1581799680.0000\n",
      "Epoch 222/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1448079360.0000 - val_loss: 1581750656.0000\n",
      "Epoch 223/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1446917504.0000 - val_loss: 1581668096.0000\n",
      "Epoch 224/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1445707008.0000 - val_loss: 1581288064.0000\n",
      "Epoch 225/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1444486528.0000 - val_loss: 1580797056.0000\n",
      "Epoch 226/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1443290496.0000 - val_loss: 1580793344.0000\n",
      "Epoch 227/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1442039552.0000 - val_loss: 1580126720.0000\n",
      "Epoch 228/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1440784512.0000 - val_loss: 1579041408.0000\n",
      "Epoch 229/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1439518464.0000 - val_loss: 1578725632.0000\n",
      "Epoch 230/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 1438263808.0000 - val_loss: 1578748160.0000\n",
      "Epoch 231/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1437031296.0000 - val_loss: 1578377728.0000\n",
      "Epoch 232/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1435788672.0000 - val_loss: 1577589504.0000\n",
      "Epoch 233/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1434493056.0000 - val_loss: 1576520960.0000\n",
      "Epoch 234/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1433014528.0000 - val_loss: 1575417088.0000\n",
      "Epoch 235/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1431404032.0000 - val_loss: 1574493056.0000\n",
      "Epoch 236/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1429713152.0000 - val_loss: 1573879296.0000\n",
      "Epoch 237/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1428149760.0000 - val_loss: 1572477696.0000\n",
      "Epoch 238/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1426715648.0000 - val_loss: 1569082880.0000\n",
      "Epoch 239/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1425297024.0000 - val_loss: 1566802944.0000\n",
      "Epoch 240/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1424070656.0000 - val_loss: 1566762752.0000\n",
      "Epoch 241/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1423038848.0000 - val_loss: 1567452288.0000\n",
      "Epoch 242/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1421979520.0000 - val_loss: 1567265920.0000\n",
      "Epoch 243/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1420871936.0000 - val_loss: 1567459328.0000\n",
      "Epoch 244/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1419768960.0000 - val_loss: 1567234432.0000\n",
      "Epoch 245/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1418610176.0000 - val_loss: 1565384448.0000\n",
      "Epoch 246/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1417389184.0000 - val_loss: 1564574080.0000\n",
      "Epoch 247/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1416121088.0000 - val_loss: 1564443520.0000\n",
      "Epoch 248/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1414878080.0000 - val_loss: 1563005056.0000\n",
      "Epoch 249/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1413718656.0000 - val_loss: 1562075008.0000\n",
      "Epoch 250/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1412653696.0000 - val_loss: 1561848448.0000\n",
      "Epoch 251/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1411560064.0000 - val_loss: 1561239680.0000\n",
      "Epoch 252/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1410497408.0000 - val_loss: 1559760256.0000\n",
      "Epoch 253/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1409496448.0000 - val_loss: 1559229440.0000\n",
      "Epoch 254/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1408496768.0000 - val_loss: 1559811072.0000\n",
      "Epoch 255/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1407424384.0000 - val_loss: 1560500224.0000\n",
      "Epoch 256/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1406326400.0000 - val_loss: 1560382208.0000\n",
      "Epoch 257/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1405239296.0000 - val_loss: 1560393472.0000\n",
      "Epoch 258/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1404112128.0000 - val_loss: 1560994816.0000\n",
      "Epoch 259/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1403017984.0000 - val_loss: 1561362944.0000\n",
      "Epoch 260/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1401910528.0000 - val_loss: 1561554432.0000\n",
      "Epoch 261/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1400868864.0000 - val_loss: 1562678656.0000\n",
      "Epoch 262/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1399747968.0000 - val_loss: 1563401856.0000\n",
      "Epoch 263/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1398652544.0000 - val_loss: 1563038336.0000\n",
      "Epoch 264/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1397555712.0000 - val_loss: 1563287424.0000\n",
      "Epoch 265/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1396422784.0000 - val_loss: 1563637888.0000\n",
      "Epoch 266/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1395297152.0000 - val_loss: 1562943872.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 267/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1394169344.0000 - val_loss: 1562308608.0000\n",
      "Epoch 268/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1393084544.0000 - val_loss: 1562531968.0000\n",
      "Epoch 269/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1391999488.0000 - val_loss: 1563497088.0000\n",
      "Epoch 270/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1390853120.0000 - val_loss: 1563037952.0000\n",
      "Epoch 271/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1389737600.0000 - val_loss: 1562530944.0000\n",
      "Epoch 272/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1388702208.0000 - val_loss: 1563655552.0000\n",
      "Epoch 273/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1387597056.0000 - val_loss: 1563518208.0000\n",
      "Epoch 274/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1386495616.0000 - val_loss: 1562121216.0000\n",
      "Epoch 275/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1385413760.0000 - val_loss: 1562158464.0000\n",
      "Epoch 276/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1384315136.0000 - val_loss: 1563257856.0000\n",
      "Epoch 277/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1383187200.0000 - val_loss: 1563209728.0000\n",
      "Epoch 278/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 1382103040.0000 - val_loss: 1562476416.0000\n",
      "Epoch 279/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1381054720.0000 - val_loss: 1563246976.0000\n",
      "Epoch 280/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1379903232.0000 - val_loss: 1562125440.0000\n",
      "Epoch 281/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1378793216.0000 - val_loss: 1560331648.0000\n",
      "Epoch 282/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1377734784.0000 - val_loss: 1560987904.0000\n",
      "Epoch 283/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1376604800.0000 - val_loss: 1561381504.0000\n",
      "Epoch 284/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1375530624.0000 - val_loss: 1559357696.0000\n",
      "Epoch 285/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1374475392.0000 - val_loss: 1559277696.0000\n",
      "Epoch 286/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1373416448.0000 - val_loss: 1560058368.0000\n",
      "Epoch 287/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1372374912.0000 - val_loss: 1557717760.0000\n",
      "Epoch 288/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1371282304.0000 - val_loss: 1556724992.0000\n",
      "Epoch 289/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1370252416.0000 - val_loss: 1558082048.0000\n",
      "Epoch 290/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 1369203584.0000 - val_loss: 1557439232.0000\n",
      "Epoch 291/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1368134016.0000 - val_loss: 1556201216.0000\n",
      "Epoch 292/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1367089792.0000 - val_loss: 1557412480.0000\n",
      "Epoch 293/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1366015104.0000 - val_loss: 1557611904.0000\n",
      "Epoch 294/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1364963200.0000 - val_loss: 1555695360.0000\n",
      "Epoch 295/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1363898624.0000 - val_loss: 1556093056.0000\n",
      "Epoch 296/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1362832640.0000 - val_loss: 1557110016.0000\n",
      "Epoch 297/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1361779968.0000 - val_loss: 1555386496.0000\n",
      "Epoch 298/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1360693376.0000 - val_loss: 1555590784.0000\n",
      "Epoch 299/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1359617920.0000 - val_loss: 1556346240.0000\n",
      "Epoch 300/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1358555520.0000 - val_loss: 1554954496.0000\n",
      "Epoch 301/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1357467008.0000 - val_loss: 1554222592.0000\n",
      "Epoch 302/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1356432000.0000 - val_loss: 1555532032.0000\n",
      "Epoch 303/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1355355520.0000 - val_loss: 1554502144.0000\n",
      "Epoch 304/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1354255744.0000 - val_loss: 1553672832.0000\n",
      "Epoch 305/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1353208448.0000 - val_loss: 1555377280.0000\n",
      "Epoch 306/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1352131584.0000 - val_loss: 1554246144.0000\n",
      "Epoch 307/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1351034112.0000 - val_loss: 1553331328.0000\n",
      "Epoch 308/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1349964288.0000 - val_loss: 1555089664.0000\n",
      "Epoch 309/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1348892416.0000 - val_loss: 1553384192.0000\n",
      "Epoch 310/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1347788672.0000 - val_loss: 1552596608.0000\n",
      "Epoch 311/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1346715008.0000 - val_loss: 1554828800.0000\n",
      "Epoch 312/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1345661440.0000 - val_loss: 1551911680.0000\n",
      "Epoch 313/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1344571392.0000 - val_loss: 1552602880.0000\n",
      "Epoch 314/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 1343460224.0000 - val_loss: 1551422080.0000\n",
      "Epoch 315/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1342363264.0000 - val_loss: 1549411456.0000\n",
      "Epoch 316/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1341288064.0000 - val_loss: 1552374144.0000\n",
      "Epoch 317/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1340209664.0000 - val_loss: 1549549440.0000\n",
      "Epoch 318/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1339079296.0000 - val_loss: 1550048768.0000\n",
      "Epoch 319/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1337938688.0000 - val_loss: 1550857472.0000\n",
      "Epoch 320/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1336863616.0000 - val_loss: 1547736832.0000\n",
      "Epoch 321/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1335784192.0000 - val_loss: 1549387776.0000\n",
      "Epoch 322/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1334676480.0000 - val_loss: 1548477184.0000\n",
      "Epoch 323/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1333555840.0000 - val_loss: 1548131840.0000\n",
      "Epoch 324/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1332470272.0000 - val_loss: 1549506432.0000\n",
      "Epoch 325/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1331384704.0000 - val_loss: 1546595968.0000\n",
      "Epoch 326/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1330259200.0000 - val_loss: 1547483776.0000\n",
      "Epoch 327/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1329146368.0000 - val_loss: 1546497920.0000\n",
      "Epoch 328/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1328015232.0000 - val_loss: 1546399360.0000\n",
      "Epoch 329/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1326922880.0000 - val_loss: 1546716672.0000\n",
      "Epoch 330/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1325809024.0000 - val_loss: 1545709824.0000\n",
      "Epoch 331/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1324675968.0000 - val_loss: 1546080384.0000\n",
      "Epoch 332/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1323551488.0000 - val_loss: 1543973888.0000\n",
      "Epoch 333/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 25us/step - loss: 1322419584.0000 - val_loss: 1545335296.0000\n",
      "Epoch 334/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1321251456.0000 - val_loss: 1544281984.0000\n",
      "Epoch 335/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1320072448.0000 - val_loss: 1544264960.0000\n",
      "Epoch 336/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1318896768.0000 - val_loss: 1544043136.0000\n",
      "Epoch 337/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1317723264.0000 - val_loss: 1542234240.0000\n",
      "Epoch 338/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1316515200.0000 - val_loss: 1542976640.0000\n",
      "Epoch 339/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1315386112.0000 - val_loss: 1540828416.0000\n",
      "Epoch 340/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1314219008.0000 - val_loss: 1541805184.0000\n",
      "Epoch 341/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1313014144.0000 - val_loss: 1539385344.0000\n",
      "Epoch 342/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1311726208.0000 - val_loss: 1541189632.0000\n",
      "Epoch 343/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1310507904.0000 - val_loss: 1539177088.0000\n",
      "Epoch 344/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1309279616.0000 - val_loss: 1540187904.0000\n",
      "Epoch 345/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1307990400.0000 - val_loss: 1539629440.0000\n",
      "Epoch 346/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1306780160.0000 - val_loss: 1539204992.0000\n",
      "Epoch 347/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1305567104.0000 - val_loss: 1540032128.0000\n",
      "Epoch 348/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1304357248.0000 - val_loss: 1536094976.0000\n",
      "Epoch 349/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1303189248.0000 - val_loss: 1541321600.0000\n",
      "Epoch 350/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1302056704.0000 - val_loss: 1533526528.0000\n",
      "Epoch 351/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1300944512.0000 - val_loss: 1540889600.0000\n",
      "Epoch 352/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1299602560.0000 - val_loss: 1532876032.0000\n",
      "Epoch 353/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1298244864.0000 - val_loss: 1537687680.0000\n",
      "Epoch 354/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1296847488.0000 - val_loss: 1533171584.0000\n",
      "Epoch 355/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1295474560.0000 - val_loss: 1533306112.0000\n",
      "Epoch 356/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1294153472.0000 - val_loss: 1535703424.0000\n",
      "Epoch 357/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1292954880.0000 - val_loss: 1529084544.0000\n",
      "Epoch 358/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1291849856.0000 - val_loss: 1538096512.0000\n",
      "Epoch 359/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1290724480.0000 - val_loss: 1527024256.0000\n",
      "Epoch 360/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1289356800.0000 - val_loss: 1535789312.0000\n",
      "Epoch 361/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1287606272.0000 - val_loss: 1529246848.0000\n",
      "Epoch 362/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1285893376.0000 - val_loss: 1529440384.0000\n",
      "Epoch 363/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1284445440.0000 - val_loss: 1532243840.0000\n",
      "Epoch 364/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1283234688.0000 - val_loss: 1525664000.0000\n",
      "Epoch 365/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1282271872.0000 - val_loss: 1531834368.0000\n",
      "Epoch 366/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1281091328.0000 - val_loss: 1524911360.0000\n",
      "Epoch 367/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1279655168.0000 - val_loss: 1528323200.0000\n",
      "Epoch 368/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1278111232.0000 - val_loss: 1527497472.0000\n",
      "Epoch 369/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1276736000.0000 - val_loss: 1524437760.0000\n",
      "Epoch 370/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1275504640.0000 - val_loss: 1530449280.0000\n",
      "Epoch 371/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1274360704.0000 - val_loss: 1522373248.0000\n",
      "Epoch 372/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1273260288.0000 - val_loss: 1531927296.0000\n",
      "Epoch 373/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1272153344.0000 - val_loss: 1520120832.0000\n",
      "Epoch 374/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1270939648.0000 - val_loss: 1529513344.0000\n",
      "Epoch 375/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1269375360.0000 - val_loss: 1521284224.0000\n",
      "Epoch 376/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1267773824.0000 - val_loss: 1523474944.0000\n",
      "Epoch 377/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1266365824.0000 - val_loss: 1525047680.0000\n",
      "Epoch 378/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1265235456.0000 - val_loss: 1518112768.0000\n",
      "Epoch 379/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1264186880.0000 - val_loss: 1527085696.0000\n",
      "Epoch 380/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1262949120.0000 - val_loss: 1516729088.0000\n",
      "Epoch 381/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1261638144.0000 - val_loss: 1524873856.0000\n",
      "Epoch 382/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1260133504.0000 - val_loss: 1518418816.0000\n",
      "Epoch 383/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1258687488.0000 - val_loss: 1519269504.0000\n",
      "Epoch 384/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1257289856.0000 - val_loss: 1522660224.0000\n",
      "Epoch 385/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1256142080.0000 - val_loss: 1514611968.0000\n",
      "Epoch 386/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1255172224.0000 - val_loss: 1525588736.0000\n",
      "Epoch 387/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1254191232.0000 - val_loss: 1511350528.0000\n",
      "Epoch 388/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1253091840.0000 - val_loss: 1524137472.0000\n",
      "Epoch 389/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1251646720.0000 - val_loss: 1511371776.0000\n",
      "Epoch 390/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1250039168.0000 - val_loss: 1518615168.0000\n",
      "Epoch 391/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1248318976.0000 - val_loss: 1514935680.0000\n",
      "Epoch 392/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1246817152.0000 - val_loss: 1513458176.0000\n",
      "Epoch 393/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1245582592.0000 - val_loss: 1517943168.0000\n",
      "Epoch 394/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1244479872.0000 - val_loss: 1509269504.0000\n",
      "Epoch 395/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1243391360.0000 - val_loss: 1520140544.0000\n",
      "Epoch 396/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1242318592.0000 - val_loss: 1506939904.0000\n",
      "Epoch 397/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1241169024.0000 - val_loss: 1520019712.0000\n",
      "Epoch 398/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1239791104.0000 - val_loss: 1507282176.0000\n",
      "Epoch 399/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1238254464.0000 - val_loss: 1515829888.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 400/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1236575104.0000 - val_loss: 1509429248.0000\n",
      "Epoch 401/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1235035904.0000 - val_loss: 1510869760.0000\n",
      "Epoch 402/500\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 1233663232.0000 - val_loss: 1512005504.0000\n",
      "Epoch 403/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1232369024.0000 - val_loss: 1507477632.0000\n",
      "Epoch 404/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1231129600.0000 - val_loss: 1513702400.0000\n",
      "Epoch 405/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1229905920.0000 - val_loss: 1504629632.0000\n",
      "Epoch 406/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1228712320.0000 - val_loss: 1513926912.0000\n",
      "Epoch 407/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1227484032.0000 - val_loss: 1502637184.0000\n",
      "Epoch 408/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1226293376.0000 - val_loss: 1514172160.0000\n",
      "Epoch 409/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1224995840.0000 - val_loss: 1500980224.0000\n",
      "Epoch 410/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1223972864.0000 - val_loss: 1516079360.0000\n",
      "Epoch 411/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1222867456.0000 - val_loss: 1499116672.0000\n",
      "Epoch 412/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1221966080.0000 - val_loss: 1516876800.0000\n",
      "Epoch 413/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1220515840.0000 - val_loss: 1499039104.0000\n",
      "Epoch 414/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1219026048.0000 - val_loss: 1512796160.0000\n",
      "Epoch 415/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1217119488.0000 - val_loss: 1501139712.0000\n",
      "Epoch 416/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1215285632.0000 - val_loss: 1505926272.0000\n",
      "Epoch 417/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1213593472.0000 - val_loss: 1503801472.0000\n",
      "Epoch 418/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1212189824.0000 - val_loss: 1500774528.0000\n",
      "Epoch 419/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1210898688.0000 - val_loss: 1507152768.0000\n",
      "Epoch 420/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1209835776.0000 - val_loss: 1495283584.0000\n",
      "Epoch 421/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1209248640.0000 - val_loss: 1512979584.0000\n",
      "Epoch 422/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1208905472.0000 - val_loss: 1490441728.0000\n",
      "Epoch 423/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1208868224.0000 - val_loss: 1516494848.0000\n",
      "Epoch 424/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1207885696.0000 - val_loss: 1489693312.0000\n",
      "Epoch 425/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1206185856.0000 - val_loss: 1509794944.0000\n",
      "Epoch 426/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1203437056.0000 - val_loss: 1494216832.0000\n",
      "Epoch 427/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1200923264.0000 - val_loss: 1499613952.0000\n",
      "Epoch 428/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1199088640.0000 - val_loss: 1503135232.0000\n",
      "Epoch 429/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1198155264.0000 - val_loss: 1491778048.0000\n",
      "Epoch 430/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1197841152.0000 - val_loss: 1509929728.0000\n",
      "Epoch 431/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1197499392.0000 - val_loss: 1487880576.0000\n",
      "Epoch 432/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1196825472.0000 - val_loss: 1508320384.0000\n",
      "Epoch 433/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1195005312.0000 - val_loss: 1489633024.0000\n",
      "Epoch 434/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1192723968.0000 - val_loss: 1498746368.0000\n",
      "Epoch 435/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1190524544.0000 - val_loss: 1496951552.0000\n",
      "Epoch 436/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1189176448.0000 - val_loss: 1489726464.0000\n",
      "Epoch 437/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1188576896.0000 - val_loss: 1503949696.0000\n",
      "Epoch 438/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1188148224.0000 - val_loss: 1485429376.0000\n",
      "Epoch 439/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1187658624.0000 - val_loss: 1505358336.0000\n",
      "Epoch 440/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1186451200.0000 - val_loss: 1484989312.0000\n",
      "Epoch 441/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1184652928.0000 - val_loss: 1499126784.0000\n",
      "Epoch 442/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1182483840.0000 - val_loss: 1488499584.0000\n",
      "Epoch 443/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1180551040.0000 - val_loss: 1490045568.0000\n",
      "Epoch 444/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1179115264.0000 - val_loss: 1493871744.0000\n",
      "Epoch 445/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1178134528.0000 - val_loss: 1484529024.0000\n",
      "Epoch 446/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1177398016.0000 - val_loss: 1498859136.0000\n",
      "Epoch 447/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1176688000.0000 - val_loss: 1482140288.0000\n",
      "Epoch 448/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1176086912.0000 - val_loss: 1502117376.0000\n",
      "Epoch 449/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1175210752.0000 - val_loss: 1480761600.0000\n",
      "Epoch 450/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1174172416.0000 - val_loss: 1500500608.0000\n",
      "Epoch 451/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1172514816.0000 - val_loss: 1481118592.0000\n",
      "Epoch 452/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1170648448.0000 - val_loss: 1493055872.0000\n",
      "Epoch 453/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1168616064.0000 - val_loss: 1483492096.0000\n",
      "Epoch 454/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1166923904.0000 - val_loss: 1485960832.0000\n",
      "Epoch 455/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1165516544.0000 - val_loss: 1486658816.0000\n",
      "Epoch 456/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1164386048.0000 - val_loss: 1480857088.0000\n",
      "Epoch 457/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1163425536.0000 - val_loss: 1490918912.0000\n",
      "Epoch 458/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1162715776.0000 - val_loss: 1476921600.0000\n",
      "Epoch 459/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1162347392.0000 - val_loss: 1496785280.0000\n",
      "Epoch 460/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1162123648.0000 - val_loss: 1473521664.0000\n",
      "Epoch 461/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1162390400.0000 - val_loss: 1502674432.0000\n",
      "Epoch 462/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1162281088.0000 - val_loss: 1471717632.0000\n",
      "Epoch 463/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1161344256.0000 - val_loss: 1497917696.0000\n",
      "Epoch 464/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1158531584.0000 - val_loss: 1474148480.0000\n",
      "Epoch 465/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1155276032.0000 - val_loss: 1483144832.0000\n",
      "Epoch 466/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 1152656384.0000 - val_loss: 1483358592.0000\n",
      "Epoch 467/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1151581312.0000 - val_loss: 1472434432.0000\n",
      "Epoch 468/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1151661824.0000 - val_loss: 1491292032.0000\n",
      "Epoch 469/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1151585408.0000 - val_loss: 1469085824.0000\n",
      "Epoch 470/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1151208448.0000 - val_loss: 1491826688.0000\n",
      "Epoch 471/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1149708416.0000 - val_loss: 1469820544.0000\n",
      "Epoch 472/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1147585280.0000 - val_loss: 1483383168.0000\n",
      "Epoch 473/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1145080192.0000 - val_loss: 1474972800.0000\n",
      "Epoch 474/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1143231104.0000 - val_loss: 1474159104.0000\n",
      "Epoch 475/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1142145408.0000 - val_loss: 1481808640.0000\n",
      "Epoch 476/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1141632000.0000 - val_loss: 1468264320.0000\n",
      "Epoch 477/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1141519104.0000 - val_loss: 1488567808.0000\n",
      "Epoch 478/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1141546624.0000 - val_loss: 1465031424.0000\n",
      "Epoch 479/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1141496320.0000 - val_loss: 1490781952.0000\n",
      "Epoch 480/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1140316160.0000 - val_loss: 1464846208.0000\n",
      "Epoch 481/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1138440832.0000 - val_loss: 1483369600.0000\n",
      "Epoch 482/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1135783552.0000 - val_loss: 1468733184.0000\n",
      "Epoch 483/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1133416704.0000 - val_loss: 1472303744.0000\n",
      "Epoch 484/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1131782784.0000 - val_loss: 1476431360.0000\n",
      "Epoch 485/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1131077120.0000 - val_loss: 1465166976.0000\n",
      "Epoch 486/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1130950144.0000 - val_loss: 1483527424.0000\n",
      "Epoch 487/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1130928000.0000 - val_loss: 1461610112.0000\n",
      "Epoch 488/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1131233280.0000 - val_loss: 1488589824.0000\n",
      "Epoch 489/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1130817920.0000 - val_loss: 1460738560.0000\n",
      "Epoch 490/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1129931904.0000 - val_loss: 1486317824.0000\n",
      "Epoch 491/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1127738368.0000 - val_loss: 1462443392.0000\n",
      "Epoch 492/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1125247744.0000 - val_loss: 1476817408.0000\n",
      "Epoch 493/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1122605312.0000 - val_loss: 1467649280.0000\n",
      "Epoch 494/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1120704256.0000 - val_loss: 1468379520.0000\n",
      "Epoch 495/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1119493120.0000 - val_loss: 1473581440.0000\n",
      "Epoch 496/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1118739072.0000 - val_loss: 1463494528.0000\n",
      "Epoch 497/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1118286080.0000 - val_loss: 1478947200.0000\n",
      "Epoch 498/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1117937664.0000 - val_loss: 1461045504.0000\n",
      "Epoch 499/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1117534464.0000 - val_loss: 1482173568.0000\n",
      "Epoch 500/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1116913408.0000 - val_loss: 1460020864.0000\n"
     ]
    }
   ],
   "source": [
    "NN_500E_Adam_LReLU.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = NN_500E_Adam_LReLU.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1164/1164 [==============================] - 0s 118us/step - loss: 384.8524 - val_loss: 4047.5051\n",
      "Epoch 2/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 5067.0640 - val_loss: 3049.7812\n",
      "Epoch 3/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 3833.0850 - val_loss: 932.3732\n",
      "Epoch 4/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1170.4578 - val_loss: 1301.4948\n",
      "Epoch 5/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1645.2466 - val_loss: 1648.0166\n",
      "Epoch 6/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2098.5935 - val_loss: 1042.1290\n",
      "Epoch 7/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1343.7072 - val_loss: 67.4875\n",
      "Epoch 8/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 66.8633 - val_loss: 538.8273\n",
      "Epoch 9/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 679.9147 - val_loss: 372.4651\n",
      "Epoch 10/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 481.4191 - val_loss: 165.0356\n",
      "Epoch 11/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 195.4883 - val_loss: 224.2989\n",
      "Epoch 12/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 264.5858 - val_loss: 65.3700\n",
      "Epoch 13/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 110.4932 - val_loss: 34.2772\n",
      "Epoch 14/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 61.9176 - val_loss: 239.2167\n",
      "Epoch 15/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 301.6340 - val_loss: 184.9531\n",
      "Epoch 16/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 245.5936 - val_loss: 136.0376\n",
      "Epoch 17/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 151.6463 - val_loss: 151.3983\n",
      "Epoch 18/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 168.8296 - val_loss: 87.9859\n",
      "Epoch 19/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 131.3847 - val_loss: 60.8306\n",
      "Epoch 20/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 89.4316 - val_loss: 182.1338\n",
      "Epoch 21/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 227.8609 - val_loss: 168.6763\n",
      "Epoch 22/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 220.3031 - val_loss: 55.0205\n",
      "Epoch 23/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 58.9995 - val_loss: 54.7428\n",
      "Epoch 24/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 58.0288 - val_loss: 130.4059\n",
      "Epoch 25/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 176.3131 - val_loss: 101.7786\n",
      "Epoch 26/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 133.5223 - val_loss: 103.2768\n",
      "Epoch 27/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 135.1726 - val_loss: 106.8829\n",
      "Epoch 28/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 143.7963 - val_loss: 58.6684\n",
      "Epoch 29/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 66.8216 - val_loss: 41.3671\n",
      "Epoch 30/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45.3410 - val_loss: 128.5317\n",
      "Epoch 31/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 170.2650 - val_loss: 121.7801\n",
      "Epoch 32/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 157.2135 - val_loss: 34.4936\n",
      "Epoch 33/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48.5845 - val_loss: 29.6638\n",
      "Epoch 34/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43.4966 - val_loss: 111.0485\n",
      "Epoch 35/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 139.7963 - val_loss: 94.9263\n",
      "Epoch 36/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 121.0285 - val_loss: 53.3030\n",
      "Epoch 37/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 68.3936 - val_loss: 53.0320\n",
      "Epoch 38/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 66.3727 - val_loss: 73.9931\n",
      "Epoch 39/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 98.9610 - val_loss: 62.0680\n",
      "Epoch 40/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 83.4544 - val_loss: 67.9888\n",
      "Epoch 41/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 85.6221 - val_loss: 64.2357\n",
      "Epoch 42/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 81.6448 - val_loss: 53.7711\n",
      "Epoch 43/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 69.8317 - val_loss: 45.9922\n",
      "Epoch 44/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 58.6377 - val_loss: 69.7881\n",
      "Epoch 45/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 91.7498 - val_loss: 65.3238\n",
      "Epoch 46/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 86.0995 - val_loss: 42.1513\n",
      "Epoch 47/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 53.0205 - val_loss: 35.5253\n",
      "Epoch 48/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44.9020 - val_loss: 68.8059\n",
      "Epoch 49/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 89.1533 - val_loss: 64.0618\n",
      "Epoch 50/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 82.2862 - val_loss: 34.3501\n",
      "Epoch 51/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45.4378 - val_loss: 29.9687\n",
      "Epoch 52/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39.5511 - val_loss: 62.5893\n",
      "Epoch 53/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 81.2484 - val_loss: 56.1351\n",
      "Epoch 54/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 73.3532 - val_loss: 34.8950\n",
      "Epoch 55/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44.3797 - val_loss: 31.8690\n",
      "Epoch 56/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40.2342 - val_loss: 52.1867\n",
      "Epoch 57/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 68.7891 - val_loss: 46.5726\n",
      "Epoch 58/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 61.1054 - val_loss: 36.2503\n",
      "Epoch 59/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47.0537 - val_loss: 33.2327\n",
      "Epoch 60/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43.3072 - val_loss: 43.6836\n",
      "Epoch 61/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 56.4921 - val_loss: 38.1670\n",
      "Epoch 62/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 49.3725 - val_loss: 38.5102\n",
      "Epoch 63/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 49.8930 - val_loss: 35.9621\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00063: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN_500E_Adam_LReLU05 = Sequential()\n",
    "NN_500E_Adam_LReLU05.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam_LReLU05.add(Dense(1))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience = 30, mode='min', restore_best_weights=True, verbose=1)\n",
    "NN_500E_Adam_LReLU05.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "history = NN_500E_Adam_LReLU05.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=1000,validation_split=0.2,callbacks=[es])\n",
    "\n",
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_13 (Dense)             (None, 512)               207872    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)   (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 733,697\n",
      "Trainable params: 733,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/10000\n",
      "1164/1164 [==============================] - 0s 131us/step - loss: 196357.5469 - val_loss: 195508.9531\n",
      "Epoch 2/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 195810.5781 - val_loss: 194989.0156\n",
      "Epoch 3/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 195268.4844 - val_loss: 194471.6406\n",
      "Epoch 4/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 194730.0469 - val_loss: 193957.9531\n",
      "Epoch 5/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 194196.0469 - val_loss: 193442.1719\n",
      "Epoch 6/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 193661.4219 - val_loss: 192922.8594\n",
      "Epoch 7/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 193123.4375 - val_loss: 192397.0156\n",
      "Epoch 8/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 192579.8438 - val_loss: 191860.8594\n",
      "Epoch 9/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 192025.9844 - val_loss: 191314.1094\n",
      "Epoch 10/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 191462.5781 - val_loss: 190756.0000\n",
      "Epoch 11/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 190888.5938 - val_loss: 190183.8438\n",
      "Epoch 12/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 190301.8750 - val_loss: 189594.1250\n",
      "Epoch 13/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 189697.8750 - val_loss: 188984.3438\n",
      "Epoch 14/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 189074.0156 - val_loss: 188349.5469\n",
      "Epoch 15/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 188426.1562 - val_loss: 187694.9531\n",
      "Epoch 16/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 187760.0156 - val_loss: 187014.5469\n",
      "Epoch 17/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 187069.4219 - val_loss: 186306.4844\n",
      "Epoch 18/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 186351.9531 - val_loss: 185567.9531\n",
      "Epoch 19/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 185605.4219 - val_loss: 184797.0000\n",
      "Epoch 20/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 184827.9844 - val_loss: 183991.8750\n",
      "Epoch 21/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 184018.6250 - val_loss: 183151.0156\n",
      "Epoch 22/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 183176.4844 - val_loss: 182271.9844\n",
      "Epoch 23/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 182298.5938 - val_loss: 181351.8750\n",
      "Epoch 24/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 181383.3750 - val_loss: 180387.8438\n",
      "Epoch 25/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 180427.4844 - val_loss: 179379.0156\n",
      "Epoch 26/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 179431.3750 - val_loss: 178324.6250\n",
      "Epoch 27/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 178394.0469 - val_loss: 177223.3750\n",
      "Epoch 28/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 177315.9844 - val_loss: 176073.1562\n",
      "Epoch 29/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 176195.4688 - val_loss: 174872.6562\n",
      "Epoch 30/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 175031.8281 - val_loss: 173620.6562\n",
      "Epoch 31/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 173824.1719 - val_loss: 172315.4844\n",
      "Epoch 32/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 172573.8281 - val_loss: 170955.8750\n",
      "Epoch 33/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 171278.6719 - val_loss: 169540.1719\n",
      "Epoch 34/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 169940.0469 - val_loss: 168067.0469\n",
      "Epoch 35/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 168557.4375 - val_loss: 166534.9531\n",
      "Epoch 36/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 167130.1875 - val_loss: 164942.6406\n",
      "Epoch 37/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 165660.5156 - val_loss: 163289.3281\n",
      "Epoch 38/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 164148.0469 - val_loss: 161573.5156\n",
      "Epoch 39/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 162594.5781 - val_loss: 159794.8281\n",
      "Epoch 40/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 161001.9844 - val_loss: 157952.9531\n",
      "Epoch 41/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 159372.0625 - val_loss: 156047.8281\n",
      "Epoch 42/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 157708.1719 - val_loss: 154079.8281\n",
      "Epoch 43/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 156014.0469 - val_loss: 152049.0625\n",
      "Epoch 44/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 154292.7344 - val_loss: 149957.0156\n",
      "Epoch 45/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 152549.4219 - val_loss: 147805.0625\n",
      "Epoch 46/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 150789.7969 - val_loss: 145595.5156\n",
      "Epoch 47/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 149019.2656 - val_loss: 143331.1562\n",
      "Epoch 48/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 147245.9531 - val_loss: 141015.4375\n",
      "Epoch 49/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 145477.3906 - val_loss: 138652.7969\n",
      "Epoch 50/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 143722.6562 - val_loss: 136248.6719\n",
      "Epoch 51/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 141991.8281 - val_loss: 133809.2031\n",
      "Epoch 52/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 140294.6719 - val_loss: 131341.7969\n",
      "Epoch 53/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 138643.2031 - val_loss: 128855.0391\n",
      "Epoch 54/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 137048.7969 - val_loss: 126359.0156\n",
      "Epoch 55/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 135524.0156 - val_loss: 123864.7812\n",
      "Epoch 56/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 134080.8281 - val_loss: 121385.0156\n",
      "Epoch 57/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 132731.1875 - val_loss: 118933.5234\n",
      "Epoch 58/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 131486.0469 - val_loss: 116525.0156\n",
      "Epoch 59/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 130355.7812 - val_loss: 114175.5469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 60/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 129346.8438 - val_loss: 111901.9844\n",
      "Epoch 61/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 128464.8906 - val_loss: 109720.8047\n",
      "Epoch 62/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 127711.9453 - val_loss: 107632.5703\n",
      "Epoch 63/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 127080.7422 - val_loss: 105684.1875\n",
      "Epoch 64/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 126574.8984 - val_loss: 103872.5234\n",
      "Epoch 65/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 126178.2422 - val_loss: 102208.4297\n",
      "Epoch 66/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 125876.9062 - val_loss: 100699.0078\n",
      "Epoch 67/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 125652.7422 - val_loss: 99348.4766\n",
      "Epoch 68/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 125485.9922 - val_loss: 98157.2266\n",
      "Epoch 69/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 125356.0547 - val_loss: 97122.5078\n",
      "Epoch 70/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 125243.2578 - val_loss: 96238.9297\n",
      "Epoch 71/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 125129.2500 - val_loss: 95498.7422\n",
      "Epoch 72/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 124999.0938 - val_loss: 94892.9766\n",
      "Epoch 73/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 124841.2891 - val_loss: 94411.6875\n",
      "Epoch 74/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 124649.2500 - val_loss: 94044.7266\n",
      "Epoch 75/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 124419.1328 - val_loss: 93781.7734\n",
      "Epoch 76/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 124151.8203 - val_loss: 93612.8125\n",
      "Epoch 77/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 123849.9766 - val_loss: 93527.7734\n",
      "Epoch 78/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 123519.1172 - val_loss: 93516.7266\n",
      "Epoch 79/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 123166.0625 - val_loss: 93569.5234\n",
      "Epoch 80/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 122797.9766 - val_loss: 93676.2266\n",
      "Epoch 81/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 122421.2656 - val_loss: 93826.4219\n",
      "Epoch 82/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 122042.9453 - val_loss: 94009.8203\n",
      "Epoch 83/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 121667.8047 - val_loss: 94216.3125\n",
      "Epoch 84/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 121299.7656 - val_loss: 94435.7734\n",
      "Epoch 85/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 120941.2344 - val_loss: 94658.2734\n",
      "Epoch 86/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 120594.2422 - val_loss: 94874.5078\n",
      "Epoch 87/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 120259.2734 - val_loss: 95075.7266\n",
      "Epoch 88/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 119935.2188 - val_loss: 95253.9375\n",
      "Epoch 89/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 119620.9062 - val_loss: 95402.3047\n",
      "Epoch 90/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 119313.9766 - val_loss: 95515.5000\n",
      "Epoch 91/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 119010.9688 - val_loss: 95587.2734\n",
      "Epoch 92/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 118712.7266 - val_loss: 95614.7266\n",
      "Epoch 93/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 118414.7891 - val_loss: 95595.2344\n",
      "Epoch 94/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 118115.0625 - val_loss: 95527.5078\n",
      "Epoch 95/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 117812.2500 - val_loss: 95411.2344\n",
      "Epoch 96/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 117504.9609 - val_loss: 95246.7422\n",
      "Epoch 97/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 117191.2578 - val_loss: 95034.4922\n",
      "Epoch 98/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 116869.9219 - val_loss: 94778.6875\n",
      "Epoch 99/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 116542.7500 - val_loss: 94481.0625\n",
      "Epoch 100/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 116208.0547 - val_loss: 94145.0859\n",
      "Epoch 101/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 115866.8047 - val_loss: 93774.5859\n",
      "Epoch 102/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 115518.2422 - val_loss: 93373.9219\n",
      "Epoch 103/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 115163.9375 - val_loss: 92947.4219\n",
      "Epoch 104/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 114803.0547 - val_loss: 92499.7422\n",
      "Epoch 105/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 114437.0625 - val_loss: 92035.5625\n",
      "Epoch 106/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 114066.7656 - val_loss: 91559.2266\n",
      "Epoch 107/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 113691.7969 - val_loss: 91074.9766\n",
      "Epoch 108/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 113312.7422 - val_loss: 90586.7578\n",
      "Epoch 109/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 112929.7578 - val_loss: 90098.6719\n",
      "Epoch 110/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 112542.7578 - val_loss: 89614.2422\n",
      "Epoch 111/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 112151.2109 - val_loss: 89136.7422\n",
      "Epoch 112/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 111755.0625 - val_loss: 88668.7734\n",
      "Epoch 113/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 111353.9375 - val_loss: 88212.2734\n",
      "Epoch 114/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 110946.7422 - val_loss: 87767.9766\n",
      "Epoch 115/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 110533.0234 - val_loss: 87336.5234\n",
      "Epoch 116/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 110113.0078 - val_loss: 86918.4375\n",
      "Epoch 117/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 109685.7969 - val_loss: 86513.4766\n",
      "Epoch 118/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 109250.7344 - val_loss: 86121.7266\n",
      "Epoch 119/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 108806.9453 - val_loss: 85742.0234\n",
      "Epoch 120/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 108354.0547 - val_loss: 85372.0078\n",
      "Epoch 121/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 107890.2500 - val_loss: 85012.6875\n",
      "Epoch 122/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 107418.9922 - val_loss: 84660.9375\n",
      "Epoch 123/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 106938.2031 - val_loss: 84315.0859\n",
      "Epoch 124/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 106448.7422 - val_loss: 83973.5078\n",
      "Epoch 125/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 105949.7422 - val_loss: 83634.5234\n",
      "Epoch 126/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 105440.9844 - val_loss: 83295.8203\n",
      "Epoch 127/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 104922.7344 - val_loss: 82954.6797\n",
      "Epoch 128/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 104394.7734 - val_loss: 82608.4141\n",
      "Epoch 129/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 103856.7891 - val_loss: 82254.7578\n",
      "Epoch 130/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 103308.7812 - val_loss: 81891.2578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 102750.0078 - val_loss: 81517.1641\n",
      "Epoch 132/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 102179.7188 - val_loss: 81130.0234\n",
      "Epoch 133/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 101595.0703 - val_loss: 80731.5000\n",
      "Epoch 134/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 101001.0000 - val_loss: 80318.6797\n",
      "Epoch 135/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 100392.2812 - val_loss: 79894.2266\n",
      "Epoch 136/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 99774.0234 - val_loss: 79456.4922\n",
      "Epoch 137/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 99143.9297 - val_loss: 79004.6797\n",
      "Epoch 138/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 98500.2188 - val_loss: 78539.0234\n",
      "Epoch 139/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 97843.2891 - val_loss: 78059.7422\n",
      "Epoch 140/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 97172.9219 - val_loss: 77567.2422\n",
      "Epoch 141/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 96488.0781 - val_loss: 77062.2500\n",
      "Epoch 142/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 95789.9219 - val_loss: 76545.4062\n",
      "Epoch 143/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 95076.0625 - val_loss: 76015.7266\n",
      "Epoch 144/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 94344.7109 - val_loss: 75476.9922\n",
      "Epoch 145/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 93599.7500 - val_loss: 74928.3281\n",
      "Epoch 146/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 92838.7109 - val_loss: 74371.4688\n",
      "Epoch 147/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 92060.0078 - val_loss: 73806.0781\n",
      "Epoch 148/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 91265.0625 - val_loss: 73232.7734\n",
      "Epoch 149/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 90451.2422 - val_loss: 72653.1484\n",
      "Epoch 150/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 89619.6953 - val_loss: 72067.4062\n",
      "Epoch 151/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 88771.0703 - val_loss: 71474.9062\n",
      "Epoch 152/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 87905.0625 - val_loss: 70877.7422\n",
      "Epoch 153/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 87020.0703 - val_loss: 70275.6094\n",
      "Epoch 154/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 86115.9766 - val_loss: 69669.1406\n",
      "Epoch 155/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 85191.3359 - val_loss: 69058.1641\n",
      "Epoch 156/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 84244.6562 - val_loss: 68441.8359\n",
      "Epoch 157/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 83276.0234 - val_loss: 67820.5078\n",
      "Epoch 158/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 82285.0234 - val_loss: 67193.6641\n",
      "Epoch 159/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 81273.3438 - val_loss: 66559.5234\n",
      "Epoch 160/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 80240.2656 - val_loss: 65918.0859\n",
      "Epoch 161/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 79185.3438 - val_loss: 65268.3359\n",
      "Epoch 162/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 78106.0000 - val_loss: 64612.6445\n",
      "Epoch 163/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 77010.2812 - val_loss: 63948.8555\n",
      "Epoch 164/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 75896.0000 - val_loss: 63276.5273\n",
      "Epoch 165/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 74764.6328 - val_loss: 62597.5273\n",
      "Epoch 166/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 73615.9766 - val_loss: 61913.2188\n",
      "Epoch 167/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 72448.9766 - val_loss: 61225.1367\n",
      "Epoch 168/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 71266.6016 - val_loss: 60533.3633\n",
      "Epoch 169/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 70072.3281 - val_loss: 59839.7383\n",
      "Epoch 170/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 68868.0859 - val_loss: 59144.5352\n",
      "Epoch 171/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 67657.4062 - val_loss: 58451.8750\n",
      "Epoch 172/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 66442.4062 - val_loss: 57763.2383\n",
      "Epoch 173/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 65228.1016 - val_loss: 57080.4961\n",
      "Epoch 174/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 64020.8867 - val_loss: 56406.2188\n",
      "Epoch 175/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 62821.8828 - val_loss: 55744.7578\n",
      "Epoch 176/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 61636.1172 - val_loss: 55101.5078\n",
      "Epoch 177/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 60470.0273 - val_loss: 54479.5938\n",
      "Epoch 178/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 59329.1211 - val_loss: 53884.9062\n",
      "Epoch 179/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 58219.8828 - val_loss: 53318.8867\n",
      "Epoch 180/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 57149.8789 - val_loss: 52784.5938\n",
      "Epoch 181/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 56127.3711 - val_loss: 52286.3711\n",
      "Epoch 182/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 55159.6094 - val_loss: 51827.5039\n",
      "Epoch 183/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 54253.6094 - val_loss: 51411.0000\n",
      "Epoch 184/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 53417.0039 - val_loss: 51037.5977\n",
      "Epoch 185/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 52654.8906 - val_loss: 50708.9023\n",
      "Epoch 186/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 51975.0352 - val_loss: 50426.2852\n",
      "Epoch 187/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 51382.3867 - val_loss: 50188.3750\n",
      "Epoch 188/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 50876.5273 - val_loss: 49994.7539\n",
      "Epoch 189/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 50457.0312 - val_loss: 49844.3828\n",
      "Epoch 190/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 50120.1406 - val_loss: 49733.7852\n",
      "Epoch 191/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49860.6133 - val_loss: 49658.9648\n",
      "Epoch 192/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49671.0391 - val_loss: 49614.5938\n",
      "Epoch 193/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49542.8594 - val_loss: 49594.3867\n",
      "Epoch 194/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49465.3828 - val_loss: 49591.7852\n",
      "Epoch 195/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49428.4883 - val_loss: 49600.2461\n",
      "Epoch 196/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49420.6445 - val_loss: 49613.8789\n",
      "Epoch 197/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49431.4609 - val_loss: 49627.4062\n",
      "Epoch 198/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49450.6172 - val_loss: 49636.6211\n",
      "Epoch 199/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49470.0391 - val_loss: 49638.5039\n",
      "Epoch 200/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49483.1172 - val_loss: 49630.7617\n",
      "Epoch 201/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49484.8906 - val_loss: 49612.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 202/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49471.9688 - val_loss: 49581.5352\n",
      "Epoch 203/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49442.8672 - val_loss: 49539.4883\n",
      "Epoch 204/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49397.4609 - val_loss: 49486.3477\n",
      "Epoch 205/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49336.6406 - val_loss: 49423.2539\n",
      "Epoch 206/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49262.5117 - val_loss: 49351.6133\n",
      "Epoch 207/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49177.4609 - val_loss: 49272.9648\n",
      "Epoch 208/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49084.0039 - val_loss: 49189.1211\n",
      "Epoch 209/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48985.3594 - val_loss: 49102.1211\n",
      "Epoch 210/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48883.9961 - val_loss: 49013.7539\n",
      "Epoch 211/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48782.4688 - val_loss: 48925.5898\n",
      "Epoch 212/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48682.6172 - val_loss: 48838.9570\n",
      "Epoch 213/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48586.3789 - val_loss: 48754.8711\n",
      "Epoch 214/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 48494.4922 - val_loss: 48674.0039\n",
      "Epoch 215/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48407.9609 - val_loss: 48596.6211\n",
      "Epoch 216/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48326.6484 - val_loss: 48523.0352\n",
      "Epoch 217/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48251.4883 - val_loss: 48453.0000\n",
      "Epoch 218/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48181.5312 - val_loss: 48386.3398\n",
      "Epoch 219/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48116.5391 - val_loss: 48322.5273\n",
      "Epoch 220/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 48055.9961 - val_loss: 48261.3438\n",
      "Epoch 221/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47999.0000 - val_loss: 48202.3398\n",
      "Epoch 222/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47944.9961 - val_loss: 48145.0430\n",
      "Epoch 223/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47893.0391 - val_loss: 48089.5898\n",
      "Epoch 224/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47842.9648 - val_loss: 48035.4961\n",
      "Epoch 225/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47793.6133 - val_loss: 47982.7148\n",
      "Epoch 226/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47744.8594 - val_loss: 47931.0039\n",
      "Epoch 227/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47695.9883 - val_loss: 47880.2852\n",
      "Epoch 228/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47646.8555 - val_loss: 47830.4727\n",
      "Epoch 229/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47597.0391 - val_loss: 47781.2500\n",
      "Epoch 230/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47546.6250 - val_loss: 47732.7539\n",
      "Epoch 231/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 47495.6172 - val_loss: 47684.7852\n",
      "Epoch 232/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 47443.9883 - val_loss: 47637.2617\n",
      "Epoch 233/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47391.5391 - val_loss: 47590.0312\n",
      "Epoch 234/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 47338.6211 - val_loss: 47543.1289\n",
      "Epoch 235/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47285.4961 - val_loss: 47496.4648\n",
      "Epoch 236/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 47232.0039 - val_loss: 47449.9883\n",
      "Epoch 237/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47178.4883 - val_loss: 47403.6641\n",
      "Epoch 238/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 47124.9883 - val_loss: 47357.6562\n",
      "Epoch 239/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47071.5117 - val_loss: 47311.9141\n",
      "Epoch 240/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 47018.4609 - val_loss: 47266.5938\n",
      "Epoch 241/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46965.5391 - val_loss: 47221.6562\n",
      "Epoch 242/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46913.1445 - val_loss: 47177.1641\n",
      "Epoch 243/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46861.1523 - val_loss: 47133.0859\n",
      "Epoch 244/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46809.6523 - val_loss: 47089.3711\n",
      "Epoch 245/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46758.6250 - val_loss: 47045.8867\n",
      "Epoch 246/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46708.0312 - val_loss: 47002.6133\n",
      "Epoch 247/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46657.8828 - val_loss: 46959.2461\n",
      "Epoch 248/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46607.9609 - val_loss: 46915.8789\n",
      "Epoch 249/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46558.3828 - val_loss: 46872.3633\n",
      "Epoch 250/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46509.0391 - val_loss: 46828.6289\n",
      "Epoch 251/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46460.0117 - val_loss: 46784.7461\n",
      "Epoch 252/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46411.1484 - val_loss: 46740.7891\n",
      "Epoch 253/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46362.5039 - val_loss: 46696.8438\n",
      "Epoch 254/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46313.9688 - val_loss: 46652.8789\n",
      "Epoch 255/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46265.4883 - val_loss: 46608.9883\n",
      "Epoch 256/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46217.0391 - val_loss: 46565.0391\n",
      "Epoch 257/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46168.8711 - val_loss: 46521.1133\n",
      "Epoch 258/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46120.5430 - val_loss: 46477.0938\n",
      "Epoch 259/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46072.5039 - val_loss: 46432.9883\n",
      "Epoch 260/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46024.4961 - val_loss: 46388.7383\n",
      "Epoch 261/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45976.5391 - val_loss: 46344.3633\n",
      "Epoch 262/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45928.8438 - val_loss: 46300.0117\n",
      "Epoch 263/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45881.1211 - val_loss: 46255.5938\n",
      "Epoch 264/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45833.5391 - val_loss: 46211.2383\n",
      "Epoch 265/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45786.3711 - val_loss: 46167.0859\n",
      "Epoch 266/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45739.1562 - val_loss: 46123.0312\n",
      "Epoch 267/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45692.3516 - val_loss: 46079.2109\n",
      "Epoch 268/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 45645.5039 - val_loss: 46035.5312\n",
      "Epoch 269/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 45598.9688 - val_loss: 45992.0938\n",
      "Epoch 270/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45552.4883 - val_loss: 45948.7539\n",
      "Epoch 271/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45506.1484 - val_loss: 45905.5898\n",
      "Epoch 272/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45460.0391 - val_loss: 45862.4961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 273/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45414.1484 - val_loss: 45819.4883\n",
      "Epoch 274/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45368.4961 - val_loss: 45776.5391\n",
      "Epoch 275/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45322.9609 - val_loss: 45733.7383\n",
      "Epoch 276/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45277.4961 - val_loss: 45691.0312\n",
      "Epoch 277/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45232.1289 - val_loss: 45648.5312\n",
      "Epoch 278/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 45187.0117 - val_loss: 45606.2891\n",
      "Epoch 279/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 45142.0117 - val_loss: 45564.3711\n",
      "Epoch 280/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 45097.1562 - val_loss: 45522.7188\n",
      "Epoch 281/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45052.5039 - val_loss: 45481.3359\n",
      "Epoch 282/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 45007.9883 - val_loss: 45440.1289\n",
      "Epoch 283/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 44963.5117 - val_loss: 45399.1289\n",
      "Epoch 284/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44919.3398 - val_loss: 45358.2812\n",
      "Epoch 285/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44875.1289 - val_loss: 45317.6133\n",
      "Epoch 286/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44831.1562 - val_loss: 45277.0859\n",
      "Epoch 287/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44787.4688 - val_loss: 45236.7188\n",
      "Epoch 288/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44743.8438 - val_loss: 45196.4961\n",
      "Epoch 289/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44700.3398 - val_loss: 45156.4883\n",
      "Epoch 290/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 44656.9648 - val_loss: 45116.5859\n",
      "Epoch 291/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 44613.6250 - val_loss: 45076.8867\n",
      "Epoch 292/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 44570.5312 - val_loss: 45037.3789\n",
      "Epoch 293/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44527.6328 - val_loss: 44997.9961\n",
      "Epoch 294/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44484.9570 - val_loss: 44958.7383\n",
      "Epoch 295/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44442.1602 - val_loss: 44919.5117\n",
      "Epoch 296/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 44399.6641 - val_loss: 44880.4141\n",
      "Epoch 297/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44357.4570 - val_loss: 44841.3633\n",
      "Epoch 298/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44315.0312 - val_loss: 44802.3867\n",
      "Epoch 299/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44272.9688 - val_loss: 44763.5938\n",
      "Epoch 300/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44230.9688 - val_loss: 44724.8867\n",
      "Epoch 301/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44189.0117 - val_loss: 44686.3711\n",
      "Epoch 302/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 44147.3672 - val_loss: 44647.9609\n",
      "Epoch 303/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 44105.8398 - val_loss: 44609.6367\n",
      "Epoch 304/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44064.3672 - val_loss: 44571.4609\n",
      "Epoch 305/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44023.0117 - val_loss: 44533.2891\n",
      "Epoch 306/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 43981.9609 - val_loss: 44495.2812\n",
      "Epoch 307/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 43940.9648 - val_loss: 44457.3867\n",
      "Epoch 308/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43900.0117 - val_loss: 44419.6211\n",
      "Epoch 309/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 43859.4570 - val_loss: 44381.9688\n",
      "Epoch 310/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 43818.8711 - val_loss: 44344.3867\n",
      "Epoch 311/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43778.4883 - val_loss: 44307.0000\n",
      "Epoch 312/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43738.1602 - val_loss: 44269.7812\n",
      "Epoch 313/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 43698.0430 - val_loss: 44232.7383\n",
      "Epoch 314/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43658.1289 - val_loss: 44195.8359\n",
      "Epoch 315/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43618.3633 - val_loss: 44159.0391\n",
      "Epoch 316/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43578.6328 - val_loss: 44122.3633\n",
      "Epoch 317/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 43539.1250 - val_loss: 44085.8359\n",
      "Epoch 318/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43499.8359 - val_loss: 44049.4961\n",
      "Epoch 319/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 43460.5312 - val_loss: 44013.3359\n",
      "Epoch 320/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 43421.5117 - val_loss: 43977.2891\n",
      "Epoch 321/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 43382.6562 - val_loss: 43941.3438\n",
      "Epoch 322/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43344.0039 - val_loss: 43905.4609\n",
      "Epoch 323/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43305.4961 - val_loss: 43869.7500\n",
      "Epoch 324/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43267.0430 - val_loss: 43834.2383\n",
      "Epoch 325/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 43228.9648 - val_loss: 43798.8867\n",
      "Epoch 326/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43190.9570 - val_loss: 43763.7812\n",
      "Epoch 327/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43153.0039 - val_loss: 43728.8867\n",
      "Epoch 328/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43115.3398 - val_loss: 43694.2539\n",
      "Epoch 329/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43077.8359 - val_loss: 43659.8438\n",
      "Epoch 330/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 43040.4648 - val_loss: 43625.5820\n",
      "Epoch 331/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43003.1641 - val_loss: 43591.3867\n",
      "Epoch 332/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 42966.1641 - val_loss: 43557.3633\n",
      "Epoch 333/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42929.3711 - val_loss: 43523.4141\n",
      "Epoch 334/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42892.6602 - val_loss: 43489.6367\n",
      "Epoch 335/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42856.1562 - val_loss: 43456.0859\n",
      "Epoch 336/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42819.9570 - val_loss: 43422.6680\n",
      "Epoch 337/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42783.8320 - val_loss: 43389.5039\n",
      "Epoch 338/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42747.8398 - val_loss: 43356.5430\n",
      "Epoch 339/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42711.9961 - val_loss: 43323.7539\n",
      "Epoch 340/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42676.4570 - val_loss: 43291.0391\n",
      "Epoch 341/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42640.9883 - val_loss: 43258.4883\n",
      "Epoch 342/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42605.6367 - val_loss: 43226.1367\n",
      "Epoch 343/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42570.5352 - val_loss: 43194.0859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 344/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42535.6328 - val_loss: 43162.1562\n",
      "Epoch 345/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 42500.9570 - val_loss: 43130.4609\n",
      "Epoch 346/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 42466.3594 - val_loss: 43098.8867\n",
      "Epoch 347/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42431.9648 - val_loss: 43067.4883\n",
      "Epoch 348/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 42397.6602 - val_loss: 43036.2461\n",
      "Epoch 349/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42363.6328 - val_loss: 43005.2070\n",
      "Epoch 350/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42329.8320 - val_loss: 42974.2891\n",
      "Epoch 351/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42296.0352 - val_loss: 42943.6133\n",
      "Epoch 352/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42262.5117 - val_loss: 42913.1367\n",
      "Epoch 353/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42229.1680 - val_loss: 42882.9180\n",
      "Epoch 354/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42196.0352 - val_loss: 42852.9141\n",
      "Epoch 355/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42163.0430 - val_loss: 42823.0391\n",
      "Epoch 356/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42130.3359 - val_loss: 42793.3320\n",
      "Epoch 357/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 42097.6641 - val_loss: 42763.7617\n",
      "Epoch 358/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42065.3281 - val_loss: 42734.4062\n",
      "Epoch 359/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42033.0117 - val_loss: 42705.2539\n",
      "Epoch 360/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 42001.0000 - val_loss: 42676.2930\n",
      "Epoch 361/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41969.1367 - val_loss: 42647.4688\n",
      "Epoch 362/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 41937.5039 - val_loss: 42618.8359\n",
      "Epoch 363/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41906.0352 - val_loss: 42590.4180\n",
      "Epoch 364/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41874.8633 - val_loss: 42562.2461\n",
      "Epoch 365/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41843.8281 - val_loss: 42534.2539\n",
      "Epoch 366/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41812.9570 - val_loss: 42506.5000\n",
      "Epoch 367/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41782.1680 - val_loss: 42478.9688\n",
      "Epoch 368/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41751.8281 - val_loss: 42451.6367\n",
      "Epoch 369/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41721.4961 - val_loss: 42424.5117\n",
      "Epoch 370/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41691.4570 - val_loss: 42397.5000\n",
      "Epoch 371/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41661.5117 - val_loss: 42370.6367\n",
      "Epoch 372/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41631.8711 - val_loss: 42344.1289\n",
      "Epoch 373/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41602.4531 - val_loss: 42317.8359\n",
      "Epoch 374/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41573.0430 - val_loss: 42291.6562\n",
      "Epoch 375/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41544.0039 - val_loss: 42265.7109\n",
      "Epoch 376/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 41515.1289 - val_loss: 42239.8789\n",
      "Epoch 377/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41486.4883 - val_loss: 42214.2461\n",
      "Epoch 378/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41457.9961 - val_loss: 42188.8867\n",
      "Epoch 379/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41429.6719 - val_loss: 42163.8438\n",
      "Epoch 380/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41401.6445 - val_loss: 42138.9688\n",
      "Epoch 381/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41373.8398 - val_loss: 42114.3359\n",
      "Epoch 382/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41346.1406 - val_loss: 42089.9062\n",
      "Epoch 383/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41318.6719 - val_loss: 42065.7148\n",
      "Epoch 384/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 41291.4961 - val_loss: 42041.6289\n",
      "Epoch 385/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41264.4648 - val_loss: 42017.7539\n",
      "Epoch 386/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41237.5352 - val_loss: 41994.1289\n",
      "Epoch 387/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41210.9648 - val_loss: 41970.7383\n",
      "Epoch 388/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41184.4961 - val_loss: 41947.6133\n",
      "Epoch 389/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41158.1719 - val_loss: 41924.7539\n",
      "Epoch 390/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41132.1641 - val_loss: 41902.0898\n",
      "Epoch 391/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41106.3633 - val_loss: 41879.5820\n",
      "Epoch 392/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41080.6758 - val_loss: 41857.2539\n",
      "Epoch 393/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41055.3555 - val_loss: 41835.1602\n",
      "Epoch 394/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41030.0430 - val_loss: 41813.2852\n",
      "Epoch 395/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41005.0430 - val_loss: 41791.6211\n",
      "Epoch 396/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40980.3320 - val_loss: 41770.1680\n",
      "Epoch 397/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40955.6719 - val_loss: 41748.9570\n",
      "Epoch 398/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40931.3555 - val_loss: 41727.9961\n",
      "Epoch 399/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40907.1328 - val_loss: 41707.2852\n",
      "Epoch 400/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40883.1641 - val_loss: 41686.8633\n",
      "Epoch 401/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40859.4961 - val_loss: 41666.6289\n",
      "Epoch 402/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40835.9844 - val_loss: 41646.6602\n",
      "Epoch 403/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40812.5469 - val_loss: 41626.7461\n",
      "Epoch 404/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40789.5039 - val_loss: 41606.9531\n",
      "Epoch 405/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40766.5430 - val_loss: 41587.3398\n",
      "Epoch 406/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40743.9570 - val_loss: 41568.1211\n",
      "Epoch 407/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40721.4570 - val_loss: 41549.2070\n",
      "Epoch 408/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40699.0430 - val_loss: 41530.6211\n",
      "Epoch 409/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40677.0039 - val_loss: 41512.2539\n",
      "Epoch 410/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40655.1367 - val_loss: 41494.0898\n",
      "Epoch 411/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40633.4961 - val_loss: 41476.0117\n",
      "Epoch 412/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40611.9961 - val_loss: 41458.1289\n",
      "Epoch 413/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40590.6758 - val_loss: 41440.3711\n",
      "Epoch 414/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40569.6445 - val_loss: 41422.9102\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 415/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40548.8320 - val_loss: 41405.8711\n",
      "Epoch 416/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40528.1328 - val_loss: 41389.1211\n",
      "Epoch 417/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40507.6719 - val_loss: 41372.5820\n",
      "Epoch 418/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 40487.4883 - val_loss: 41356.1680\n",
      "Epoch 419/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40467.4570 - val_loss: 41339.8633\n",
      "Epoch 420/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40447.5117 - val_loss: 41323.5430\n",
      "Epoch 421/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40427.9609 - val_loss: 41307.3711\n",
      "Epoch 422/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40408.4961 - val_loss: 41291.4883\n",
      "Epoch 423/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40389.1758 - val_loss: 41275.9883\n",
      "Epoch 424/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40370.1680 - val_loss: 41260.8867\n",
      "Epoch 425/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40351.3633 - val_loss: 41246.0820\n",
      "Epoch 426/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40332.6758 - val_loss: 41231.4570\n",
      "Epoch 427/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40314.3242 - val_loss: 41216.9102\n",
      "Epoch 428/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40296.0117 - val_loss: 41202.4883\n",
      "Epoch 429/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40277.9961 - val_loss: 41188.1602\n",
      "Epoch 430/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40260.1328 - val_loss: 41173.9961\n",
      "Epoch 431/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40242.4961 - val_loss: 41159.9961\n",
      "Epoch 432/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40225.0039 - val_loss: 41146.2148\n",
      "Epoch 433/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40207.6758 - val_loss: 41132.6602\n",
      "Epoch 434/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40190.5469 - val_loss: 41119.6367\n",
      "Epoch 435/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40173.6758 - val_loss: 41106.9180\n",
      "Epoch 436/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40157.0039 - val_loss: 41094.3711\n",
      "Epoch 437/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40140.5117 - val_loss: 41081.7617\n",
      "Epoch 438/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40124.1758 - val_loss: 41069.0469\n",
      "Epoch 439/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40108.0430 - val_loss: 41056.3633\n",
      "Epoch 440/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40092.1445 - val_loss: 41044.0352\n",
      "Epoch 441/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40076.4609 - val_loss: 41032.0898\n",
      "Epoch 442/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40060.8633 - val_loss: 41020.6680\n",
      "Epoch 443/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40045.4883 - val_loss: 41009.5117\n",
      "Epoch 444/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40030.3203 - val_loss: 40998.3867\n",
      "Epoch 445/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40015.1758 - val_loss: 40987.2070\n",
      "Epoch 446/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40000.4531 - val_loss: 40975.9883\n",
      "Epoch 447/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39985.6719 - val_loss: 40964.9531\n",
      "Epoch 448/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39971.1719 - val_loss: 40954.2461\n",
      "Epoch 449/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39956.9570 - val_loss: 40943.9648\n",
      "Epoch 450/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39942.6797 - val_loss: 40934.0352\n",
      "Epoch 451/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39928.8203 - val_loss: 40924.2383\n",
      "Epoch 452/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39914.9883 - val_loss: 40914.3320\n",
      "Epoch 453/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39901.3594 - val_loss: 40904.5430\n",
      "Epoch 454/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39887.9648 - val_loss: 40895.0117\n",
      "Epoch 455/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39874.6484 - val_loss: 40885.7461\n",
      "Epoch 456/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39861.5391 - val_loss: 40876.7617\n",
      "Epoch 457/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39848.6445 - val_loss: 40867.9531\n",
      "Epoch 458/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39835.9570 - val_loss: 40859.1211\n",
      "Epoch 459/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39823.3203 - val_loss: 40850.4961\n",
      "Epoch 460/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39810.8633 - val_loss: 40842.1211\n",
      "Epoch 461/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39798.5352 - val_loss: 40834.1133\n",
      "Epoch 462/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39786.4883 - val_loss: 40826.2500\n",
      "Epoch 463/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39774.4961 - val_loss: 40818.2930\n",
      "Epoch 464/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39762.6719 - val_loss: 40810.2852\n",
      "Epoch 465/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39751.0156 - val_loss: 40802.3320\n",
      "Epoch 466/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39739.5430 - val_loss: 40794.8398\n",
      "Epoch 467/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39728.3203 - val_loss: 40787.7617\n",
      "Epoch 468/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39717.0430 - val_loss: 40780.9180\n",
      "Epoch 469/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39706.0430 - val_loss: 40774.2539\n",
      "Epoch 470/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39695.1719 - val_loss: 40767.5352\n",
      "Epoch 471/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39684.4961 - val_loss: 40760.5898\n",
      "Epoch 472/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39673.9570 - val_loss: 40753.6367\n",
      "Epoch 473/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39663.4883 - val_loss: 40747.0820\n",
      "Epoch 474/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39653.1367 - val_loss: 40740.8867\n",
      "Epoch 475/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39642.9961 - val_loss: 40735.0117\n",
      "Epoch 476/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39632.9883 - val_loss: 40729.1367\n",
      "Epoch 477/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39623.0352 - val_loss: 40723.1367\n",
      "Epoch 478/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39613.3281 - val_loss: 40717.3789\n",
      "Epoch 479/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39603.6719 - val_loss: 40711.8867\n",
      "Epoch 480/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39594.1719 - val_loss: 40706.4961\n",
      "Epoch 481/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39584.8555 - val_loss: 40701.0430\n",
      "Epoch 482/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39575.5469 - val_loss: 40695.7617\n",
      "Epoch 483/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39566.4961 - val_loss: 40690.5039\n",
      "Epoch 484/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39557.5000 - val_loss: 40685.4883\n",
      "Epoch 485/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39548.6406 - val_loss: 40680.5820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 486/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39539.9570 - val_loss: 40675.7852\n",
      "Epoch 487/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39531.1875 - val_loss: 40670.8867\n",
      "Epoch 488/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39522.6797 - val_loss: 40666.1680\n",
      "Epoch 489/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39514.3516 - val_loss: 40661.6367\n",
      "Epoch 490/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39506.0039 - val_loss: 40657.3398\n",
      "Epoch 491/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39497.8633 - val_loss: 40652.9883\n",
      "Epoch 492/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39489.8203 - val_loss: 40648.5820\n",
      "Epoch 493/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39481.8281 - val_loss: 40644.3789\n",
      "Epoch 494/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39473.9883 - val_loss: 40640.2930\n",
      "Epoch 495/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39466.1719 - val_loss: 40635.9648\n",
      "Epoch 496/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39458.5156 - val_loss: 40631.9883\n",
      "Epoch 497/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39451.0039 - val_loss: 40628.4102\n",
      "Epoch 498/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39443.5117 - val_loss: 40625.1133\n",
      "Epoch 499/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39436.1484 - val_loss: 40621.4961\n",
      "Epoch 500/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39428.9570 - val_loss: 40617.9883\n",
      "Epoch 501/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39421.6797 - val_loss: 40614.4570\n",
      "Epoch 502/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39414.5469 - val_loss: 40610.7539\n",
      "Epoch 503/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39407.5469 - val_loss: 40607.4102\n",
      "Epoch 504/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39400.6719 - val_loss: 40604.3320\n",
      "Epoch 505/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39393.8633 - val_loss: 40601.2070\n",
      "Epoch 506/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39387.0430 - val_loss: 40598.1211\n",
      "Epoch 507/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39380.4883 - val_loss: 40595.2852\n",
      "Epoch 508/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39373.9570 - val_loss: 40592.7461\n",
      "Epoch 509/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39367.4531 - val_loss: 40590.2461\n",
      "Epoch 510/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39360.9961 - val_loss: 40587.3789\n",
      "Epoch 511/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39354.6445 - val_loss: 40584.3633\n",
      "Epoch 512/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39348.4531 - val_loss: 40581.1367\n",
      "Epoch 513/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39342.1797 - val_loss: 40578.3867\n",
      "Epoch 514/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39336.1406 - val_loss: 40576.3789\n",
      "Epoch 515/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39330.1445 - val_loss: 40574.4570\n",
      "Epoch 516/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39324.1875 - val_loss: 40571.9180\n",
      "Epoch 517/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39318.4531 - val_loss: 40569.0820\n",
      "Epoch 518/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39312.5391 - val_loss: 40566.6133\n",
      "Epoch 519/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39306.8555 - val_loss: 40564.4570\n",
      "Epoch 520/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39301.1484 - val_loss: 40562.4648\n",
      "Epoch 521/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39295.5391 - val_loss: 40560.6680\n",
      "Epoch 522/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39290.0352 - val_loss: 40558.4883\n",
      "Epoch 523/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39284.5391 - val_loss: 40556.1680\n",
      "Epoch 524/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39279.1445 - val_loss: 40554.1602\n",
      "Epoch 525/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39273.8281 - val_loss: 40552.4961\n",
      "Epoch 526/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39268.4961 - val_loss: 40550.9531\n",
      "Epoch 527/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39263.1797 - val_loss: 40549.1289\n",
      "Epoch 528/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39258.0039 - val_loss: 40546.7461\n",
      "Epoch 529/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39252.8633 - val_loss: 40544.2617\n",
      "Epoch 530/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39247.8164 - val_loss: 40542.3789\n",
      "Epoch 531/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39242.6836 - val_loss: 40540.8398\n",
      "Epoch 532/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39237.8164 - val_loss: 40539.1211\n",
      "Epoch 533/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39232.8516 - val_loss: 40536.9102\n",
      "Epoch 534/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39227.9883 - val_loss: 40534.7070\n",
      "Epoch 535/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39223.1445 - val_loss: 40532.6289\n",
      "Epoch 536/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39218.4609 - val_loss: 40531.1250\n",
      "Epoch 537/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39213.6797 - val_loss: 40529.8398\n",
      "Epoch 538/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39209.0156 - val_loss: 40528.2930\n",
      "Epoch 539/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39204.4883 - val_loss: 40526.3711\n",
      "Epoch 540/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39199.9570 - val_loss: 40524.3867\n",
      "Epoch 541/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39195.3516 - val_loss: 40522.8633\n",
      "Epoch 542/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39190.8242 - val_loss: 40521.5820\n",
      "Epoch 543/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39186.3242 - val_loss: 40520.3789\n",
      "Epoch 544/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39181.8477 - val_loss: 40518.7617\n",
      "Epoch 545/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39177.3555 - val_loss: 40516.9180\n",
      "Epoch 546/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39172.9961 - val_loss: 40515.2539\n",
      "Epoch 547/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39168.5391 - val_loss: 40514.0430\n",
      "Epoch 548/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39164.1484 - val_loss: 40512.7383\n",
      "Epoch 549/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39159.8164 - val_loss: 40511.1133\n",
      "Epoch 550/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39155.4492 - val_loss: 40509.5898\n",
      "Epoch 551/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39151.0039 - val_loss: 40508.2617\n",
      "Epoch 552/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39146.6836 - val_loss: 40506.9570\n",
      "Epoch 553/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39142.4961 - val_loss: 40505.9570\n",
      "Epoch 554/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39138.3555 - val_loss: 40504.7930\n",
      "Epoch 555/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39134.3281 - val_loss: 40503.5039\n",
      "Epoch 556/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39130.3242 - val_loss: 40502.1133\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 557/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39126.1836 - val_loss: 40500.9961\n",
      "Epoch 558/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 39122.1719 - val_loss: 40499.7930\n",
      "Epoch 559/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39118.1484 - val_loss: 40498.6680\n",
      "Epoch 560/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39114.3477 - val_loss: 40497.7383\n",
      "Epoch 561/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39110.5469 - val_loss: 40496.0820\n",
      "Epoch 562/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39106.9883 - val_loss: 40494.4648\n",
      "Epoch 563/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39103.3555 - val_loss: 40493.7617\n",
      "Epoch 564/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39099.8242 - val_loss: 40493.5430\n",
      "Epoch 565/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39096.1836 - val_loss: 40492.8789\n",
      "Epoch 566/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39092.8164 - val_loss: 40491.7383\n",
      "Epoch 567/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39089.3281 - val_loss: 40490.3789\n",
      "Epoch 568/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39085.8594 - val_loss: 40489.5039\n",
      "Epoch 569/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39082.4531 - val_loss: 40488.7500\n",
      "Epoch 570/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39078.9648 - val_loss: 40487.9180\n",
      "Epoch 571/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39075.5039 - val_loss: 40487.1602\n",
      "Epoch 572/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39072.1406 - val_loss: 40486.2500\n",
      "Epoch 573/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39068.8164 - val_loss: 40485.2070\n",
      "Epoch 574/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39065.4883 - val_loss: 40484.1133\n",
      "Epoch 575/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39062.1484 - val_loss: 40483.4648\n",
      "Epoch 576/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39058.8594 - val_loss: 40482.9570\n",
      "Epoch 577/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39055.5391 - val_loss: 40482.6133\n",
      "Epoch 578/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39052.3516 - val_loss: 40481.6680\n",
      "Epoch 579/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39049.0391 - val_loss: 40480.3633\n",
      "Epoch 580/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39045.8594 - val_loss: 40479.1133\n",
      "Epoch 581/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39042.6484 - val_loss: 40478.4180\n",
      "Epoch 582/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39039.5117 - val_loss: 40477.8789\n",
      "Epoch 583/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39036.4844 - val_loss: 40477.3711\n",
      "Epoch 584/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39033.3555 - val_loss: 40476.4180\n",
      "Epoch 585/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39030.3281 - val_loss: 40475.4648\n",
      "Epoch 586/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39027.3281 - val_loss: 40474.5039\n",
      "Epoch 587/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39024.3555 - val_loss: 40473.7852\n",
      "Epoch 588/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39021.4883 - val_loss: 40473.2383\n",
      "Epoch 589/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39018.5117 - val_loss: 40472.7070\n",
      "Epoch 590/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39015.6445 - val_loss: 40471.4102\n",
      "Epoch 591/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39012.6836 - val_loss: 40470.2070\n",
      "Epoch 592/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39009.8594 - val_loss: 40469.3320\n",
      "Epoch 593/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39007.0039 - val_loss: 40468.5430\n",
      "Epoch 594/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39004.1484 - val_loss: 40467.9180\n",
      "Epoch 595/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39001.3516 - val_loss: 40467.1211\n",
      "Epoch 596/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38998.5039 - val_loss: 40466.1133\n",
      "Epoch 597/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38995.6836 - val_loss: 40465.1680\n",
      "Epoch 598/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38992.9609 - val_loss: 40464.5820\n",
      "Epoch 599/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38990.1406 - val_loss: 40463.6602\n",
      "Epoch 600/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38987.3555 - val_loss: 40462.5430\n",
      "Epoch 601/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38984.5469 - val_loss: 40461.2148\n",
      "Epoch 602/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38981.8477 - val_loss: 40459.9648\n",
      "Epoch 603/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38979.0117 - val_loss: 40459.4102\n",
      "Epoch 604/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38976.3164 - val_loss: 40458.9180\n",
      "Epoch 605/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38973.5000 - val_loss: 40458.3867\n",
      "Epoch 606/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38970.8164 - val_loss: 40457.5898\n",
      "Epoch 607/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38968.0156 - val_loss: 40456.7539\n",
      "Epoch 608/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38965.4531 - val_loss: 40456.0898\n",
      "Epoch 609/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38962.6719 - val_loss: 40455.5352\n",
      "Epoch 610/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38959.9961 - val_loss: 40454.6367\n",
      "Epoch 611/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38957.3242 - val_loss: 40453.4531\n",
      "Epoch 612/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38954.5469 - val_loss: 40452.2148\n",
      "Epoch 613/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38951.9648 - val_loss: 40451.3789\n",
      "Epoch 614/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38949.3164 - val_loss: 40451.2930\n",
      "Epoch 615/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38946.5469 - val_loss: 40450.8867\n",
      "Epoch 616/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38943.9609 - val_loss: 40450.1680\n",
      "Epoch 617/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38941.1836 - val_loss: 40449.0117\n",
      "Epoch 618/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38938.5039 - val_loss: 40448.0117\n",
      "Epoch 619/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38935.8594 - val_loss: 40447.6289\n",
      "Epoch 620/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38933.1719 - val_loss: 40447.3398\n",
      "Epoch 621/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38930.5039 - val_loss: 40446.8711\n",
      "Epoch 622/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38927.8594 - val_loss: 40446.2148\n",
      "Epoch 623/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38925.1758 - val_loss: 40445.4883\n",
      "Epoch 624/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38922.6445 - val_loss: 40445.4570\n",
      "Epoch 625/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38920.0352 - val_loss: 40445.0352\n",
      "Epoch 626/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38917.5039 - val_loss: 40444.3320\n",
      "Epoch 627/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38914.9883 - val_loss: 40443.5820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 628/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38912.4883 - val_loss: 40443.1367\n",
      "Epoch 629/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38909.9883 - val_loss: 40442.8320\n",
      "Epoch 630/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38907.5000 - val_loss: 40442.5430\n",
      "Epoch 631/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38905.0391 - val_loss: 40441.6367\n",
      "Epoch 632/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38902.6445 - val_loss: 40441.0352\n",
      "Epoch 633/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38900.1836 - val_loss: 40440.6680\n",
      "Epoch 634/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38897.8477 - val_loss: 40440.6211\n",
      "Epoch 635/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38895.4609 - val_loss: 40439.9883\n",
      "Epoch 636/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38893.0117 - val_loss: 40439.2148\n",
      "Epoch 637/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38890.6484 - val_loss: 40438.4648\n",
      "Epoch 638/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38888.3242 - val_loss: 40437.8320\n",
      "Epoch 639/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38885.9609 - val_loss: 40437.7500\n",
      "Epoch 640/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38883.5156 - val_loss: 40437.6289\n",
      "Epoch 641/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38881.1758 - val_loss: 40436.9961\n",
      "Epoch 642/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38878.8594 - val_loss: 40436.4570\n",
      "Epoch 643/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38876.5117 - val_loss: 40436.0117\n",
      "Epoch 644/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38874.1836 - val_loss: 40435.2852\n",
      "Epoch 645/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38871.9609 - val_loss: 40434.7930\n",
      "Epoch 646/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38869.5469 - val_loss: 40434.4180\n",
      "Epoch 647/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38867.3555 - val_loss: 40433.6680\n",
      "Epoch 648/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38865.0391 - val_loss: 40433.1133\n",
      "Epoch 649/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38862.8516 - val_loss: 40432.6367\n",
      "Epoch 650/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38860.5391 - val_loss: 40431.8789\n",
      "Epoch 651/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38858.3555 - val_loss: 40431.0430\n",
      "Epoch 652/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38856.1406 - val_loss: 40430.4180\n",
      "Epoch 653/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38853.9609 - val_loss: 40429.8711\n",
      "Epoch 654/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38851.6836 - val_loss: 40429.6133\n",
      "Epoch 655/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38849.5039 - val_loss: 40429.1289\n",
      "Epoch 656/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38847.3555 - val_loss: 40428.2930\n",
      "Epoch 657/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38845.1523 - val_loss: 40427.2617\n",
      "Epoch 658/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38843.0039 - val_loss: 40426.9570\n",
      "Epoch 659/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38840.8594 - val_loss: 40426.3789\n",
      "Epoch 660/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38838.6719 - val_loss: 40425.3789\n",
      "Epoch 661/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38836.5117 - val_loss: 40424.3789\n",
      "Epoch 662/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38834.4531 - val_loss: 40423.7930\n",
      "Epoch 663/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38832.3125 - val_loss: 40423.0352\n",
      "Epoch 664/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38830.0469 - val_loss: 40422.1289\n",
      "Epoch 665/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38827.9961 - val_loss: 40421.0352\n",
      "Epoch 666/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38825.8594 - val_loss: 40420.6133\n",
      "Epoch 667/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38823.6758 - val_loss: 40419.8867\n",
      "Epoch 668/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38821.5391 - val_loss: 40418.7383\n",
      "Epoch 669/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38819.4844 - val_loss: 40417.7617\n",
      "Epoch 670/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38817.3477 - val_loss: 40416.8867\n",
      "Epoch 671/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38815.1758 - val_loss: 40415.7617\n",
      "Epoch 672/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38813.0391 - val_loss: 40414.8867\n",
      "Epoch 673/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38810.9961 - val_loss: 40414.1211\n",
      "Epoch 674/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38808.8594 - val_loss: 40412.8867\n",
      "Epoch 675/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38806.6875 - val_loss: 40411.5820\n",
      "Epoch 676/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38804.6445 - val_loss: 40410.9102\n",
      "Epoch 677/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38802.5039 - val_loss: 40410.6602\n",
      "Epoch 678/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38800.4609 - val_loss: 40409.6602\n",
      "Epoch 679/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38798.3477 - val_loss: 40407.8633\n",
      "Epoch 680/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38796.1758 - val_loss: 40406.7930\n",
      "Epoch 681/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38794.0469 - val_loss: 40406.2070\n",
      "Epoch 682/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38792.0039 - val_loss: 40405.7383\n",
      "Epoch 683/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38789.9609 - val_loss: 40404.9102\n",
      "Epoch 684/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38787.8477 - val_loss: 40403.9570\n",
      "Epoch 685/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38785.6875 - val_loss: 40403.0820\n",
      "Epoch 686/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38783.6445 - val_loss: 40402.1211\n",
      "Epoch 687/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38781.5117 - val_loss: 40401.1211\n",
      "Epoch 688/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38779.4883 - val_loss: 40400.0820\n",
      "Epoch 689/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38777.3555 - val_loss: 40399.2070\n",
      "Epoch 690/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38775.3242 - val_loss: 40398.3867\n",
      "Epoch 691/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38773.1758 - val_loss: 40397.5117\n",
      "Epoch 692/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38771.1406 - val_loss: 40396.9570\n",
      "Epoch 693/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38769.0391 - val_loss: 40396.0898\n",
      "Epoch 694/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38767.0039 - val_loss: 40394.4180\n",
      "Epoch 695/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38764.9883 - val_loss: 40393.1211\n",
      "Epoch 696/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38762.9609 - val_loss: 40392.6602\n",
      "Epoch 697/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38760.8594 - val_loss: 40392.1680\n",
      "Epoch 698/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38758.8242 - val_loss: 40391.1289\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 699/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38756.8125 - val_loss: 40389.8320\n",
      "Epoch 700/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38754.6758 - val_loss: 40388.7383\n",
      "Epoch 701/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38752.6445 - val_loss: 40387.8320\n",
      "Epoch 702/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38750.5391 - val_loss: 40387.1680\n",
      "Epoch 703/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38748.4961 - val_loss: 40386.1680\n",
      "Epoch 704/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38746.4609 - val_loss: 40385.1211\n",
      "Epoch 705/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38744.3477 - val_loss: 40384.1289\n",
      "Epoch 706/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38742.1875 - val_loss: 40383.1211\n",
      "Epoch 707/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38740.1406 - val_loss: 40382.3320\n",
      "Epoch 708/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38738.0391 - val_loss: 40381.4648\n",
      "Epoch 709/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38735.9961 - val_loss: 40380.6289\n",
      "Epoch 710/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38733.9609 - val_loss: 40379.7148\n",
      "Epoch 711/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38731.8477 - val_loss: 40378.9180\n",
      "Epoch 712/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38729.6875 - val_loss: 40378.1133\n",
      "Epoch 713/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38727.6484 - val_loss: 40376.7617\n",
      "Epoch 714/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38725.5391 - val_loss: 40375.7070\n",
      "Epoch 715/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38723.4961 - val_loss: 40374.7539\n",
      "Epoch 716/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38721.3516 - val_loss: 40374.0820\n",
      "Epoch 717/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38719.1523 - val_loss: 40372.8867\n",
      "Epoch 718/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38717.0039 - val_loss: 40371.8320\n",
      "Epoch 719/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38714.9531 - val_loss: 40371.3711\n",
      "Epoch 720/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38712.8242 - val_loss: 40370.6211\n",
      "Epoch 721/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38710.6445 - val_loss: 40369.6289\n",
      "Epoch 722/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38708.4961 - val_loss: 40369.3320\n",
      "Epoch 723/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38706.3242 - val_loss: 40368.8398\n",
      "Epoch 724/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38704.1406 - val_loss: 40367.8750\n",
      "Epoch 725/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38701.9961 - val_loss: 40366.9180\n",
      "Epoch 726/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38699.8477 - val_loss: 40366.2148\n",
      "Epoch 727/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38697.6523 - val_loss: 40365.0352\n",
      "Epoch 728/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38695.5156 - val_loss: 40364.2930\n",
      "Epoch 729/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38693.4883 - val_loss: 40363.5469\n",
      "Epoch 730/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38691.4609 - val_loss: 40362.5898\n",
      "Epoch 731/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38689.4492 - val_loss: 40361.9570\n",
      "Epoch 732/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38687.4531 - val_loss: 40361.5430\n",
      "Epoch 733/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38685.4844 - val_loss: 40360.7930\n",
      "Epoch 734/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38683.5039 - val_loss: 40359.8711\n",
      "Epoch 735/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38681.5391 - val_loss: 40358.7461\n",
      "Epoch 736/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38679.6523 - val_loss: 40357.8711\n",
      "Epoch 737/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38677.8125 - val_loss: 40357.2148\n",
      "Epoch 738/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38675.8594 - val_loss: 40356.5898\n",
      "Epoch 739/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38673.9609 - val_loss: 40355.9883\n",
      "Epoch 740/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38672.0039 - val_loss: 40355.0820\n",
      "Epoch 741/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38670.0430 - val_loss: 40353.9648\n",
      "Epoch 742/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38668.1523 - val_loss: 40353.0820\n",
      "Epoch 743/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38666.1875 - val_loss: 40352.4180\n",
      "Epoch 744/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38664.3516 - val_loss: 40352.0352\n",
      "Epoch 745/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38662.4531 - val_loss: 40351.0039\n",
      "Epoch 746/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38660.4961 - val_loss: 40349.6289\n",
      "Epoch 747/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38658.5391 - val_loss: 40348.5820\n",
      "Epoch 748/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38656.6523 - val_loss: 40348.1680\n",
      "Epoch 749/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38654.8164 - val_loss: 40347.4883\n",
      "Epoch 750/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38652.8594 - val_loss: 40346.7148\n",
      "Epoch 751/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38650.9961 - val_loss: 40345.8711\n",
      "Epoch 752/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38649.0391 - val_loss: 40345.2070\n",
      "Epoch 753/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38647.1758 - val_loss: 40344.4961\n",
      "Epoch 754/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38645.3242 - val_loss: 40343.5039\n",
      "Epoch 755/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38643.4492 - val_loss: 40342.4102\n",
      "Epoch 756/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38641.4961 - val_loss: 40341.5430\n",
      "Epoch 757/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38639.5039 - val_loss: 40340.9570\n",
      "Epoch 758/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38637.5391 - val_loss: 40340.0820\n",
      "Epoch 759/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38635.6445 - val_loss: 40338.9180\n",
      "Epoch 760/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38633.6836 - val_loss: 40338.0117\n",
      "Epoch 761/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38631.8477 - val_loss: 40337.4961\n",
      "Epoch 762/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38629.9609 - val_loss: 40336.9570\n",
      "Epoch 763/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38628.0039 - val_loss: 40335.7617\n",
      "Epoch 764/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38626.1406 - val_loss: 40334.7617\n",
      "Epoch 765/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38624.3125 - val_loss: 40334.3711\n",
      "Epoch 766/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38622.4492 - val_loss: 40333.9883\n",
      "Epoch 767/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38620.5039 - val_loss: 40332.8867\n",
      "Epoch 768/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38618.6445 - val_loss: 40332.0820\n",
      "Epoch 769/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38616.8164 - val_loss: 40331.1211\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 770/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38614.9609 - val_loss: 40330.0898\n",
      "Epoch 771/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38613.0391 - val_loss: 40329.6211\n",
      "Epoch 772/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38611.1758 - val_loss: 40329.0352\n",
      "Epoch 773/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38609.3555 - val_loss: 40328.0898\n",
      "Epoch 774/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38607.5039 - val_loss: 40326.9570\n",
      "Epoch 775/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38605.6523 - val_loss: 40326.1719\n",
      "Epoch 776/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38603.8516 - val_loss: 40325.8711\n",
      "Epoch 777/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38601.9961 - val_loss: 40325.5117\n",
      "Epoch 778/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38600.0469 - val_loss: 40324.4961\n",
      "Epoch 779/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38598.3125 - val_loss: 40323.0898\n",
      "Epoch 780/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38596.4531 - val_loss: 40322.6367\n",
      "Epoch 781/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38594.5156 - val_loss: 40322.3711\n",
      "Epoch 782/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38592.6523 - val_loss: 40321.5898\n",
      "Epoch 783/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38590.8242 - val_loss: 40320.2852\n",
      "Epoch 784/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38588.9609 - val_loss: 40319.4961\n",
      "Epoch 785/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38587.0156 - val_loss: 40319.0117\n",
      "Epoch 786/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38585.1484 - val_loss: 40318.1211\n",
      "Epoch 787/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38583.3164 - val_loss: 40317.3789\n",
      "Epoch 788/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38581.4492 - val_loss: 40316.5859\n",
      "Epoch 789/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38579.4844 - val_loss: 40315.3633\n",
      "Epoch 790/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38577.5039 - val_loss: 40314.3320\n",
      "Epoch 791/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38575.5391 - val_loss: 40313.5039\n",
      "Epoch 792/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38573.6445 - val_loss: 40313.0352\n",
      "Epoch 793/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38571.6484 - val_loss: 40312.2383\n",
      "Epoch 794/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38569.6523 - val_loss: 40311.1367\n",
      "Epoch 795/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38567.6523 - val_loss: 40310.6094\n",
      "Epoch 796/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38565.6484 - val_loss: 40309.9844\n",
      "Epoch 797/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38563.5508 - val_loss: 40308.7539\n",
      "Epoch 798/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38561.5391 - val_loss: 40308.0039\n",
      "Epoch 799/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38559.4961 - val_loss: 40307.1250\n",
      "Epoch 800/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38557.4531 - val_loss: 40306.2891\n",
      "Epoch 801/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38555.3164 - val_loss: 40305.5781\n",
      "Epoch 802/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38553.1406 - val_loss: 40304.8594\n",
      "Epoch 803/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38551.0039 - val_loss: 40304.0430\n",
      "Epoch 804/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38548.9609 - val_loss: 40302.5039\n",
      "Epoch 805/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38546.8242 - val_loss: 40301.8398\n",
      "Epoch 806/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38544.6758 - val_loss: 40302.0430\n",
      "Epoch 807/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38542.5391 - val_loss: 40301.8750\n",
      "Epoch 808/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38540.4844 - val_loss: 40300.8633\n",
      "Epoch 809/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38538.3555 - val_loss: 40299.9570\n",
      "Epoch 810/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38536.3242 - val_loss: 40299.7539\n",
      "Epoch 811/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38534.1758 - val_loss: 40300.1367\n",
      "Epoch 812/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38532.1484 - val_loss: 40300.1211\n",
      "Epoch 813/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38530.0469 - val_loss: 40299.2070\n",
      "Epoch 814/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38528.0039 - val_loss: 40298.5117\n",
      "Epoch 815/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38525.9883 - val_loss: 40298.2070\n",
      "Epoch 816/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38523.9609 - val_loss: 40298.5391\n",
      "Epoch 817/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38521.9609 - val_loss: 40298.5781\n",
      "Epoch 818/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38519.9531 - val_loss: 40298.4570\n",
      "Epoch 819/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38517.9570 - val_loss: 40297.9961\n",
      "Epoch 820/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38515.9961 - val_loss: 40297.3281\n",
      "Epoch 821/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38514.0156 - val_loss: 40297.1367\n",
      "Epoch 822/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38512.1406 - val_loss: 40296.9961\n",
      "Epoch 823/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38510.1484 - val_loss: 40296.9961\n",
      "Epoch 824/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38508.1758 - val_loss: 40297.0039\n",
      "Epoch 825/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38506.1836 - val_loss: 40296.5430\n",
      "Epoch 826/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38504.3125 - val_loss: 40295.7852\n",
      "Epoch 827/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38502.3242 - val_loss: 40296.1680\n",
      "Epoch 828/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38500.3555 - val_loss: 40296.2617\n",
      "Epoch 829/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38498.4844 - val_loss: 40295.7930\n",
      "Epoch 830/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38496.5117 - val_loss: 40295.0391\n",
      "Epoch 831/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38494.6523 - val_loss: 40295.1094\n",
      "Epoch 832/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38492.8477 - val_loss: 40295.4844\n",
      "Epoch 833/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38491.0039 - val_loss: 40295.1602\n",
      "Epoch 834/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38489.1875 - val_loss: 40294.5352\n",
      "Epoch 835/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38487.4883 - val_loss: 40294.0000\n",
      "Epoch 836/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38485.6758 - val_loss: 40293.8398\n",
      "Epoch 837/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38483.9609 - val_loss: 40293.7383\n",
      "Epoch 838/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38482.1406 - val_loss: 40293.1133\n",
      "Epoch 839/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38480.3555 - val_loss: 40292.0391\n",
      "Epoch 840/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38478.5391 - val_loss: 40291.8398\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 841/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38476.8477 - val_loss: 40292.2539\n",
      "Epoch 842/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38475.0469 - val_loss: 40291.7070\n",
      "Epoch 843/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38473.4492 - val_loss: 40290.2070\n",
      "Epoch 844/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38471.6445 - val_loss: 40289.5117\n",
      "Epoch 845/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38469.9609 - val_loss: 40290.2070\n",
      "Epoch 846/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38468.1523 - val_loss: 40290.0117\n",
      "Epoch 847/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38466.4609 - val_loss: 40288.7461\n",
      "Epoch 848/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38464.6523 - val_loss: 40287.4141\n",
      "Epoch 849/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38462.9844 - val_loss: 40287.2070\n",
      "Epoch 850/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38461.1758 - val_loss: 40287.1094\n",
      "Epoch 851/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38459.4844 - val_loss: 40286.7461\n",
      "Epoch 852/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38457.6758 - val_loss: 40285.5820\n",
      "Epoch 853/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38455.9844 - val_loss: 40284.3711\n",
      "Epoch 854/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38454.1758 - val_loss: 40284.0430\n",
      "Epoch 855/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38452.4844 - val_loss: 40284.1602\n",
      "Epoch 856/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38450.6758 - val_loss: 40284.0117\n",
      "Epoch 857/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38448.9844 - val_loss: 40282.8867\n",
      "Epoch 858/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38447.1797 - val_loss: 40282.0820\n",
      "Epoch 859/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38445.4961 - val_loss: 40281.7031\n",
      "Epoch 860/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38443.8125 - val_loss: 40281.2148\n",
      "Epoch 861/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38442.0391 - val_loss: 40280.1289\n",
      "Epoch 862/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38440.3555 - val_loss: 40279.5039\n",
      "Epoch 863/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38438.6445 - val_loss: 40279.0430\n",
      "Epoch 864/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38436.9609 - val_loss: 40278.3633\n",
      "Epoch 865/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38435.1758 - val_loss: 40277.5117\n",
      "Epoch 866/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38433.4883 - val_loss: 40276.6367\n",
      "Epoch 867/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38431.6875 - val_loss: 40276.1289\n",
      "Epoch 868/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38430.0117 - val_loss: 40275.6719\n",
      "Epoch 869/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38428.3477 - val_loss: 40274.6641\n",
      "Epoch 870/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38426.5391 - val_loss: 40273.7109\n",
      "Epoch 871/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38424.8555 - val_loss: 40273.0117\n",
      "Epoch 872/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38423.0469 - val_loss: 40272.4883\n",
      "Epoch 873/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38421.3555 - val_loss: 40271.7383\n",
      "Epoch 874/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38419.6523 - val_loss: 40270.5469\n",
      "Epoch 875/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38418.0000 - val_loss: 40269.4961\n",
      "Epoch 876/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38416.3477 - val_loss: 40268.6680\n",
      "Epoch 877/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38414.6445 - val_loss: 40267.9883\n",
      "Epoch 878/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38412.9844 - val_loss: 40267.2031\n",
      "Epoch 879/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38411.1875 - val_loss: 40266.3867\n",
      "Epoch 880/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38409.5117 - val_loss: 40265.5859\n",
      "Epoch 881/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38407.8555 - val_loss: 40264.3867\n",
      "Epoch 882/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38406.1758 - val_loss: 40263.7461\n",
      "Epoch 883/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38404.5117 - val_loss: 40262.9961\n",
      "Epoch 884/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38402.9570 - val_loss: 40262.1289\n",
      "Epoch 885/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38401.1914 - val_loss: 40260.9102\n",
      "Epoch 886/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38399.5391 - val_loss: 40259.5000\n",
      "Epoch 887/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38397.9609 - val_loss: 40258.8633\n",
      "Epoch 888/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38396.3125 - val_loss: 40258.1641\n",
      "Epoch 889/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38394.6445 - val_loss: 40257.4102\n",
      "Epoch 890/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38393.0039 - val_loss: 40256.2852\n",
      "Epoch 891/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38391.4492 - val_loss: 40255.4141\n",
      "Epoch 892/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38389.8125 - val_loss: 40254.8711\n",
      "Epoch 893/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38388.1406 - val_loss: 40254.1680\n",
      "Epoch 894/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38386.5039 - val_loss: 40253.1133\n",
      "Epoch 895/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38384.8555 - val_loss: 40252.2930\n",
      "Epoch 896/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38383.1758 - val_loss: 40251.8320\n",
      "Epoch 897/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38381.5039 - val_loss: 40251.7109\n",
      "Epoch 898/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38379.8477 - val_loss: 40250.4102\n",
      "Epoch 899/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38378.1406 - val_loss: 40249.1211\n",
      "Epoch 900/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38376.4844 - val_loss: 40248.4102\n",
      "Epoch 901/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38374.8125 - val_loss: 40247.8320\n",
      "Epoch 902/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38373.0430 - val_loss: 40247.0898\n",
      "Epoch 903/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38371.4609 - val_loss: 40246.2461\n",
      "Epoch 904/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38369.9531 - val_loss: 40245.7969\n",
      "Epoch 905/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38368.3477 - val_loss: 40245.2148\n",
      "Epoch 906/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38366.8125 - val_loss: 40245.2344\n",
      "Epoch 907/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38365.1523 - val_loss: 40245.1211\n",
      "Epoch 908/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38363.5039 - val_loss: 40244.2617\n",
      "Epoch 909/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38361.8594 - val_loss: 40243.4219\n",
      "Epoch 910/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38360.1523 - val_loss: 40243.4180\n",
      "Epoch 911/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38358.5000 - val_loss: 40243.5352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 912/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38356.8477 - val_loss: 40243.1289\n",
      "Epoch 913/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38355.1445 - val_loss: 40242.1211\n",
      "Epoch 914/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38353.5156 - val_loss: 40241.2930\n",
      "Epoch 915/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38351.9844 - val_loss: 40241.1680\n",
      "Epoch 916/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38350.3477 - val_loss: 40240.7617\n",
      "Epoch 917/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38348.6758 - val_loss: 40239.0898\n",
      "Epoch 918/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38347.0391 - val_loss: 40237.4570\n",
      "Epoch 919/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38345.4844 - val_loss: 40237.1602\n",
      "Epoch 920/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38343.8477 - val_loss: 40236.7930\n",
      "Epoch 921/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38342.1758 - val_loss: 40235.3906\n",
      "Epoch 922/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38340.5391 - val_loss: 40234.0352\n",
      "Epoch 923/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38338.9844 - val_loss: 40233.7617\n",
      "Epoch 924/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38337.3242 - val_loss: 40233.6367\n",
      "Epoch 925/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38335.6445 - val_loss: 40232.4648\n",
      "Epoch 926/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38333.9961 - val_loss: 40231.4102\n",
      "Epoch 927/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38332.3477 - val_loss: 40231.3281\n",
      "Epoch 928/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38330.6523 - val_loss: 40231.5820\n",
      "Epoch 929/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38328.9961 - val_loss: 40231.2539\n",
      "Epoch 930/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38327.3242 - val_loss: 40230.5430\n",
      "Epoch 931/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38325.6445 - val_loss: 40230.1367\n",
      "Epoch 932/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38323.9844 - val_loss: 40229.9531\n",
      "Epoch 933/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38322.3125 - val_loss: 40230.0781\n",
      "Epoch 934/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38320.5391 - val_loss: 40230.1367\n",
      "Epoch 935/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38318.8555 - val_loss: 40229.2070\n",
      "Epoch 936/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38317.1445 - val_loss: 40228.2070\n",
      "Epoch 937/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38315.4844 - val_loss: 40228.0039\n",
      "Epoch 938/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38313.6875 - val_loss: 40228.0156\n",
      "Epoch 939/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38312.0039 - val_loss: 40227.0859\n",
      "Epoch 940/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38310.3477 - val_loss: 40225.6406\n",
      "Epoch 941/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38308.5508 - val_loss: 40225.3359\n",
      "Epoch 942/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38306.9531 - val_loss: 40225.0391\n",
      "Epoch 943/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38305.1523 - val_loss: 40224.1289\n",
      "Epoch 944/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38303.4961 - val_loss: 40222.9141\n",
      "Epoch 945/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38301.8242 - val_loss: 40221.6680\n",
      "Epoch 946/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38300.1797 - val_loss: 40222.1211\n",
      "Epoch 947/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38298.5508 - val_loss: 40222.3867\n",
      "Epoch 948/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38297.0039 - val_loss: 40221.3359\n",
      "Epoch 949/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38295.4844 - val_loss: 40220.0430\n",
      "Epoch 950/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38293.8594 - val_loss: 40220.0820\n",
      "Epoch 951/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38292.3203 - val_loss: 40220.7109\n",
      "Epoch 952/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38290.6797 - val_loss: 40219.5352\n",
      "Epoch 953/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38289.1445 - val_loss: 40218.1094\n",
      "Epoch 954/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38287.5469 - val_loss: 40218.2383\n",
      "Epoch 955/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38286.0391 - val_loss: 40218.2969\n",
      "Epoch 956/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38284.5117 - val_loss: 40217.8359\n",
      "Epoch 957/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38283.0000 - val_loss: 40216.9180\n",
      "Epoch 958/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38281.4961 - val_loss: 40216.2383\n",
      "Epoch 959/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38279.9883 - val_loss: 40216.0117\n",
      "Epoch 960/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38278.4844 - val_loss: 40215.7461\n",
      "Epoch 961/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38276.9609 - val_loss: 40214.7852\n",
      "Epoch 962/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38275.4531 - val_loss: 40213.8594\n",
      "Epoch 963/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38273.8594 - val_loss: 40213.4570\n",
      "Epoch 964/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38272.3555 - val_loss: 40213.4570\n",
      "Epoch 965/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38270.8477 - val_loss: 40212.7539\n",
      "Epoch 966/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38269.3203 - val_loss: 40211.2891\n",
      "Epoch 967/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38267.8086 - val_loss: 40210.3359\n",
      "Epoch 968/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38266.1797 - val_loss: 40210.2539\n",
      "Epoch 969/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38264.6523 - val_loss: 40209.9648\n",
      "Epoch 970/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38263.1445 - val_loss: 40208.4648\n",
      "Epoch 971/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38261.5469 - val_loss: 40207.8281\n",
      "Epoch 972/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38260.0391 - val_loss: 40208.5039\n",
      "Epoch 973/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38258.5039 - val_loss: 40207.7148\n",
      "Epoch 974/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38256.9961 - val_loss: 40206.0430\n",
      "Epoch 975/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38255.4844 - val_loss: 40205.1719\n",
      "Epoch 976/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38253.9609 - val_loss: 40205.9648\n",
      "Epoch 977/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38252.4492 - val_loss: 40205.5469\n",
      "Epoch 978/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38250.8555 - val_loss: 40203.5820\n",
      "Epoch 979/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38249.3477 - val_loss: 40202.4102\n",
      "Epoch 980/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38247.8242 - val_loss: 40202.5039\n",
      "Epoch 981/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38246.3125 - val_loss: 40202.7383\n",
      "Epoch 982/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38244.6875 - val_loss: 40201.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 983/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38243.1875 - val_loss: 40199.8320\n",
      "Epoch 984/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38241.6758 - val_loss: 40200.0039\n",
      "Epoch 985/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38240.1523 - val_loss: 40199.7070\n",
      "Epoch 986/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38238.6445 - val_loss: 40198.4961\n",
      "Epoch 987/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38237.0469 - val_loss: 40197.7930\n",
      "Epoch 988/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38235.5391 - val_loss: 40197.9883\n",
      "Epoch 989/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38234.0117 - val_loss: 40197.4570\n",
      "Epoch 990/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38232.5039 - val_loss: 40196.4531\n",
      "Epoch 991/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38230.9961 - val_loss: 40195.7383\n",
      "Epoch 992/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38229.4844 - val_loss: 40195.5039\n",
      "Epoch 993/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38227.9570 - val_loss: 40195.1719\n",
      "Epoch 994/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38226.3555 - val_loss: 40194.3867\n",
      "Epoch 995/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38224.8477 - val_loss: 40194.4648\n",
      "Epoch 996/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38223.3203 - val_loss: 40194.2070\n",
      "Epoch 997/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38221.6875 - val_loss: 40192.9609\n",
      "Epoch 998/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38220.1797 - val_loss: 40192.0039\n",
      "Epoch 999/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38218.6445 - val_loss: 40191.9531\n",
      "Epoch 1000/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38217.0430 - val_loss: 40191.3789\n",
      "Epoch 1001/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38215.5156 - val_loss: 40190.6289\n",
      "Epoch 1002/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38214.0000 - val_loss: 40190.5391\n",
      "Epoch 1003/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38212.4844 - val_loss: 40190.2930\n",
      "Epoch 1004/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38210.9531 - val_loss: 40189.4180\n",
      "Epoch 1005/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38209.3477 - val_loss: 40188.3711\n",
      "Epoch 1006/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38207.8125 - val_loss: 40187.8281\n",
      "Epoch 1007/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38206.1875 - val_loss: 40187.5820\n",
      "Epoch 1008/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38204.6523 - val_loss: 40187.0000\n",
      "Epoch 1009/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38203.0430 - val_loss: 40185.7852\n",
      "Epoch 1010/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38201.5117 - val_loss: 40185.5000\n",
      "Epoch 1011/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38199.9961 - val_loss: 40185.2617\n",
      "Epoch 1012/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38198.4609 - val_loss: 40184.0156\n",
      "Epoch 1013/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38196.8555 - val_loss: 40183.0039\n",
      "Epoch 1014/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38195.3125 - val_loss: 40182.8867\n",
      "Epoch 1015/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38193.6758 - val_loss: 40182.8789\n",
      "Epoch 1016/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38192.0469 - val_loss: 40181.5156\n",
      "Epoch 1017/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38190.5117 - val_loss: 40180.4219\n",
      "Epoch 1018/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38188.9961 - val_loss: 40181.1367\n",
      "Epoch 1019/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38187.4531 - val_loss: 40180.5859\n",
      "Epoch 1020/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38185.8242 - val_loss: 40178.4141\n",
      "Epoch 1021/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38184.1875 - val_loss: 40177.8789\n",
      "Epoch 1022/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38182.6523 - val_loss: 40178.9883\n",
      "Epoch 1023/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38181.0430 - val_loss: 40178.2461\n",
      "Epoch 1024/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38179.5039 - val_loss: 40176.5039\n",
      "Epoch 1025/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38177.9883 - val_loss: 40176.0781\n",
      "Epoch 1026/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38176.3555 - val_loss: 40176.2656\n",
      "Epoch 1027/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38174.8125 - val_loss: 40175.1289\n",
      "Epoch 1028/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38173.1445 - val_loss: 40173.9609\n",
      "Epoch 1029/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38171.5156 - val_loss: 40173.7852\n",
      "Epoch 1030/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38169.9609 - val_loss: 40173.7148\n",
      "Epoch 1031/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38168.3203 - val_loss: 40173.7500\n",
      "Epoch 1032/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38166.6445 - val_loss: 40172.9531\n",
      "Epoch 1033/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38164.9961 - val_loss: 40172.2461\n",
      "Epoch 1034/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38163.3555 - val_loss: 40171.5898\n",
      "Epoch 1035/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38161.6797 - val_loss: 40170.8867\n",
      "Epoch 1036/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38160.0156 - val_loss: 40170.0352\n",
      "Epoch 1037/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38158.4609 - val_loss: 40168.8594\n",
      "Epoch 1038/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38156.8125 - val_loss: 40168.8711\n",
      "Epoch 1039/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38155.0469 - val_loss: 40168.5430\n",
      "Epoch 1040/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38153.4961 - val_loss: 40167.4961\n",
      "Epoch 1041/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38151.8594 - val_loss: 40167.0039\n",
      "Epoch 1042/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38150.3203 - val_loss: 40166.8711\n",
      "Epoch 1043/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38148.6523 - val_loss: 40165.8867\n",
      "Epoch 1044/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38147.0391 - val_loss: 40165.3633\n",
      "Epoch 1045/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38145.4844 - val_loss: 40165.3633\n",
      "Epoch 1046/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38143.9570 - val_loss: 40164.3281\n",
      "Epoch 1047/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38142.3477 - val_loss: 40163.2383\n",
      "Epoch 1048/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38140.8203 - val_loss: 40163.1211\n",
      "Epoch 1049/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38139.1914 - val_loss: 40163.3711\n",
      "Epoch 1050/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38137.6875 - val_loss: 40162.5820\n",
      "Epoch 1051/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38136.1797 - val_loss: 40161.8320\n",
      "Epoch 1052/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38134.6758 - val_loss: 40161.5781\n",
      "Epoch 1053/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38133.1758 - val_loss: 40161.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1054/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38131.6875 - val_loss: 40161.2148\n",
      "Epoch 1055/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38130.3125 - val_loss: 40160.8789\n",
      "Epoch 1056/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38128.8203 - val_loss: 40160.6133\n",
      "Epoch 1057/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38127.3203 - val_loss: 40159.9883\n",
      "Epoch 1058/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38125.8203 - val_loss: 40159.4219\n",
      "Epoch 1059/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38124.3203 - val_loss: 40159.0430\n",
      "Epoch 1060/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38122.8203 - val_loss: 40159.2461\n",
      "Epoch 1061/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38121.3125 - val_loss: 40158.5117\n",
      "Epoch 1062/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38119.8086 - val_loss: 40157.9570\n",
      "Epoch 1063/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38118.1875 - val_loss: 40158.1289\n",
      "Epoch 1064/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38116.6797 - val_loss: 40157.3320\n",
      "Epoch 1065/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38115.1797 - val_loss: 40157.1133\n",
      "Epoch 1066/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38113.6523 - val_loss: 40156.0898\n",
      "Epoch 1067/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38112.1445 - val_loss: 40155.5430\n",
      "Epoch 1068/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38110.5508 - val_loss: 40155.7148\n",
      "Epoch 1069/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38109.0469 - val_loss: 40155.1602\n",
      "Epoch 1070/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38107.5469 - val_loss: 40155.0039\n",
      "Epoch 1071/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38106.0391 - val_loss: 40154.7500\n",
      "Epoch 1072/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38104.5156 - val_loss: 40153.0039\n",
      "Epoch 1073/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38103.0156 - val_loss: 40152.7539\n",
      "Epoch 1074/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38101.5117 - val_loss: 40153.0430\n",
      "Epoch 1075/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38100.0117 - val_loss: 40152.3711\n",
      "Epoch 1076/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38098.5039 - val_loss: 40151.3906\n",
      "Epoch 1077/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38097.0039 - val_loss: 40151.2383\n",
      "Epoch 1078/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38095.4961 - val_loss: 40151.1641\n",
      "Epoch 1079/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38093.9961 - val_loss: 40150.8867\n",
      "Epoch 1080/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38092.4883 - val_loss: 40150.1641\n",
      "Epoch 1081/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38090.9883 - val_loss: 40149.7070\n",
      "Epoch 1082/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38089.4844 - val_loss: 40149.6133\n",
      "Epoch 1083/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38087.9844 - val_loss: 40149.7109\n",
      "Epoch 1084/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38086.4844 - val_loss: 40148.8789\n",
      "Epoch 1085/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38084.9844 - val_loss: 40148.6211\n",
      "Epoch 1086/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38083.4844 - val_loss: 40148.4883\n",
      "Epoch 1087/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38081.9844 - val_loss: 40147.5352\n",
      "Epoch 1088/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38080.4844 - val_loss: 40146.5039\n",
      "Epoch 1089/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38078.9844 - val_loss: 40146.7500\n",
      "Epoch 1090/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38077.4844 - val_loss: 40146.0039\n",
      "Epoch 1091/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38075.9844 - val_loss: 40145.2383\n",
      "Epoch 1092/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38074.4844 - val_loss: 40144.6250\n",
      "Epoch 1093/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38072.9883 - val_loss: 40145.0430\n",
      "Epoch 1094/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38071.4961 - val_loss: 40144.8633\n",
      "Epoch 1095/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38069.9961 - val_loss: 40143.2852\n",
      "Epoch 1096/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38068.4961 - val_loss: 40142.8789\n",
      "Epoch 1097/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38066.9961 - val_loss: 40143.1602\n",
      "Epoch 1098/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38065.5000 - val_loss: 40142.4648\n",
      "Epoch 1099/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38064.0039 - val_loss: 40141.2383\n",
      "Epoch 1100/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38062.5039 - val_loss: 40141.4180\n",
      "Epoch 1101/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38061.0039 - val_loss: 40141.4570\n",
      "Epoch 1102/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38059.5156 - val_loss: 40140.1719\n",
      "Epoch 1103/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38058.0156 - val_loss: 40139.4883\n",
      "Epoch 1104/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38056.5391 - val_loss: 40139.7539\n",
      "Epoch 1105/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38055.0430 - val_loss: 40139.4961\n",
      "Epoch 1106/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38053.5508 - val_loss: 40138.1133\n",
      "Epoch 1107/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38052.1445 - val_loss: 40138.0430\n",
      "Epoch 1108/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38050.6445 - val_loss: 40138.4570\n",
      "Epoch 1109/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38049.1523 - val_loss: 40137.7539\n",
      "Epoch 1110/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38047.6562 - val_loss: 40136.9102\n",
      "Epoch 1111/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38046.1797 - val_loss: 40136.5352\n",
      "Epoch 1112/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38044.6797 - val_loss: 40136.1406\n",
      "Epoch 1113/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38043.1875 - val_loss: 40135.4883\n",
      "Epoch 1114/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38041.8086 - val_loss: 40135.1289\n",
      "Epoch 1115/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38040.3086 - val_loss: 40134.5430\n",
      "Epoch 1116/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38038.8203 - val_loss: 40133.7930\n",
      "Epoch 1117/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38037.3203 - val_loss: 40133.7539\n",
      "Epoch 1118/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38035.8438 - val_loss: 40133.5039\n",
      "Epoch 1119/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38034.3555 - val_loss: 40132.6094\n",
      "Epoch 1120/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38032.8555 - val_loss: 40132.1602\n",
      "Epoch 1121/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38031.4492 - val_loss: 40132.6602\n",
      "Epoch 1122/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38029.9609 - val_loss: 40132.3398\n",
      "Epoch 1123/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38028.4609 - val_loss: 40131.4570\n",
      "Epoch 1124/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 20us/step - loss: 38026.9844 - val_loss: 40131.4648\n",
      "Epoch 1125/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38025.4961 - val_loss: 40131.3906\n",
      "Epoch 1126/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38023.9961 - val_loss: 40130.1211\n",
      "Epoch 1127/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38022.5039 - val_loss: 40129.9883\n",
      "Epoch 1128/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38021.0039 - val_loss: 40129.3867\n",
      "Epoch 1129/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38019.5117 - val_loss: 40128.0039\n",
      "Epoch 1130/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38018.0391 - val_loss: 40128.4883\n",
      "Epoch 1131/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38016.5469 - val_loss: 40128.6211\n",
      "Epoch 1132/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38015.0430 - val_loss: 40127.2617\n",
      "Epoch 1133/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38013.5508 - val_loss: 40126.7969\n",
      "Epoch 1134/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38012.1445 - val_loss: 40127.5430\n",
      "Epoch 1135/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38010.6562 - val_loss: 40126.5859\n",
      "Epoch 1136/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38009.1562 - val_loss: 40125.3320\n",
      "Epoch 1137/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38007.6797 - val_loss: 40125.2617\n",
      "Epoch 1138/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38006.1797 - val_loss: 40124.7344\n",
      "Epoch 1139/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38004.6797 - val_loss: 40123.4219\n",
      "Epoch 1140/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38003.1797 - val_loss: 40123.2148\n",
      "Epoch 1141/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38001.6797 - val_loss: 40123.0898\n",
      "Epoch 1142/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38000.1797 - val_loss: 40122.8789\n",
      "Epoch 1143/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37998.6797 - val_loss: 40122.0781\n",
      "Epoch 1144/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37997.1562 - val_loss: 40122.2109\n",
      "Epoch 1145/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37995.6562 - val_loss: 40122.2930\n",
      "Epoch 1146/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37994.1562 - val_loss: 40119.7500\n",
      "Epoch 1147/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37992.6562 - val_loss: 40119.4844\n",
      "Epoch 1148/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37991.1523 - val_loss: 40119.9180\n",
      "Epoch 1149/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37989.6562 - val_loss: 40119.0898\n",
      "Epoch 1150/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37988.1797 - val_loss: 40117.0898\n",
      "Epoch 1151/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37986.6797 - val_loss: 40116.7461\n",
      "Epoch 1152/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37985.1797 - val_loss: 40118.5781\n",
      "Epoch 1153/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37983.6562 - val_loss: 40117.8320\n",
      "Epoch 1154/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37982.1797 - val_loss: 40116.0898\n",
      "Epoch 1155/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37980.6797 - val_loss: 40115.9102\n",
      "Epoch 1156/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37979.1562 - val_loss: 40115.0898\n",
      "Epoch 1157/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37977.6562 - val_loss: 40114.2070\n",
      "Epoch 1158/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37976.1562 - val_loss: 40114.4102\n",
      "Epoch 1159/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37974.6562 - val_loss: 40114.2461\n",
      "Epoch 1160/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37973.1562 - val_loss: 40113.9219\n",
      "Epoch 1161/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37971.6562 - val_loss: 40113.5898\n",
      "Epoch 1162/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37970.1523 - val_loss: 40112.3906\n",
      "Epoch 1163/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37968.6445 - val_loss: 40111.8281\n",
      "Epoch 1164/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37967.1523 - val_loss: 40112.3359\n",
      "Epoch 1165/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37965.6445 - val_loss: 40111.3711\n",
      "Epoch 1166/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37964.1445 - val_loss: 40111.8867\n",
      "Epoch 1167/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37962.6445 - val_loss: 40111.2891\n",
      "Epoch 1168/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37961.1445 - val_loss: 40109.5430\n",
      "Epoch 1169/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37959.5508 - val_loss: 40109.3281\n",
      "Epoch 1170/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37958.0430 - val_loss: 40108.7461\n",
      "Epoch 1171/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37956.5391 - val_loss: 40108.7852\n",
      "Epoch 1172/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37955.0391 - val_loss: 40107.6719\n",
      "Epoch 1173/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37953.5391 - val_loss: 40107.6133\n",
      "Epoch 1174/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37952.0156 - val_loss: 40107.4570\n",
      "Epoch 1175/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37950.5156 - val_loss: 40105.8320\n",
      "Epoch 1176/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37949.0039 - val_loss: 40106.0391\n",
      "Epoch 1177/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37947.5039 - val_loss: 40105.6133\n",
      "Epoch 1178/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37945.9961 - val_loss: 40104.7070\n",
      "Epoch 1179/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37944.4883 - val_loss: 40104.0898\n",
      "Epoch 1180/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37942.9844 - val_loss: 40103.9180\n",
      "Epoch 1181/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37941.4531 - val_loss: 40103.4102\n",
      "Epoch 1182/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37939.9570 - val_loss: 40102.0352\n",
      "Epoch 1183/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37938.4492 - val_loss: 40102.4102\n",
      "Epoch 1184/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37936.8555 - val_loss: 40102.1211\n",
      "Epoch 1185/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37935.3203 - val_loss: 40099.8789\n",
      "Epoch 1186/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37933.8125 - val_loss: 40099.2070\n",
      "Epoch 1187/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37932.1914 - val_loss: 40100.1406\n",
      "Epoch 1188/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37930.6797 - val_loss: 40099.6680\n",
      "Epoch 1189/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37929.1523 - val_loss: 40098.3320\n",
      "Epoch 1190/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37927.6445 - val_loss: 40099.0391\n",
      "Epoch 1191/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37926.0391 - val_loss: 40098.5117\n",
      "Epoch 1192/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37924.5117 - val_loss: 40096.3398\n",
      "Epoch 1193/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37922.9961 - val_loss: 40096.5430\n",
      "Epoch 1194/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37921.4844 - val_loss: 40096.8320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1195/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37919.9570 - val_loss: 40094.9961\n",
      "Epoch 1196/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37918.3477 - val_loss: 40094.9648\n",
      "Epoch 1197/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37916.8125 - val_loss: 40095.8906\n",
      "Epoch 1198/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37915.1797 - val_loss: 40094.2070\n",
      "Epoch 1199/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37913.6445 - val_loss: 40093.1289\n",
      "Epoch 1200/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37912.0391 - val_loss: 40092.9570\n",
      "Epoch 1201/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37910.4961 - val_loss: 40091.4570\n",
      "Epoch 1202/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37908.9609 - val_loss: 40090.4961\n",
      "Epoch 1203/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37907.3438 - val_loss: 40089.9570\n",
      "Epoch 1204/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37905.6914 - val_loss: 40089.6211\n",
      "Epoch 1205/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37904.1445 - val_loss: 40089.6211\n",
      "Epoch 1206/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37902.5156 - val_loss: 40088.8398\n",
      "Epoch 1207/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37900.9883 - val_loss: 40089.2539\n",
      "Epoch 1208/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37899.3555 - val_loss: 40088.9570\n",
      "Epoch 1209/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37897.8125 - val_loss: 40088.0039\n",
      "Epoch 1210/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37896.1562 - val_loss: 40085.9883\n",
      "Epoch 1211/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37894.5391 - val_loss: 40086.2617\n",
      "Epoch 1212/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37892.9961 - val_loss: 40084.7930\n",
      "Epoch 1213/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37891.4492 - val_loss: 40083.4141\n",
      "Epoch 1214/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37889.8125 - val_loss: 40084.6211\n",
      "Epoch 1215/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37888.1562 - val_loss: 40083.3398\n",
      "Epoch 1216/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37886.5391 - val_loss: 40081.2031\n",
      "Epoch 1217/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37884.9961 - val_loss: 40082.2930\n",
      "Epoch 1218/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37883.4492 - val_loss: 40082.8398\n",
      "Epoch 1219/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37881.8125 - val_loss: 40079.7383\n",
      "Epoch 1220/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37880.1562 - val_loss: 40079.2148\n",
      "Epoch 1221/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37878.5391 - val_loss: 40081.3711\n",
      "Epoch 1222/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37876.9844 - val_loss: 40079.1289\n",
      "Epoch 1223/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37875.3203 - val_loss: 40076.0781\n",
      "Epoch 1224/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37873.6445 - val_loss: 40078.0156\n",
      "Epoch 1225/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37871.9844 - val_loss: 40077.4180\n",
      "Epoch 1226/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37870.3125 - val_loss: 40073.8359\n",
      "Epoch 1227/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37868.5391 - val_loss: 40074.5391\n",
      "Epoch 1228/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37866.8555 - val_loss: 40076.4219\n",
      "Epoch 1229/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37865.1445 - val_loss: 40073.1680\n",
      "Epoch 1230/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37863.4609 - val_loss: 40071.3359\n",
      "Epoch 1231/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37861.6797 - val_loss: 40074.3359\n",
      "Epoch 1232/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37859.9883 - val_loss: 40071.4219\n",
      "Epoch 1233/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37858.1875 - val_loss: 40069.1289\n",
      "Epoch 1234/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37856.4844 - val_loss: 40070.0781\n",
      "Epoch 1235/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37854.6445 - val_loss: 40070.4219\n",
      "Epoch 1236/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37852.8438 - val_loss: 40068.2930\n",
      "Epoch 1237/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37850.9609 - val_loss: 40067.5898\n",
      "Epoch 1238/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37849.0039 - val_loss: 40068.2070\n",
      "Epoch 1239/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37847.0430 - val_loss: 40066.5820\n",
      "Epoch 1240/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37845.1445 - val_loss: 40065.5039\n",
      "Epoch 1241/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37843.1445 - val_loss: 40065.7617\n",
      "Epoch 1242/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37841.1445 - val_loss: 40065.1250\n",
      "Epoch 1243/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37839.1797 - val_loss: 40064.0039\n",
      "Epoch 1244/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37837.3203 - val_loss: 40064.0820\n",
      "Epoch 1245/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37835.4844 - val_loss: 40063.2070\n",
      "Epoch 1246/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37833.6797 - val_loss: 40062.3906\n",
      "Epoch 1247/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37831.9883 - val_loss: 40062.2656\n",
      "Epoch 1248/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37830.3203 - val_loss: 40061.5352\n",
      "Epoch 1249/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37828.5469 - val_loss: 40061.0469\n",
      "Epoch 1250/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37826.9531 - val_loss: 40060.7539\n",
      "Epoch 1251/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37825.0469 - val_loss: 40060.4844\n",
      "Epoch 1252/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37823.3203 - val_loss: 40059.8867\n",
      "Epoch 1253/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37821.4609 - val_loss: 40059.9102\n",
      "Epoch 1254/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37819.5117 - val_loss: 40059.4570\n",
      "Epoch 1255/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37817.5508 - val_loss: 40059.2617\n",
      "Epoch 1256/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37815.6875 - val_loss: 40058.5430\n",
      "Epoch 1257/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37813.8555 - val_loss: 40057.2930\n",
      "Epoch 1258/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37811.9961 - val_loss: 40057.3281\n",
      "Epoch 1259/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37810.1445 - val_loss: 40056.3320\n",
      "Epoch 1260/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37808.3555 - val_loss: 40054.9570\n",
      "Epoch 1261/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37806.5156 - val_loss: 40055.0430\n",
      "Epoch 1262/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37804.8125 - val_loss: 40054.4844\n",
      "Epoch 1263/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37802.9961 - val_loss: 40052.9961\n",
      "Epoch 1264/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37801.1797 - val_loss: 40053.1602\n",
      "Epoch 1265/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 37799.5000 - val_loss: 40053.6211\n",
      "Epoch 1266/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37797.8125 - val_loss: 40051.4219\n",
      "Epoch 1267/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37796.0156 - val_loss: 40051.3867\n",
      "Epoch 1268/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37794.3203 - val_loss: 40050.8320\n",
      "Epoch 1269/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37792.5039 - val_loss: 40049.6680\n",
      "Epoch 1270/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37790.8125 - val_loss: 40049.5000\n",
      "Epoch 1271/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37788.9883 - val_loss: 40048.3789\n",
      "Epoch 1272/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37787.1562 - val_loss: 40048.3711\n",
      "Epoch 1273/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37785.4844 - val_loss: 40047.6289\n",
      "Epoch 1274/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37783.6914 - val_loss: 40046.8867\n",
      "Epoch 1275/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37782.0039 - val_loss: 40046.1406\n",
      "Epoch 1276/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37780.3438 - val_loss: 40046.0898\n",
      "Epoch 1277/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37778.6484 - val_loss: 40045.4570\n",
      "Epoch 1278/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37776.9961 - val_loss: 40045.2539\n",
      "Epoch 1279/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37775.3164 - val_loss: 40045.7461\n",
      "Epoch 1280/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37773.5508 - val_loss: 40044.9648\n",
      "Epoch 1281/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37771.9609 - val_loss: 40044.1367\n",
      "Epoch 1282/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37770.1875 - val_loss: 40044.2539\n",
      "Epoch 1283/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37768.5156 - val_loss: 40042.6680\n",
      "Epoch 1284/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37766.8555 - val_loss: 40042.2500\n",
      "Epoch 1285/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37765.1797 - val_loss: 40042.7148\n",
      "Epoch 1286/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37763.5039 - val_loss: 40040.6719\n",
      "Epoch 1287/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37761.8438 - val_loss: 40040.5820\n",
      "Epoch 1288/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37760.1445 - val_loss: 40040.4961\n",
      "Epoch 1289/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37758.4844 - val_loss: 40038.7539\n",
      "Epoch 1290/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37756.6914 - val_loss: 40039.5156\n",
      "Epoch 1291/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37755.0039 - val_loss: 40038.7148\n",
      "Epoch 1292/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37753.3086 - val_loss: 40036.9570\n",
      "Epoch 1293/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37751.5039 - val_loss: 40037.5117\n",
      "Epoch 1294/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37749.8125 - val_loss: 40037.1289\n",
      "Epoch 1295/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37748.0000 - val_loss: 40034.7461\n",
      "Epoch 1296/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37746.3086 - val_loss: 40035.6641\n",
      "Epoch 1297/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37744.5039 - val_loss: 40035.2617\n",
      "Epoch 1298/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37742.6914 - val_loss: 40033.3789\n",
      "Epoch 1299/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37740.9844 - val_loss: 40033.8750\n",
      "Epoch 1300/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37739.1484 - val_loss: 40033.6367\n",
      "Epoch 1301/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37737.4492 - val_loss: 40030.1367\n",
      "Epoch 1302/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37735.5508 - val_loss: 40031.3359\n",
      "Epoch 1303/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37733.8516 - val_loss: 40030.8320\n",
      "Epoch 1304/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37732.0430 - val_loss: 40028.6641\n",
      "Epoch 1305/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37730.3438 - val_loss: 40029.8633\n",
      "Epoch 1306/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37728.5156 - val_loss: 40029.8867\n",
      "Epoch 1307/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37726.8164 - val_loss: 40028.6367\n",
      "Epoch 1308/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37725.0117 - val_loss: 40028.4570\n",
      "Epoch 1309/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37723.3203 - val_loss: 40027.6289\n",
      "Epoch 1310/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37721.5117 - val_loss: 40026.5117\n",
      "Epoch 1311/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37719.8438 - val_loss: 40026.4531\n",
      "Epoch 1312/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37718.0391 - val_loss: 40026.1289\n",
      "Epoch 1313/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37716.3438 - val_loss: 40025.8281\n",
      "Epoch 1314/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37714.5391 - val_loss: 40025.5000\n",
      "Epoch 1315/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37712.8438 - val_loss: 40025.3906\n",
      "Epoch 1316/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37711.0156 - val_loss: 40024.7461\n",
      "Epoch 1317/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37709.3086 - val_loss: 40024.5039\n",
      "Epoch 1318/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37707.5039 - val_loss: 40024.4570\n",
      "Epoch 1319/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37705.8125 - val_loss: 40024.8867\n",
      "Epoch 1320/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37703.9961 - val_loss: 40024.0117\n",
      "Epoch 1321/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37702.1875 - val_loss: 40024.7070\n",
      "Epoch 1322/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37700.4961 - val_loss: 40023.1289\n",
      "Epoch 1323/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37698.6914 - val_loss: 40023.0430\n",
      "Epoch 1324/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37696.9609 - val_loss: 40021.5430\n",
      "Epoch 1325/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37695.1484 - val_loss: 40021.1680\n",
      "Epoch 1326/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37693.3555 - val_loss: 40021.5117\n",
      "Epoch 1327/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37691.5156 - val_loss: 40021.7109\n",
      "Epoch 1328/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37689.8125 - val_loss: 40019.9219\n",
      "Epoch 1329/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37687.9961 - val_loss: 40019.6680\n",
      "Epoch 1330/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37686.1484 - val_loss: 40019.6250\n",
      "Epoch 1331/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37684.3203 - val_loss: 40017.6211\n",
      "Epoch 1332/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37682.4961 - val_loss: 40017.8633\n",
      "Epoch 1333/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37680.6484 - val_loss: 40015.4961\n",
      "Epoch 1334/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37678.8203 - val_loss: 40014.7930\n",
      "Epoch 1335/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37676.9883 - val_loss: 40016.4883\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1336/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37675.0430 - val_loss: 40013.6680\n",
      "Epoch 1337/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37673.1875 - val_loss: 40012.5352\n",
      "Epoch 1338/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37671.4531 - val_loss: 40013.0430\n",
      "Epoch 1339/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37669.5156 - val_loss: 40010.6719\n",
      "Epoch 1340/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37667.6836 - val_loss: 40008.9648\n",
      "Epoch 1341/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37665.9609 - val_loss: 40010.0000\n",
      "Epoch 1342/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37664.1445 - val_loss: 40007.8359\n",
      "Epoch 1343/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37662.3203 - val_loss: 40006.9219\n",
      "Epoch 1344/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37660.5039 - val_loss: 40008.3711\n",
      "Epoch 1345/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37658.6797 - val_loss: 40006.0117\n",
      "Epoch 1346/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37656.9531 - val_loss: 40006.3398\n",
      "Epoch 1347/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37655.0156 - val_loss: 40006.7852\n",
      "Epoch 1348/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37653.1875 - val_loss: 40003.6406\n",
      "Epoch 1349/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37651.4492 - val_loss: 40004.0898\n",
      "Epoch 1350/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37649.5117 - val_loss: 40003.6602\n",
      "Epoch 1351/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37647.6797 - val_loss: 40001.1289\n",
      "Epoch 1352/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37645.8438 - val_loss: 40001.7461\n",
      "Epoch 1353/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37643.9961 - val_loss: 40000.9883\n",
      "Epoch 1354/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37642.0430 - val_loss: 39999.2461\n",
      "Epoch 1355/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37640.1875 - val_loss: 39999.8320\n",
      "Epoch 1356/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37638.3516 - val_loss: 39998.2383\n",
      "Epoch 1357/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37636.4883 - val_loss: 39999.1094\n",
      "Epoch 1358/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37634.5156 - val_loss: 39998.2852\n",
      "Epoch 1359/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37632.6484 - val_loss: 39997.8750\n",
      "Epoch 1360/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37630.8125 - val_loss: 39997.7852\n",
      "Epoch 1361/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37628.8555 - val_loss: 39996.1602\n",
      "Epoch 1362/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37626.9844 - val_loss: 39997.6289\n",
      "Epoch 1363/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37625.0039 - val_loss: 39996.2539\n",
      "Epoch 1364/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37623.0391 - val_loss: 39994.0898\n",
      "Epoch 1365/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37621.1562 - val_loss: 39996.4141\n",
      "Epoch 1366/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37619.3086 - val_loss: 39994.2109\n",
      "Epoch 1367/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37617.3203 - val_loss: 39992.7383\n",
      "Epoch 1368/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37615.3516 - val_loss: 39995.2148\n",
      "Epoch 1369/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37613.3555 - val_loss: 39993.1719\n",
      "Epoch 1370/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37611.3555 - val_loss: 39992.8633\n",
      "Epoch 1371/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37609.3438 - val_loss: 39995.0039\n",
      "Epoch 1372/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37607.3164 - val_loss: 39991.4219\n",
      "Epoch 1373/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37605.3086 - val_loss: 39991.2109\n",
      "Epoch 1374/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37603.3086 - val_loss: 39992.4531\n",
      "Epoch 1375/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37601.1836 - val_loss: 39988.9102\n",
      "Epoch 1376/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37599.0469 - val_loss: 39988.5352\n",
      "Epoch 1377/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37597.0039 - val_loss: 39988.2383\n",
      "Epoch 1378/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37594.9570 - val_loss: 39984.2148\n",
      "Epoch 1379/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37592.8125 - val_loss: 39984.7070\n",
      "Epoch 1380/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37590.6484 - val_loss: 39983.8750\n",
      "Epoch 1381/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37588.5039 - val_loss: 39982.0000\n",
      "Epoch 1382/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37586.3516 - val_loss: 39983.3398\n",
      "Epoch 1383/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37584.0469 - val_loss: 39982.7031\n",
      "Epoch 1384/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37581.9531 - val_loss: 39980.1133\n",
      "Epoch 1385/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37579.5508 - val_loss: 39981.4883\n",
      "Epoch 1386/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37577.3516 - val_loss: 39980.2461\n",
      "Epoch 1387/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37574.9844 - val_loss: 39977.6250\n",
      "Epoch 1388/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37572.5039 - val_loss: 39978.8711\n",
      "Epoch 1389/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37570.1562 - val_loss: 39977.2617\n",
      "Epoch 1390/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37567.8438 - val_loss: 39975.9102\n",
      "Epoch 1391/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37565.4961 - val_loss: 39977.0430\n",
      "Epoch 1392/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37563.1875 - val_loss: 39976.3633\n",
      "Epoch 1393/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37561.0430 - val_loss: 39974.9531\n",
      "Epoch 1394/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37558.9609 - val_loss: 39976.2539\n",
      "Epoch 1395/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37556.6797 - val_loss: 39975.0820\n",
      "Epoch 1396/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37554.4609 - val_loss: 39974.2031\n",
      "Epoch 1397/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37552.0430 - val_loss: 39974.8398\n",
      "Epoch 1398/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37549.8086 - val_loss: 39973.1289\n",
      "Epoch 1399/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37547.4492 - val_loss: 39973.2656\n",
      "Epoch 1400/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37545.0156 - val_loss: 39974.2148\n",
      "Epoch 1401/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37542.6914 - val_loss: 39972.7461\n",
      "Epoch 1402/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37540.4844 - val_loss: 39971.2539\n",
      "Epoch 1403/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37538.1797 - val_loss: 39971.2539\n",
      "Epoch 1404/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37536.0117 - val_loss: 39970.3320\n",
      "Epoch 1405/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37533.9844 - val_loss: 39968.9570\n",
      "Epoch 1406/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 21us/step - loss: 37531.8438 - val_loss: 39967.6289\n",
      "Epoch 1407/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37529.6562 - val_loss: 39967.1211\n",
      "Epoch 1408/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37527.4961 - val_loss: 39966.9180\n",
      "Epoch 1409/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37525.3203 - val_loss: 39964.5430\n",
      "Epoch 1410/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37523.0430 - val_loss: 39964.8789\n",
      "Epoch 1411/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37520.8516 - val_loss: 39963.2070\n",
      "Epoch 1412/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37518.6484 - val_loss: 39962.1641\n",
      "Epoch 1413/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37516.4609 - val_loss: 39961.4219\n",
      "Epoch 1414/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37514.1875 - val_loss: 39959.6719\n",
      "Epoch 1415/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37512.0156 - val_loss: 39960.0898\n",
      "Epoch 1416/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37509.9531 - val_loss: 39958.7500\n",
      "Epoch 1417/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37507.6836 - val_loss: 39956.7539\n",
      "Epoch 1418/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37505.5039 - val_loss: 39957.4102\n",
      "Epoch 1419/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37503.3516 - val_loss: 39954.8594\n",
      "Epoch 1420/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37501.1484 - val_loss: 39952.5352\n",
      "Epoch 1421/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37498.9961 - val_loss: 39952.6211\n",
      "Epoch 1422/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37496.8164 - val_loss: 39951.3867\n",
      "Epoch 1423/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37494.5391 - val_loss: 39949.0117\n",
      "Epoch 1424/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37492.4531 - val_loss: 39948.8789\n",
      "Epoch 1425/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37490.1875 - val_loss: 39947.3711\n",
      "Epoch 1426/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37488.0117 - val_loss: 39947.5781\n",
      "Epoch 1427/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37485.8555 - val_loss: 39945.4219\n",
      "Epoch 1428/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37483.6562 - val_loss: 39944.6406\n",
      "Epoch 1429/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37481.4961 - val_loss: 39943.6367\n",
      "Epoch 1430/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37479.3086 - val_loss: 39942.3320\n",
      "Epoch 1431/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37477.0391 - val_loss: 39943.6094\n",
      "Epoch 1432/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37474.8516 - val_loss: 39942.4570\n",
      "Epoch 1433/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37472.5508 - val_loss: 39938.8711\n",
      "Epoch 1434/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37470.4609 - val_loss: 39941.4570\n",
      "Epoch 1435/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37468.1562 - val_loss: 39940.7969\n",
      "Epoch 1436/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37465.9609 - val_loss: 39937.8867\n",
      "Epoch 1437/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37463.6797 - val_loss: 39939.0039\n",
      "Epoch 1438/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37461.4844 - val_loss: 39937.8789\n",
      "Epoch 1439/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37459.1797 - val_loss: 39935.7031\n",
      "Epoch 1440/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37456.9609 - val_loss: 39935.6680\n",
      "Epoch 1441/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37454.6484 - val_loss: 39934.4961\n",
      "Epoch 1442/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37452.4492 - val_loss: 39933.6289\n",
      "Epoch 1443/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37450.0430 - val_loss: 39933.0898\n",
      "Epoch 1444/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37447.8164 - val_loss: 39931.0430\n",
      "Epoch 1445/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37445.5039 - val_loss: 39929.5898\n",
      "Epoch 1446/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37443.1875 - val_loss: 39928.9648\n",
      "Epoch 1447/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37440.9609 - val_loss: 39928.5781\n",
      "Epoch 1448/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37438.6445 - val_loss: 39927.4219\n",
      "Epoch 1449/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37436.3555 - val_loss: 39924.6133\n",
      "Epoch 1450/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37434.0430 - val_loss: 39924.5156\n",
      "Epoch 1451/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37431.8164 - val_loss: 39923.5898\n",
      "Epoch 1452/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37429.5000 - val_loss: 39922.2930\n",
      "Epoch 1453/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37427.1914 - val_loss: 39921.9961\n",
      "Epoch 1454/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37424.9883 - val_loss: 39920.0391\n",
      "Epoch 1455/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37422.6602 - val_loss: 39919.1367\n",
      "Epoch 1456/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37420.4492 - val_loss: 39918.6133\n",
      "Epoch 1457/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37418.0430 - val_loss: 39919.2539\n",
      "Epoch 1458/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37415.8164 - val_loss: 39916.2617\n",
      "Epoch 1459/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37413.4883 - val_loss: 39917.3789\n",
      "Epoch 1460/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37411.0430 - val_loss: 39915.6680\n",
      "Epoch 1461/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37408.6562 - val_loss: 39912.7891\n",
      "Epoch 1462/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37406.1875 - val_loss: 39915.4531\n",
      "Epoch 1463/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37403.8086 - val_loss: 39912.4219\n",
      "Epoch 1464/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37401.1914 - val_loss: 39911.2383\n",
      "Epoch 1465/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37398.6562 - val_loss: 39912.2930\n",
      "Epoch 1466/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37396.0156 - val_loss: 39909.7617\n",
      "Epoch 1467/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37393.4844 - val_loss: 39909.2930\n",
      "Epoch 1468/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37390.9531 - val_loss: 39909.8594\n",
      "Epoch 1469/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37388.4492 - val_loss: 39907.8711\n",
      "Epoch 1470/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37386.0117 - val_loss: 39908.0820\n",
      "Epoch 1471/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37383.6602 - val_loss: 39906.0430\n",
      "Epoch 1472/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37381.4570 - val_loss: 39905.4219\n",
      "Epoch 1473/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37379.1914 - val_loss: 39904.0898\n",
      "Epoch 1474/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37377.0156 - val_loss: 39904.3789\n",
      "Epoch 1475/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37374.8438 - val_loss: 39902.7344\n",
      "Epoch 1476/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37372.5391 - val_loss: 39903.4141\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1477/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37370.3086 - val_loss: 39900.5859\n",
      "Epoch 1478/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37367.9570 - val_loss: 39900.3633\n",
      "Epoch 1479/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37365.5039 - val_loss: 39899.9570\n",
      "Epoch 1480/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37363.0469 - val_loss: 39898.4961\n",
      "Epoch 1481/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37360.6836 - val_loss: 39899.0117\n",
      "Epoch 1482/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37358.3086 - val_loss: 39897.0781\n",
      "Epoch 1483/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37355.9531 - val_loss: 39895.4883\n",
      "Epoch 1484/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37353.5039 - val_loss: 39895.2617\n",
      "Epoch 1485/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37351.1836 - val_loss: 39893.1406\n",
      "Epoch 1486/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37348.9531 - val_loss: 39892.4883\n",
      "Epoch 1487/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37346.5391 - val_loss: 39893.0156\n",
      "Epoch 1488/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37344.1914 - val_loss: 39891.2070\n",
      "Epoch 1489/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37341.9570 - val_loss: 39890.6406\n",
      "Epoch 1490/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37339.5391 - val_loss: 39891.2617\n",
      "Epoch 1491/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37337.1914 - val_loss: 39887.3711\n",
      "Epoch 1492/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37334.9531 - val_loss: 39888.6289\n",
      "Epoch 1493/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37332.5039 - val_loss: 39889.8711\n",
      "Epoch 1494/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37330.1484 - val_loss: 39885.2539\n",
      "Epoch 1495/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37327.8125 - val_loss: 39888.7383\n",
      "Epoch 1496/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37325.4570 - val_loss: 39883.3750\n",
      "Epoch 1497/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37323.0039 - val_loss: 39884.0391\n",
      "Epoch 1498/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37320.6484 - val_loss: 39884.5820\n",
      "Epoch 1499/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37318.3047 - val_loss: 39881.1289\n",
      "Epoch 1500/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37315.9531 - val_loss: 39883.2852\n",
      "Epoch 1501/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37313.5039 - val_loss: 39880.8633\n",
      "Epoch 1502/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37311.0430 - val_loss: 39880.2344\n",
      "Epoch 1503/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37308.6836 - val_loss: 39880.7148\n",
      "Epoch 1504/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37306.3438 - val_loss: 39877.2383\n",
      "Epoch 1505/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37303.9570 - val_loss: 39880.2070\n",
      "Epoch 1506/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37301.5039 - val_loss: 39875.3867\n",
      "Epoch 1507/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37299.0430 - val_loss: 39877.2461\n",
      "Epoch 1508/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37296.6445 - val_loss: 39875.0117\n",
      "Epoch 1509/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37294.1875 - val_loss: 39873.3633\n",
      "Epoch 1510/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37291.8164 - val_loss: 39874.5430\n",
      "Epoch 1511/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37289.3555 - val_loss: 39870.9961\n",
      "Epoch 1512/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37286.9570 - val_loss: 39873.7070\n",
      "Epoch 1513/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37284.4883 - val_loss: 39869.2617\n",
      "Epoch 1514/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37282.0039 - val_loss: 39871.7461\n",
      "Epoch 1515/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37279.5039 - val_loss: 39869.1406\n",
      "Epoch 1516/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37277.0156 - val_loss: 39867.4844\n",
      "Epoch 1517/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37274.5508 - val_loss: 39868.6133\n",
      "Epoch 1518/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37272.1445 - val_loss: 39864.7539\n",
      "Epoch 1519/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37269.6562 - val_loss: 39867.0117\n",
      "Epoch 1520/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37267.1562 - val_loss: 39863.5430\n",
      "Epoch 1521/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37264.5508 - val_loss: 39861.6289\n",
      "Epoch 1522/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37262.0391 - val_loss: 39864.7383\n",
      "Epoch 1523/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37259.5117 - val_loss: 39859.0820\n",
      "Epoch 1524/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37256.9961 - val_loss: 39861.5391\n",
      "Epoch 1525/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37254.4844 - val_loss: 39859.9883\n",
      "Epoch 1526/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37251.9531 - val_loss: 39857.2852\n",
      "Epoch 1527/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37249.3164 - val_loss: 39861.2461\n",
      "Epoch 1528/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37246.8086 - val_loss: 39855.0430\n",
      "Epoch 1529/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37244.1836 - val_loss: 39858.2539\n",
      "Epoch 1530/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37241.6836 - val_loss: 39853.2539\n",
      "Epoch 1531/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37239.1836 - val_loss: 39855.2109\n",
      "Epoch 1532/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37236.6836 - val_loss: 39852.1406\n",
      "Epoch 1533/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37234.1562 - val_loss: 39851.9883\n",
      "Epoch 1534/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37231.5508 - val_loss: 39849.7617\n",
      "Epoch 1535/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37229.0430 - val_loss: 39849.8867\n",
      "Epoch 1536/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37226.5156 - val_loss: 39847.1289\n",
      "Epoch 1537/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37224.0039 - val_loss: 39848.0000\n",
      "Epoch 1538/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37221.4961 - val_loss: 39844.9961\n",
      "Epoch 1539/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37218.9609 - val_loss: 39845.3320\n",
      "Epoch 1540/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37216.4492 - val_loss: 39843.0391\n",
      "Epoch 1541/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37213.8125 - val_loss: 39843.0117\n",
      "Epoch 1542/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37211.1484 - val_loss: 39839.4570\n",
      "Epoch 1543/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37208.5156 - val_loss: 39842.4180\n",
      "Epoch 1544/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37205.9570 - val_loss: 39837.2383\n",
      "Epoch 1545/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37203.3086 - val_loss: 39839.3633\n",
      "Epoch 1546/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37200.5508 - val_loss: 39835.9961\n",
      "Epoch 1547/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 20us/step - loss: 37197.9883 - val_loss: 39836.2539\n",
      "Epoch 1548/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37195.3398 - val_loss: 39834.8867\n",
      "Epoch 1549/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37192.6562 - val_loss: 39835.3320\n",
      "Epoch 1550/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37190.0117 - val_loss: 39830.8320\n",
      "Epoch 1551/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37187.4492 - val_loss: 39832.3906\n",
      "Epoch 1552/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 37184.6914 - val_loss: 39827.9648\n",
      "Epoch 1553/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37182.0430 - val_loss: 39828.5469\n",
      "Epoch 1554/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37179.4961 - val_loss: 39827.1367\n",
      "Epoch 1555/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37176.8438 - val_loss: 39825.3906\n",
      "Epoch 1556/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37174.1484 - val_loss: 39826.5039\n",
      "Epoch 1557/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37171.4883 - val_loss: 39823.2617\n",
      "Epoch 1558/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37168.8438 - val_loss: 39825.7109\n",
      "Epoch 1559/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37166.1562 - val_loss: 39819.9141\n",
      "Epoch 1560/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37163.5117 - val_loss: 39825.0039\n",
      "Epoch 1561/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37160.9531 - val_loss: 39816.2539\n",
      "Epoch 1562/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37158.1875 - val_loss: 39820.6367\n",
      "Epoch 1563/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37155.5156 - val_loss: 39816.2852\n",
      "Epoch 1564/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37152.9531 - val_loss: 39815.8281\n",
      "Epoch 1565/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37150.1836 - val_loss: 39816.5039\n",
      "Epoch 1566/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37147.4961 - val_loss: 39811.2930\n",
      "Epoch 1567/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37144.8438 - val_loss: 39816.2109\n",
      "Epoch 1568/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37142.0430 - val_loss: 39808.2070\n",
      "Epoch 1569/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37139.3398 - val_loss: 39814.5117\n",
      "Epoch 1570/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37136.5156 - val_loss: 39806.1094\n",
      "Epoch 1571/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37133.8125 - val_loss: 39811.3320\n",
      "Epoch 1572/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37130.9961 - val_loss: 39805.2148\n",
      "Epoch 1573/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37128.1484 - val_loss: 39807.5000\n",
      "Epoch 1574/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37125.3398 - val_loss: 39805.6211\n",
      "Epoch 1575/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37122.4883 - val_loss: 39803.6602\n",
      "Epoch 1576/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37119.5430 - val_loss: 39807.6211\n",
      "Epoch 1577/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37116.8086 - val_loss: 39798.2539\n",
      "Epoch 1578/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37113.9570 - val_loss: 39809.5352\n",
      "Epoch 1579/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37111.0430 - val_loss: 39793.4883\n",
      "Epoch 1580/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37108.1992 - val_loss: 39808.3867\n",
      "Epoch 1581/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37105.4531 - val_loss: 39792.7148\n",
      "Epoch 1582/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37102.4961 - val_loss: 39801.5898\n",
      "Epoch 1583/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37099.5430 - val_loss: 39794.6719\n",
      "Epoch 1584/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37096.6953 - val_loss: 39794.3867\n",
      "Epoch 1585/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37093.8516 - val_loss: 39795.5430\n",
      "Epoch 1586/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37090.9844 - val_loss: 39790.8867\n",
      "Epoch 1587/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37088.0039 - val_loss: 39796.3359\n",
      "Epoch 1588/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37085.0430 - val_loss: 39788.3711\n",
      "Epoch 1589/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37082.1484 - val_loss: 39795.2656\n",
      "Epoch 1590/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37079.1836 - val_loss: 39786.8633\n",
      "Epoch 1591/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37076.1562 - val_loss: 39791.5352\n",
      "Epoch 1592/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37073.1523 - val_loss: 39786.4648\n",
      "Epoch 1593/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37070.0469 - val_loss: 39786.4102\n",
      "Epoch 1594/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37067.0430 - val_loss: 39784.8789\n",
      "Epoch 1595/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37064.0039 - val_loss: 39782.3711\n",
      "Epoch 1596/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37060.9961 - val_loss: 39784.2383\n",
      "Epoch 1597/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37057.9883 - val_loss: 39779.0039\n",
      "Epoch 1598/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37054.9961 - val_loss: 39784.6602\n",
      "Epoch 1599/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37052.0469 - val_loss: 39774.0352\n",
      "Epoch 1600/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37049.3047 - val_loss: 39785.4102\n",
      "Epoch 1601/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37046.4492 - val_loss: 39769.9648\n",
      "Epoch 1602/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37043.5039 - val_loss: 39786.9219\n",
      "Epoch 1603/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37040.6836 - val_loss: 39766.1289\n",
      "Epoch 1604/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37037.8516 - val_loss: 39786.2383\n",
      "Epoch 1605/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37035.0391 - val_loss: 39763.7930\n",
      "Epoch 1606/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37032.1836 - val_loss: 39781.3711\n",
      "Epoch 1607/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37029.3047 - val_loss: 39766.8711\n",
      "Epoch 1608/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37026.3164 - val_loss: 39773.1289\n",
      "Epoch 1609/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37023.3438 - val_loss: 39773.0117\n",
      "Epoch 1610/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37020.4531 - val_loss: 39765.7461\n",
      "Epoch 1611/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37017.4844 - val_loss: 39775.9609\n",
      "Epoch 1612/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37014.4961 - val_loss: 39761.2070\n",
      "Epoch 1613/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37011.5039 - val_loss: 39776.7109\n",
      "Epoch 1614/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37008.4961 - val_loss: 39757.7891\n",
      "Epoch 1615/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37005.4492 - val_loss: 39774.1602\n",
      "Epoch 1616/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37002.3047 - val_loss: 39757.5781\n",
      "Epoch 1617/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36999.0430 - val_loss: 39768.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1618/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36995.9570 - val_loss: 39757.6250\n",
      "Epoch 1619/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36992.6953 - val_loss: 39762.3867\n",
      "Epoch 1620/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36989.5156 - val_loss: 39759.6133\n",
      "Epoch 1621/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36986.3477 - val_loss: 39755.8594\n",
      "Epoch 1622/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36983.1523 - val_loss: 39759.7656\n",
      "Epoch 1623/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36979.9844 - val_loss: 39750.0430\n",
      "Epoch 1624/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36976.6875 - val_loss: 39762.0898\n",
      "Epoch 1625/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36973.5039 - val_loss: 39742.8711\n",
      "Epoch 1626/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36970.3164 - val_loss: 39763.8633\n",
      "Epoch 1627/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36967.0156 - val_loss: 39736.4570\n",
      "Epoch 1628/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36963.8086 - val_loss: 39762.7930\n",
      "Epoch 1629/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36960.5039 - val_loss: 39733.6602\n",
      "Epoch 1630/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36956.9883 - val_loss: 39757.1133\n",
      "Epoch 1631/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36953.3477 - val_loss: 39735.4219\n",
      "Epoch 1632/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36949.8008 - val_loss: 39748.7461\n",
      "Epoch 1633/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36946.1914 - val_loss: 39738.4219\n",
      "Epoch 1634/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36942.6484 - val_loss: 39737.4883\n",
      "Epoch 1635/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36939.1562 - val_loss: 39739.0000\n",
      "Epoch 1636/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36935.8125 - val_loss: 39729.8594\n",
      "Epoch 1637/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36932.4531 - val_loss: 39739.3281\n",
      "Epoch 1638/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36928.9883 - val_loss: 39726.2383\n",
      "Epoch 1639/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36925.5117 - val_loss: 39739.0781\n",
      "Epoch 1640/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36922.0469 - val_loss: 39721.5898\n",
      "Epoch 1641/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36918.5469 - val_loss: 39735.3789\n",
      "Epoch 1642/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36915.1562 - val_loss: 39717.0820\n",
      "Epoch 1643/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36911.9570 - val_loss: 39732.3711\n",
      "Epoch 1644/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36908.8438 - val_loss: 39713.2148\n",
      "Epoch 1645/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36905.8477 - val_loss: 39729.2461\n",
      "Epoch 1646/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36902.8438 - val_loss: 39712.8320\n",
      "Epoch 1647/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36899.9531 - val_loss: 39723.8789\n",
      "Epoch 1648/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36897.0039 - val_loss: 39714.2070\n",
      "Epoch 1649/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36894.0430 - val_loss: 39720.7070\n",
      "Epoch 1650/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36891.1484 - val_loss: 39714.6133\n",
      "Epoch 1651/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36888.1836 - val_loss: 39718.0352\n",
      "Epoch 1652/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36885.1914 - val_loss: 39713.4648\n",
      "Epoch 1653/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36882.1875 - val_loss: 39715.4219\n",
      "Epoch 1654/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36879.1562 - val_loss: 39712.9844\n",
      "Epoch 1655/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36876.0430 - val_loss: 39714.3711\n",
      "Epoch 1656/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36872.9961 - val_loss: 39713.3320\n",
      "Epoch 1657/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36869.9531 - val_loss: 39712.9180\n",
      "Epoch 1658/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36866.6953 - val_loss: 39711.1133\n",
      "Epoch 1659/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36863.5391 - val_loss: 39711.4648\n",
      "Epoch 1660/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36860.4844 - val_loss: 39709.5430\n",
      "Epoch 1661/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36857.3398 - val_loss: 39709.8789\n",
      "Epoch 1662/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36854.1562 - val_loss: 39710.9648\n",
      "Epoch 1663/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36851.1484 - val_loss: 39707.2070\n",
      "Epoch 1664/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36848.0117 - val_loss: 39711.4219\n",
      "Epoch 1665/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36844.9570 - val_loss: 39700.8867\n",
      "Epoch 1666/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36841.9531 - val_loss: 39716.4180\n",
      "Epoch 1667/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36838.9570 - val_loss: 39691.4570\n",
      "Epoch 1668/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36836.0039 - val_loss: 39726.3320\n",
      "Epoch 1669/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36833.4609 - val_loss: 39678.7383\n",
      "Epoch 1670/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36830.8164 - val_loss: 39734.7617\n",
      "Epoch 1671/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36828.1914 - val_loss: 39670.1133\n",
      "Epoch 1672/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36825.3398 - val_loss: 39735.1211\n",
      "Epoch 1673/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36822.1875 - val_loss: 39671.7617\n",
      "Epoch 1674/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36818.5039 - val_loss: 39718.4570\n",
      "Epoch 1675/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36814.5039 - val_loss: 39689.9219\n",
      "Epoch 1676/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36810.6875 - val_loss: 39690.5898\n",
      "Epoch 1677/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36807.5156 - val_loss: 39708.6641\n",
      "Epoch 1678/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36804.6953 - val_loss: 39672.1602\n",
      "Epoch 1679/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36802.0391 - val_loss: 39717.1133\n",
      "Epoch 1680/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36799.3438 - val_loss: 39665.3594\n",
      "Epoch 1681/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36796.1484 - val_loss: 39713.1133\n",
      "Epoch 1682/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36792.8086 - val_loss: 39670.6406\n",
      "Epoch 1683/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36789.0469 - val_loss: 39695.0781\n",
      "Epoch 1684/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36785.4883 - val_loss: 39687.3320\n",
      "Epoch 1685/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36782.1484 - val_loss: 39674.8711\n",
      "Epoch 1686/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36779.1562 - val_loss: 39699.9570\n",
      "Epoch 1687/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36776.3125 - val_loss: 39662.0469\n",
      "Epoch 1688/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 20us/step - loss: 36773.4492 - val_loss: 39700.6680\n",
      "Epoch 1689/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36770.3125 - val_loss: 39660.6719\n",
      "Epoch 1690/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36766.9531 - val_loss: 39690.1211\n",
      "Epoch 1691/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36763.3125 - val_loss: 39670.3711\n",
      "Epoch 1692/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36759.8125 - val_loss: 39675.0781\n",
      "Epoch 1693/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36756.4961 - val_loss: 39677.1211\n",
      "Epoch 1694/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36753.3047 - val_loss: 39660.2617\n",
      "Epoch 1695/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36750.1562 - val_loss: 39682.9883\n",
      "Epoch 1696/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36747.1562 - val_loss: 39647.2461\n",
      "Epoch 1697/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36744.1875 - val_loss: 39687.2070\n",
      "Epoch 1698/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36741.0469 - val_loss: 39642.7656\n",
      "Epoch 1699/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36737.6992 - val_loss: 39681.9570\n",
      "Epoch 1700/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36734.0469 - val_loss: 39650.0430\n",
      "Epoch 1701/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36730.4570 - val_loss: 39664.7930\n",
      "Epoch 1702/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36726.8047 - val_loss: 39661.2930\n",
      "Epoch 1703/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36723.3125 - val_loss: 39648.0430\n",
      "Epoch 1704/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36720.0039 - val_loss: 39667.6094\n",
      "Epoch 1705/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36716.6953 - val_loss: 39637.0469\n",
      "Epoch 1706/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36713.3516 - val_loss: 39669.8359\n",
      "Epoch 1707/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36709.8438 - val_loss: 39632.7461\n",
      "Epoch 1708/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36706.1953 - val_loss: 39668.8320\n",
      "Epoch 1709/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36702.6484 - val_loss: 39632.1133\n",
      "Epoch 1710/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36699.0039 - val_loss: 39662.0117\n",
      "Epoch 1711/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36695.4492 - val_loss: 39636.8633\n",
      "Epoch 1712/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36691.8125 - val_loss: 39649.1133\n",
      "Epoch 1713/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36688.3398 - val_loss: 39645.1367\n",
      "Epoch 1714/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36685.0430 - val_loss: 39638.3711\n",
      "Epoch 1715/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36681.9531 - val_loss: 39651.5039\n",
      "Epoch 1716/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36678.8086 - val_loss: 39628.1641\n",
      "Epoch 1717/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36675.6602 - val_loss: 39656.7539\n",
      "Epoch 1718/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36672.5117 - val_loss: 39619.0117\n",
      "Epoch 1719/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36669.3125 - val_loss: 39659.5859\n",
      "Epoch 1720/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36666.0156 - val_loss: 39613.7109\n",
      "Epoch 1721/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36662.5117 - val_loss: 39655.8789\n",
      "Epoch 1722/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36658.9961 - val_loss: 39615.9219\n",
      "Epoch 1723/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36655.0156 - val_loss: 39640.5000\n",
      "Epoch 1724/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36651.1562 - val_loss: 39625.0352\n",
      "Epoch 1725/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36647.5039 - val_loss: 39623.8789\n",
      "Epoch 1726/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36644.0039 - val_loss: 39633.7461\n",
      "Epoch 1727/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36640.5430 - val_loss: 39611.3867\n",
      "Epoch 1728/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36637.3125 - val_loss: 39641.6211\n",
      "Epoch 1729/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36634.0039 - val_loss: 39603.3633\n",
      "Epoch 1730/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36630.6602 - val_loss: 39647.3711\n",
      "Epoch 1731/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36627.5000 - val_loss: 39596.3789\n",
      "Epoch 1732/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36624.0469 - val_loss: 39646.2070\n",
      "Epoch 1733/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36620.3477 - val_loss: 39596.9102\n",
      "Epoch 1734/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36616.4609 - val_loss: 39634.8789\n",
      "Epoch 1735/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36612.4453 - val_loss: 39604.9141\n",
      "Epoch 1736/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36608.4844 - val_loss: 39620.3633\n",
      "Epoch 1737/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36604.6523 - val_loss: 39615.5430\n",
      "Epoch 1738/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36601.0117 - val_loss: 39607.2461\n",
      "Epoch 1739/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36597.5156 - val_loss: 39624.7539\n",
      "Epoch 1740/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36594.3125 - val_loss: 39592.5352\n",
      "Epoch 1741/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36591.3125 - val_loss: 39636.2656\n",
      "Epoch 1742/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36588.4883 - val_loss: 39577.4883\n",
      "Epoch 1743/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36585.8477 - val_loss: 39645.2656\n",
      "Epoch 1744/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36583.1562 - val_loss: 39570.2617\n",
      "Epoch 1745/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36579.9531 - val_loss: 39639.9961\n",
      "Epoch 1746/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36576.0391 - val_loss: 39577.9570\n",
      "Epoch 1747/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36571.8047 - val_loss: 39619.3633\n",
      "Epoch 1748/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36567.4844 - val_loss: 39597.7383\n",
      "Epoch 1749/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36563.5000 - val_loss: 39592.6680\n",
      "Epoch 1750/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36560.0391 - val_loss: 39617.9570\n",
      "Epoch 1751/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36557.0469 - val_loss: 39573.4883\n",
      "Epoch 1752/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36554.3125 - val_loss: 39628.9648\n",
      "Epoch 1753/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36551.4570 - val_loss: 39565.0898\n",
      "Epoch 1754/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36547.9961 - val_loss: 39625.8711\n",
      "Epoch 1755/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36544.3125 - val_loss: 39569.1211\n",
      "Epoch 1756/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36540.1562 - val_loss: 39607.0781\n",
      "Epoch 1757/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36535.9961 - val_loss: 39584.7070\n",
      "Epoch 1758/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36532.0469 - val_loss: 39584.3711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1759/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36528.5508 - val_loss: 39601.6094\n",
      "Epoch 1760/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36525.4883 - val_loss: 39566.1133\n",
      "Epoch 1761/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36522.5430 - val_loss: 39614.6211\n",
      "Epoch 1762/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36519.5508 - val_loss: 39557.5469\n",
      "Epoch 1763/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36516.3516 - val_loss: 39613.8711\n",
      "Epoch 1764/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36512.6992 - val_loss: 39559.8398\n",
      "Epoch 1765/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36508.6992 - val_loss: 39596.6133\n",
      "Epoch 1766/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36504.5508 - val_loss: 39573.5352\n",
      "Epoch 1767/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36500.6602 - val_loss: 39573.0898\n",
      "Epoch 1768/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 36497.0430 - val_loss: 39590.0898\n",
      "Epoch 1769/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36493.9531 - val_loss: 39555.5117\n",
      "Epoch 1770/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36490.8125 - val_loss: 39600.0352\n",
      "Epoch 1771/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36487.6992 - val_loss: 39546.0859\n",
      "Epoch 1772/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36484.3516 - val_loss: 39597.7539\n",
      "Epoch 1773/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36480.6602 - val_loss: 39549.7852\n",
      "Epoch 1774/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36476.5508 - val_loss: 39582.0781\n",
      "Epoch 1775/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36472.5117 - val_loss: 39563.1680\n",
      "Epoch 1776/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36468.6641 - val_loss: 39562.2109\n",
      "Epoch 1777/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36465.0469 - val_loss: 39576.4570\n",
      "Epoch 1778/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36461.8047 - val_loss: 39545.0039\n",
      "Epoch 1779/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36458.6875 - val_loss: 39587.6367\n",
      "Epoch 1780/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36455.5508 - val_loss: 39533.7930\n",
      "Epoch 1781/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36452.4570 - val_loss: 39590.8789\n",
      "Epoch 1782/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36448.9844 - val_loss: 39531.5859\n",
      "Epoch 1783/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36445.1523 - val_loss: 39581.0117\n",
      "Epoch 1784/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36441.0391 - val_loss: 39539.6680\n",
      "Epoch 1785/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36436.8477 - val_loss: 39560.5898\n",
      "Epoch 1786/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36432.8398 - val_loss: 39553.8633\n",
      "Epoch 1787/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36429.0469 - val_loss: 39541.9961\n",
      "Epoch 1788/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36425.6602 - val_loss: 39565.7148\n",
      "Epoch 1789/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36422.3477 - val_loss: 39526.7891\n",
      "Epoch 1790/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36419.3086 - val_loss: 39575.7539\n",
      "Epoch 1791/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36416.1875 - val_loss: 39515.6602\n",
      "Epoch 1792/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 36412.9961 - val_loss: 39577.7070\n",
      "Epoch 1793/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36409.4570 - val_loss: 39513.3750\n",
      "Epoch 1794/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36405.5156 - val_loss: 39567.0430\n",
      "Epoch 1795/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36401.4492 - val_loss: 39521.7930\n",
      "Epoch 1796/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36397.0156 - val_loss: 39547.1133\n",
      "Epoch 1797/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36392.9531 - val_loss: 39535.9883\n",
      "Epoch 1798/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36389.0469 - val_loss: 39528.8320\n",
      "Epoch 1799/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36385.5000 - val_loss: 39547.6367\n",
      "Epoch 1800/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36382.0508 - val_loss: 39512.2109\n",
      "Epoch 1801/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36378.9570 - val_loss: 39558.0820\n",
      "Epoch 1802/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36375.9531 - val_loss: 39498.1133\n",
      "Epoch 1803/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36372.6875 - val_loss: 39562.7852\n",
      "Epoch 1804/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36369.4492 - val_loss: 39493.6289\n",
      "Epoch 1805/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36365.5117 - val_loss: 39555.7383\n",
      "Epoch 1806/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36361.3477 - val_loss: 39501.6094\n",
      "Epoch 1807/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36356.8477 - val_loss: 39536.1289\n",
      "Epoch 1808/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36352.4844 - val_loss: 39517.3711\n",
      "Epoch 1809/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36348.3477 - val_loss: 39513.5430\n",
      "Epoch 1810/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36344.6523 - val_loss: 39532.9180\n",
      "Epoch 1811/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36341.3086 - val_loss: 39493.0039\n",
      "Epoch 1812/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36338.1953 - val_loss: 39546.9180\n",
      "Epoch 1813/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36335.3398 - val_loss: 39477.0430\n",
      "Epoch 1814/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36332.4492 - val_loss: 39553.5117\n",
      "Epoch 1815/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36329.1523 - val_loss: 39472.6133\n",
      "Epoch 1816/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36325.0430 - val_loss: 39543.3711\n",
      "Epoch 1817/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36320.4883 - val_loss: 39485.2344\n",
      "Epoch 1818/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36315.4844 - val_loss: 39516.1211\n",
      "Epoch 1819/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36310.8125 - val_loss: 39507.9570\n",
      "Epoch 1820/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36306.8125 - val_loss: 39488.5430\n",
      "Epoch 1821/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36303.4609 - val_loss: 39527.6133\n",
      "Epoch 1822/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36300.4492 - val_loss: 39470.2969\n",
      "Epoch 1823/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36297.3086 - val_loss: 39535.2969\n",
      "Epoch 1824/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36293.9609 - val_loss: 39464.0469\n",
      "Epoch 1825/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36290.0039 - val_loss: 39529.3867\n",
      "Epoch 1826/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36285.8398 - val_loss: 39468.8711\n",
      "Epoch 1827/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36281.1953 - val_loss: 39511.0352\n",
      "Epoch 1828/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36276.5508 - val_loss: 39482.2461\n",
      "Epoch 1829/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 20us/step - loss: 36272.1875 - val_loss: 39488.0391\n",
      "Epoch 1830/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36268.1602 - val_loss: 39497.5039\n",
      "Epoch 1831/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36264.4844 - val_loss: 39471.0117\n",
      "Epoch 1832/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36260.9531 - val_loss: 39508.3320\n",
      "Epoch 1833/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36257.3477 - val_loss: 39460.1289\n",
      "Epoch 1834/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36253.8398 - val_loss: 39513.4961\n",
      "Epoch 1835/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36250.1953 - val_loss: 39453.2617\n",
      "Epoch 1836/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36246.3477 - val_loss: 39508.3906\n",
      "Epoch 1837/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36242.0508 - val_loss: 39456.0039\n",
      "Epoch 1838/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36237.6602 - val_loss: 39492.3789\n",
      "Epoch 1839/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36233.0469 - val_loss: 39467.7930\n",
      "Epoch 1840/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36228.6992 - val_loss: 39472.5430\n",
      "Epoch 1841/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36224.6523 - val_loss: 39480.0781\n",
      "Epoch 1842/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36220.8047 - val_loss: 39454.7383\n",
      "Epoch 1843/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36217.0508 - val_loss: 39491.4180\n",
      "Epoch 1844/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36213.6641 - val_loss: 39438.8906\n",
      "Epoch 1845/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36210.1992 - val_loss: 39500.1367\n",
      "Epoch 1846/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36206.9570 - val_loss: 39428.1602\n",
      "Epoch 1847/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36203.4492 - val_loss: 39504.3867\n",
      "Epoch 1848/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36199.8047 - val_loss: 39423.1133\n",
      "Epoch 1849/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36195.6641 - val_loss: 39498.6211\n",
      "Epoch 1850/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36191.1953 - val_loss: 39428.1719\n",
      "Epoch 1851/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36186.3125 - val_loss: 39477.5859\n",
      "Epoch 1852/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36181.1953 - val_loss: 39445.9648\n",
      "Epoch 1853/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36176.4883 - val_loss: 39451.0000\n",
      "Epoch 1854/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36172.1953 - val_loss: 39465.4648\n",
      "Epoch 1855/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36168.4961 - val_loss: 39430.5469\n",
      "Epoch 1856/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36164.9883 - val_loss: 39477.1406\n",
      "Epoch 1857/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36161.4844 - val_loss: 39417.0469\n",
      "Epoch 1858/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36157.9531 - val_loss: 39481.8633\n",
      "Epoch 1859/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36154.1562 - val_loss: 39411.2070\n",
      "Epoch 1860/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36150.0508 - val_loss: 39478.9219\n",
      "Epoch 1861/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36145.6602 - val_loss: 39415.8789\n",
      "Epoch 1862/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36140.9609 - val_loss: 39466.1133\n",
      "Epoch 1863/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36136.0469 - val_loss: 39427.5039\n",
      "Epoch 1864/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36131.3086 - val_loss: 39444.4570\n",
      "Epoch 1865/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36126.6914 - val_loss: 39441.4219\n",
      "Epoch 1866/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36122.5039 - val_loss: 39423.2617\n",
      "Epoch 1867/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36118.5156 - val_loss: 39453.0820\n",
      "Epoch 1868/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36114.6992 - val_loss: 39409.7070\n",
      "Epoch 1869/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36111.0117 - val_loss: 39462.1602\n",
      "Epoch 1870/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36107.4844 - val_loss: 39398.2070\n",
      "Epoch 1871/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36103.8047 - val_loss: 39468.6211\n",
      "Epoch 1872/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36100.1562 - val_loss: 39387.9531\n",
      "Epoch 1873/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36096.3086 - val_loss: 39470.1211\n",
      "Epoch 1874/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36092.3086 - val_loss: 39385.8633\n",
      "Epoch 1875/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36087.6641 - val_loss: 39460.2109\n",
      "Epoch 1876/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36082.4844 - val_loss: 39397.7461\n",
      "Epoch 1877/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36076.9961 - val_loss: 39438.0039\n",
      "Epoch 1878/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36071.6641 - val_loss: 39416.9570\n",
      "Epoch 1879/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36066.8125 - val_loss: 39411.6602\n",
      "Epoch 1880/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36062.4961 - val_loss: 39434.9141\n",
      "Epoch 1881/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36058.5508 - val_loss: 39391.2852\n",
      "Epoch 1882/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36054.9961 - val_loss: 39449.1289\n",
      "Epoch 1883/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36051.5156 - val_loss: 39376.1211\n",
      "Epoch 1884/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36048.1602 - val_loss: 39458.3906\n",
      "Epoch 1885/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36044.5391 - val_loss: 39368.7031\n",
      "Epoch 1886/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36040.4961 - val_loss: 39454.2617\n",
      "Epoch 1887/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36035.9492 - val_loss: 39374.2461\n",
      "Epoch 1888/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36030.6562 - val_loss: 39435.9648\n",
      "Epoch 1889/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36025.3359 - val_loss: 39390.6680\n",
      "Epoch 1890/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36020.0508 - val_loss: 39411.6250\n",
      "Epoch 1891/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36015.3359 - val_loss: 39408.7539\n",
      "Epoch 1892/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36010.9844 - val_loss: 39390.4570\n",
      "Epoch 1893/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36006.8477 - val_loss: 39422.1719\n",
      "Epoch 1894/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36002.9844 - val_loss: 39376.2148\n",
      "Epoch 1895/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35999.1875 - val_loss: 39434.8711\n",
      "Epoch 1896/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35995.6562 - val_loss: 39361.6719\n",
      "Epoch 1897/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35992.3086 - val_loss: 39445.0898\n",
      "Epoch 1898/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35988.8008 - val_loss: 39348.1367\n",
      "Epoch 1899/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35985.4961 - val_loss: 39449.7891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1900/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35981.4961 - val_loss: 39347.1602\n",
      "Epoch 1901/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35976.3008 - val_loss: 39431.8906\n",
      "Epoch 1902/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35970.0469 - val_loss: 39369.2461\n",
      "Epoch 1903/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35963.8477 - val_loss: 39396.7070\n",
      "Epoch 1904/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35958.4961 - val_loss: 39400.9883\n",
      "Epoch 1905/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35954.1992 - val_loss: 39365.5469\n",
      "Epoch 1906/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35950.9531 - val_loss: 39424.6719\n",
      "Epoch 1907/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35947.9531 - val_loss: 39344.6094\n",
      "Epoch 1908/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35944.8477 - val_loss: 39433.6602\n",
      "Epoch 1909/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35941.1875 - val_loss: 39338.2461\n",
      "Epoch 1910/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35936.8125 - val_loss: 39425.1211\n",
      "Epoch 1911/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35931.5391 - val_loss: 39349.0117\n",
      "Epoch 1912/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35925.8398 - val_loss: 39400.2617\n",
      "Epoch 1913/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35920.1562 - val_loss: 39371.3320\n",
      "Epoch 1914/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35915.0430 - val_loss: 39373.1211\n",
      "Epoch 1915/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35910.6523 - val_loss: 39392.1211\n",
      "Epoch 1916/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35906.6914 - val_loss: 39351.2617\n",
      "Epoch 1917/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35903.0156 - val_loss: 39407.7930\n",
      "Epoch 1918/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35899.5156 - val_loss: 39337.1211\n",
      "Epoch 1919/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35895.9570 - val_loss: 39415.7461\n",
      "Epoch 1920/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35891.9961 - val_loss: 39331.1211\n",
      "Epoch 1921/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35887.8047 - val_loss: 39413.2148\n",
      "Epoch 1922/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35882.9961 - val_loss: 39334.6719\n",
      "Epoch 1923/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35877.9492 - val_loss: 39397.4883\n",
      "Epoch 1924/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35872.4570 - val_loss: 39349.1133\n",
      "Epoch 1925/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35867.0039 - val_loss: 39373.3750\n",
      "Epoch 1926/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35862.0039 - val_loss: 39366.9961\n",
      "Epoch 1927/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35857.4844 - val_loss: 39352.1289\n",
      "Epoch 1928/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35853.1875 - val_loss: 39382.4102\n",
      "Epoch 1929/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35849.1992 - val_loss: 39334.3711\n",
      "Epoch 1930/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35845.4883 - val_loss: 39394.1367\n",
      "Epoch 1931/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35841.6562 - val_loss: 39320.2070\n",
      "Epoch 1932/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35838.0469 - val_loss: 39404.2656\n",
      "Epoch 1933/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35834.4844 - val_loss: 39309.6250\n",
      "Epoch 1934/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35830.6523 - val_loss: 39407.3867\n",
      "Epoch 1935/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35826.1992 - val_loss: 39307.6133\n",
      "Epoch 1936/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35821.4609 - val_loss: 39396.1680\n",
      "Epoch 1937/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35815.8008 - val_loss: 39317.2344\n",
      "Epoch 1938/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35809.9883 - val_loss: 39371.3711\n",
      "Epoch 1939/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35804.0156 - val_loss: 39336.0430\n",
      "Epoch 1940/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35798.6680 - val_loss: 39343.2344\n",
      "Epoch 1941/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35793.8477 - val_loss: 39354.6680\n",
      "Epoch 1942/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35789.5039 - val_loss: 39321.1367\n",
      "Epoch 1943/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35785.5039 - val_loss: 39371.7891\n",
      "Epoch 1944/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35781.8438 - val_loss: 39302.2891\n",
      "Epoch 1945/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35778.4453 - val_loss: 39386.3789\n",
      "Epoch 1946/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35774.9609 - val_loss: 39287.5117\n",
      "Epoch 1947/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35771.4609 - val_loss: 39393.3633\n",
      "Epoch 1948/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35767.3047 - val_loss: 39283.0781\n",
      "Epoch 1949/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35762.5508 - val_loss: 39385.2617\n",
      "Epoch 1950/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35756.8125 - val_loss: 39295.1719\n",
      "Epoch 1951/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35750.4453 - val_loss: 39357.7656\n",
      "Epoch 1952/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35743.9492 - val_loss: 39321.0898\n",
      "Epoch 1953/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35738.0117 - val_loss: 39323.6211\n",
      "Epoch 1954/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35733.1562 - val_loss: 39345.2148\n",
      "Epoch 1955/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35729.0039 - val_loss: 39295.7148\n",
      "Epoch 1956/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35725.3477 - val_loss: 39363.8789\n",
      "Epoch 1957/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35721.9570 - val_loss: 39276.6367\n",
      "Epoch 1958/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35718.5430 - val_loss: 39377.9141\n",
      "Epoch 1959/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35714.9492 - val_loss: 39267.3789\n",
      "Epoch 1960/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35710.8125 - val_loss: 39379.4570\n",
      "Epoch 1961/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35705.6992 - val_loss: 39269.5820\n",
      "Epoch 1962/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35700.1562 - val_loss: 39362.8320\n",
      "Epoch 1963/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35693.5391 - val_loss: 39286.4883\n",
      "Epoch 1964/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35686.9570 - val_loss: 39328.2383\n",
      "Epoch 1965/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35680.6562 - val_loss: 39314.2617\n",
      "Epoch 1966/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35675.3477 - val_loss: 39294.0117\n",
      "Epoch 1967/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35670.9961 - val_loss: 39336.0352\n",
      "Epoch 1968/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35667.0391 - val_loss: 39269.4883\n",
      "Epoch 1969/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35663.4570 - val_loss: 39349.5117\n",
      "Epoch 1970/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 20us/step - loss: 35659.6914 - val_loss: 39254.5898\n",
      "Epoch 1971/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35655.8086 - val_loss: 39354.9648\n",
      "Epoch 1972/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35651.1641 - val_loss: 39252.6133\n",
      "Epoch 1973/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35646.0156 - val_loss: 39348.2070\n",
      "Epoch 1974/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35640.1992 - val_loss: 39262.2969\n",
      "Epoch 1975/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35634.0469 - val_loss: 39326.5430\n",
      "Epoch 1976/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35627.6992 - val_loss: 39281.7031\n",
      "Epoch 1977/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35621.8008 - val_loss: 39296.5117\n",
      "Epoch 1978/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35616.4453 - val_loss: 39302.8398\n",
      "Epoch 1979/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35611.6523 - val_loss: 39269.6133\n",
      "Epoch 1980/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35607.3359 - val_loss: 39320.1133\n",
      "Epoch 1981/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35603.3086 - val_loss: 39249.7617\n",
      "Epoch 1982/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35599.4844 - val_loss: 39334.5469\n",
      "Epoch 1983/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35595.8008 - val_loss: 39233.9570\n",
      "Epoch 1984/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35592.0391 - val_loss: 39343.4648\n",
      "Epoch 1985/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35587.8359 - val_loss: 39227.3750\n",
      "Epoch 1986/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35583.1523 - val_loss: 39340.0039\n",
      "Epoch 1987/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35577.4961 - val_loss: 39232.3398\n",
      "Epoch 1988/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35571.4453 - val_loss: 39321.7461\n",
      "Epoch 1989/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35564.6680 - val_loss: 39249.7070\n",
      "Epoch 1990/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35558.0469 - val_loss: 39292.5117\n",
      "Epoch 1991/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35551.9492 - val_loss: 39274.2930\n",
      "Epoch 1992/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35546.3477 - val_loss: 39263.3281\n",
      "Epoch 1993/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35541.5039 - val_loss: 39295.1289\n",
      "Epoch 1994/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35537.1562 - val_loss: 39240.5117\n",
      "Epoch 1995/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35533.0039 - val_loss: 39309.0430\n",
      "Epoch 1996/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35528.9961 - val_loss: 39223.5781\n",
      "Epoch 1997/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35525.0039 - val_loss: 39319.0000\n",
      "Epoch 1998/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35520.9492 - val_loss: 39213.2891\n",
      "Epoch 1999/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35516.4961 - val_loss: 39320.8906\n",
      "Epoch 2000/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35511.4961 - val_loss: 39212.1211\n",
      "Epoch 2001/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35505.9961 - val_loss: 39310.9570\n",
      "Epoch 2002/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35499.8398 - val_loss: 39221.1367\n",
      "Epoch 2003/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35493.4961 - val_loss: 39289.3711\n",
      "Epoch 2004/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35486.9961 - val_loss: 39238.4961\n",
      "Epoch 2005/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35480.7031 - val_loss: 39260.7539\n",
      "Epoch 2006/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35475.0000 - val_loss: 39258.1406\n",
      "Epoch 2007/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35469.8086 - val_loss: 39235.7461\n",
      "Epoch 2008/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35465.0430 - val_loss: 39276.3320\n",
      "Epoch 2009/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35460.7031 - val_loss: 39213.7344\n",
      "Epoch 2010/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35456.8359 - val_loss: 39295.3711\n",
      "Epoch 2011/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35453.0469 - val_loss: 39193.1133\n",
      "Epoch 2012/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35450.0508 - val_loss: 39317.2656\n",
      "Epoch 2013/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35447.3047 - val_loss: 39173.7461\n",
      "Epoch 2014/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35444.9531 - val_loss: 39333.6367\n",
      "Epoch 2015/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35441.5156 - val_loss: 39164.0039\n",
      "Epoch 2016/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35436.9844 - val_loss: 39325.7930\n",
      "Epoch 2017/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35429.7969 - val_loss: 39177.1367\n",
      "Epoch 2018/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35420.5156 - val_loss: 39279.9570\n",
      "Epoch 2019/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35410.5039 - val_loss: 39219.3750\n",
      "Epoch 2020/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35402.2969 - val_loss: 39220.6367\n",
      "Epoch 2021/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35396.8008 - val_loss: 39266.6133\n",
      "Epoch 2022/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35393.4961 - val_loss: 39181.7617\n",
      "Epoch 2023/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35390.6953 - val_loss: 39290.6133\n",
      "Epoch 2024/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35387.1914 - val_loss: 39169.3750\n",
      "Epoch 2025/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35382.3047 - val_loss: 39282.9648\n",
      "Epoch 2026/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35375.6562 - val_loss: 39181.7930\n",
      "Epoch 2027/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35367.9883 - val_loss: 39248.7539\n",
      "Epoch 2028/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35360.4453 - val_loss: 39211.4102\n",
      "Epoch 2029/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35353.7031 - val_loss: 39208.7539\n",
      "Epoch 2030/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35348.3438 - val_loss: 39241.7539\n",
      "Epoch 2031/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35343.9492 - val_loss: 39181.1211\n",
      "Epoch 2032/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35339.6992 - val_loss: 39259.3594\n",
      "Epoch 2033/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35335.4961 - val_loss: 39166.5156\n",
      "Epoch 2034/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35330.9961 - val_loss: 39264.4531\n",
      "Epoch 2035/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35325.9492 - val_loss: 39161.9648\n",
      "Epoch 2036/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35320.4492 - val_loss: 39256.8633\n",
      "Epoch 2037/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35314.1641 - val_loss: 39167.3867\n",
      "Epoch 2038/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35307.5508 - val_loss: 39235.7539\n",
      "Epoch 2039/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35300.8086 - val_loss: 39183.5156\n",
      "Epoch 2040/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35294.1914 - val_loss: 39207.0039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2041/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35288.0156 - val_loss: 39203.2070\n",
      "Epoch 2042/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35282.4883 - val_loss: 39182.4219\n",
      "Epoch 2043/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35277.3008 - val_loss: 39219.3633\n",
      "Epoch 2044/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35272.4492 - val_loss: 39161.8359\n",
      "Epoch 2045/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35267.8086 - val_loss: 39233.3867\n",
      "Epoch 2046/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35263.2969 - val_loss: 39144.3711\n",
      "Epoch 2047/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35259.0117 - val_loss: 39246.8711\n",
      "Epoch 2048/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35254.8086 - val_loss: 39128.7891\n",
      "Epoch 2049/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35250.9844 - val_loss: 39259.7539\n",
      "Epoch 2050/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35246.9492 - val_loss: 39116.6094\n",
      "Epoch 2051/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35242.6992 - val_loss: 39264.3281\n",
      "Epoch 2052/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35237.4961 - val_loss: 39114.0352\n",
      "Epoch 2053/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35231.5430 - val_loss: 39250.0469\n",
      "Epoch 2054/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35223.8359 - val_loss: 39129.6211\n",
      "Epoch 2055/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35215.4570 - val_loss: 39212.5352\n",
      "Epoch 2056/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35207.0039 - val_loss: 39159.0039\n",
      "Epoch 2057/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35199.6680 - val_loss: 39171.7070\n",
      "Epoch 2058/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35193.5430 - val_loss: 39189.7148\n",
      "Epoch 2059/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35188.5156 - val_loss: 39139.8867\n",
      "Epoch 2060/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35184.2969 - val_loss: 39216.8711\n",
      "Epoch 2061/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35180.5430 - val_loss: 39115.7031\n",
      "Epoch 2062/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35177.1914 - val_loss: 39238.7070\n",
      "Epoch 2063/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35173.6602 - val_loss: 39099.3711\n",
      "Epoch 2064/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35170.1953 - val_loss: 39250.5117\n",
      "Epoch 2065/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35165.5430 - val_loss: 39094.2031\n",
      "Epoch 2066/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35160.0156 - val_loss: 39239.9883\n",
      "Epoch 2067/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35152.3398 - val_loss: 39107.8633\n",
      "Epoch 2068/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35143.8359 - val_loss: 39202.3281\n",
      "Epoch 2069/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35134.7031 - val_loss: 39140.8711\n",
      "Epoch 2070/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35126.8086 - val_loss: 39156.7930\n",
      "Epoch 2071/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35120.4844 - val_loss: 39176.7461\n",
      "Epoch 2072/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35115.4961 - val_loss: 39121.8711\n",
      "Epoch 2073/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35111.4570 - val_loss: 39202.7031\n",
      "Epoch 2074/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35107.5547 - val_loss: 39098.2617\n",
      "Epoch 2075/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35103.9492 - val_loss: 39216.0039\n",
      "Epoch 2076/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35099.4570 - val_loss: 39087.2070\n",
      "Epoch 2077/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35094.4453 - val_loss: 39213.7969\n",
      "Epoch 2078/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35088.3398 - val_loss: 39087.7617\n",
      "Epoch 2079/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35081.6953 - val_loss: 39197.7070\n",
      "Epoch 2080/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35074.4570 - val_loss: 39098.9180\n",
      "Epoch 2081/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35067.0039 - val_loss: 39172.0430\n",
      "Epoch 2082/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 35059.6562 - val_loss: 39118.6289\n",
      "Epoch 2083/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35052.7031 - val_loss: 39143.8398\n",
      "Epoch 2084/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35046.3438 - val_loss: 39139.8750\n",
      "Epoch 2085/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35040.5156 - val_loss: 39118.6289\n",
      "Epoch 2086/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35035.1562 - val_loss: 39157.5781\n",
      "Epoch 2087/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35030.1562 - val_loss: 39096.5898\n",
      "Epoch 2088/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35025.4961 - val_loss: 39175.7930\n",
      "Epoch 2089/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35021.1562 - val_loss: 39075.5117\n",
      "Epoch 2090/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35017.3438 - val_loss: 39197.4141\n",
      "Epoch 2091/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35014.0039 - val_loss: 39052.7148\n",
      "Epoch 2092/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35011.8086 - val_loss: 39224.2383\n",
      "Epoch 2093/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35009.5547 - val_loss: 39033.8867\n",
      "Epoch 2094/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35007.0117 - val_loss: 39234.3867\n",
      "Epoch 2095/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 35001.6953 - val_loss: 39033.6719\n",
      "Epoch 2096/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34994.3047 - val_loss: 39207.1719\n",
      "Epoch 2097/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34983.2031 - val_loss: 39063.6094\n",
      "Epoch 2098/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34971.4570 - val_loss: 39144.6133\n",
      "Epoch 2099/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 34960.9844 - val_loss: 39118.6133\n",
      "Epoch 2100/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34953.6953 - val_loss: 39085.0352\n",
      "Epoch 2101/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34949.5039 - val_loss: 39169.6211\n",
      "Epoch 2102/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34947.0430 - val_loss: 39048.5781\n",
      "Epoch 2103/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34944.8359 - val_loss: 39193.7461\n",
      "Epoch 2104/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34940.8438 - val_loss: 39038.7070\n",
      "Epoch 2105/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34935.0430 - val_loss: 39180.6211\n",
      "Epoch 2106/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 34926.9492 - val_loss: 39053.7930\n",
      "Epoch 2107/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34918.0430 - val_loss: 39141.7109\n",
      "Epoch 2108/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34908.9844 - val_loss: 39088.0039\n",
      "Epoch 2109/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34901.0039 - val_loss: 39097.2070\n",
      "Epoch 2110/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34894.6953 - val_loss: 39122.6602\n",
      "Epoch 2111/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 21us/step - loss: 34889.6562 - val_loss: 39063.2031\n",
      "Epoch 2112/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34885.3281 - val_loss: 39146.7070\n",
      "Epoch 2113/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34881.0156 - val_loss: 39043.9531\n",
      "Epoch 2114/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34876.4961 - val_loss: 39154.9531\n",
      "Epoch 2115/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34871.0430 - val_loss: 39037.9570\n",
      "Epoch 2116/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 34865.1562 - val_loss: 39148.7070\n",
      "Epoch 2117/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34858.4570 - val_loss: 39043.0898\n",
      "Epoch 2118/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34851.3398 - val_loss: 39130.9570\n",
      "Epoch 2119/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34843.9492 - val_loss: 39057.0000\n",
      "Epoch 2120/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 34836.5508 - val_loss: 39107.4180\n",
      "Epoch 2121/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34829.4883 - val_loss: 39076.2383\n",
      "Epoch 2122/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34822.7031 - val_loss: 39081.4570\n",
      "Epoch 2123/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34816.5039 - val_loss: 39092.6719\n",
      "Epoch 2124/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 34810.6680 - val_loss: 39060.2500\n",
      "Epoch 2125/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34805.1562 - val_loss: 39110.2461\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 02125: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN_500E_Adam31_LReLU05 = Sequential()\n",
    "NN_500E_Adam31_LReLU05.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam31_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam31_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam31_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam31_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam31_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam31_LReLU05.add(Dense(1))\n",
    "NN_500E_Adam31_LReLU05.summary()\n",
    "newAdam = Adam(learning_rate=0.0001)\n",
    "es = EarlyStopping(monitor='val_loss', patience = 30, mode='min', restore_best_weights=True, verbose=1)\n",
    "NN_500E_Adam31_LReLU05.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = NN_500E_Adam31_LReLU05.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=10000,validation_split=0.2,callbacks=[es])\n",
    "\n",
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/10000\n",
      "1164/1164 [==============================] - 0s 119us/step - loss: 37912.3438 - val_loss: 593062.6250\n",
      "Epoch 2/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 648152.7500 - val_loss: 162182.1719\n",
      "Epoch 3/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 168702.1406 - val_loss: 510281.0938\n",
      "Epoch 4/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 747746.4375 - val_loss: 225169.9688\n",
      "Epoch 5/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 277342.7500 - val_loss: 293966.9688\n",
      "Epoch 6/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 295642.8125 - val_loss: 110410.0625\n",
      "Epoch 7/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 115579.7500 - val_loss: 148515.7031\n",
      "Epoch 8/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 196979.5469 - val_loss: 155714.0469\n",
      "Epoch 9/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 207326.0156 - val_loss: 69829.0078\n",
      "Epoch 10/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 110847.0547 - val_loss: 140203.5625\n",
      "Epoch 11/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 141324.8125 - val_loss: 181995.6250\n",
      "Epoch 12/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 182834.1250 - val_loss: 144306.1875\n",
      "Epoch 13/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 144963.8125 - val_loss: 67152.2422\n",
      "Epoch 14/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 80681.2891 - val_loss: 81810.4297\n",
      "Epoch 15/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 114093.0078 - val_loss: 95081.0234\n",
      "Epoch 16/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 123217.9375 - val_loss: 46027.6641\n",
      "Epoch 17/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 56199.3789 - val_loss: 139860.9844\n",
      "Epoch 18/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 151265.3750 - val_loss: 110823.4766\n",
      "Epoch 19/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 119557.0859 - val_loss: 59314.3477\n",
      "Epoch 20/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 67467.5781 - val_loss: 91022.7266\n",
      "Epoch 21/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 104008.9297 - val_loss: 52625.4727\n",
      "Epoch 22/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 57172.3984 - val_loss: 86573.0234\n",
      "Epoch 23/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 93649.3047 - val_loss: 91533.5234\n",
      "Epoch 24/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 99802.9922 - val_loss: 41574.2539\n",
      "Epoch 25/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40860.9570 - val_loss: 78859.2734\n",
      "Epoch 26/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 86641.9922 - val_loss: 74972.7578\n",
      "Epoch 27/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 82551.2734 - val_loss: 41314.4570\n",
      "Epoch 28/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41204.5039 - val_loss: 72474.0859\n",
      "Epoch 29/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 76295.0781 - val_loss: 66457.6016\n",
      "Epoch 30/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 69576.0000 - val_loss: 41482.6602\n",
      "Epoch 31/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41584.6406 - val_loss: 61027.4023\n",
      "Epoch 32/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 65122.3555 - val_loss: 51232.9961\n",
      "Epoch 33/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 53095.1094 - val_loss: 45518.1562\n",
      "Epoch 34/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 46036.5117 - val_loss: 54460.0938\n",
      "Epoch 35/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 57320.9766 - val_loss: 40716.2969\n",
      "Epoch 36/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40223.1367 - val_loss: 52834.1211\n",
      "Epoch 37/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 52216.1367 - val_loss: 51169.3398\n",
      "Epoch 38/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 49960.5039 - val_loss: 40628.6602\n",
      "Epoch 39/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 40715.5117 - val_loss: 47222.4883\n",
      "Epoch 40/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50428.8672 - val_loss: 40174.0391\n",
      "Epoch 41/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39840.3516 - val_loss: 47723.3789\n",
      "Epoch 42/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 46355.6172 - val_loss: 45869.0391\n",
      "Epoch 43/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44668.4609 - val_loss: 40646.2969\n",
      "Epoch 44/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40300.9648 - val_loss: 44803.7617\n",
      "Epoch 45/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 45783.6289 - val_loss: 39899.9609\n",
      "Epoch 46/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39061.3555 - val_loss: 44842.8633\n",
      "Epoch 47/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 43646.3438 - val_loss: 42883.7617\n",
      "Epoch 48/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41329.9844 - val_loss: 40461.7383\n",
      "Epoch 49/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40252.3281 - val_loss: 41791.3398\n",
      "Epoch 50/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 42477.6406 - val_loss: 40122.7539\n",
      "Epoch 51/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38562.8125 - val_loss: 44081.6211\n",
      "Epoch 52/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 41985.1602 - val_loss: 40622.6602\n",
      "Epoch 53/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38809.6484 - val_loss: 40712.6367\n",
      "Epoch 54/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 40618.3633 - val_loss: 40225.4570\n",
      "Epoch 55/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39682.3555 - val_loss: 41027.4883\n",
      "Epoch 56/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39169.0117 - val_loss: 42039.2461\n",
      "Epoch 57/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 40146.1328 - val_loss: 39699.7383\n",
      "Epoch 58/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38333.6523 - val_loss: 40547.6289\n",
      "Epoch 59/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39973.1445 - val_loss: 39721.5859\n",
      "Epoch 60/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38167.1797 - val_loss: 41647.0352\n",
      "Epoch 61/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 39443.0117 - val_loss: 40453.4570\n",
      "Epoch 62/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38348.4961 - val_loss: 39930.4102\n",
      "Epoch 63/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38865.4609 - val_loss: 39814.5781\n",
      "Epoch 64/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 38518.0039 - val_loss: 40638.3789\n",
      "Epoch 65/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38362.8477 - val_loss: 40890.3320\n",
      "Epoch 66/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38556.5039 - val_loss: 39747.7852\n",
      "Epoch 67/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38015.5117 - val_loss: 39936.4102\n",
      "Epoch 68/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 38492.0156 - val_loss: 39993.0117\n",
      "Epoch 69/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37819.8438 - val_loss: 40786.0469\n",
      "Epoch 70/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38346.1875 - val_loss: 39871.6211\n",
      "Epoch 71/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37703.1562 - val_loss: 39874.8320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 72/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38171.6445 - val_loss: 39883.4570\n",
      "Epoch 73/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37638.8125 - val_loss: 40671.2031\n",
      "Epoch 74/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 38007.4609 - val_loss: 40051.1719\n",
      "Epoch 75/10000\n",
      "1164/1164 [==============================] - 0s 18us/step - loss: 37587.0000 - val_loss: 39833.1211\n",
      "Epoch 76/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37854.0430 - val_loss: 39850.4648\n",
      "Epoch 77/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37529.6445 - val_loss: 40431.3867\n",
      "Epoch 78/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37722.4961 - val_loss: 40066.2461\n",
      "Epoch 79/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37473.3086 - val_loss: 39821.7383\n",
      "Epoch 80/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37611.1484 - val_loss: 39852.2383\n",
      "Epoch 81/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37408.4961 - val_loss: 40329.2656\n",
      "Epoch 82/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37513.1797 - val_loss: 40075.2383\n",
      "Epoch 83/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37340.3555 - val_loss: 39826.6211\n",
      "Epoch 84/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37429.5039 - val_loss: 39893.2461\n",
      "Epoch 85/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37274.0117 - val_loss: 40285.1289\n",
      "Epoch 86/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37353.1875 - val_loss: 40019.2461\n",
      "Epoch 87/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37208.5508 - val_loss: 39829.7930\n",
      "Epoch 88/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37280.8516 - val_loss: 39921.5469\n",
      "Epoch 89/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37149.4961 - val_loss: 40202.7383\n",
      "Epoch 90/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37212.1875 - val_loss: 39947.7070\n",
      "Epoch 91/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37093.9961 - val_loss: 39837.9883\n",
      "Epoch 92/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 37141.6836 - val_loss: 39990.5000\n",
      "Epoch 93/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37042.6602 - val_loss: 40158.4609\n",
      "Epoch 94/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 37070.9961 - val_loss: 39908.9102\n",
      "Epoch 95/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36998.4961 - val_loss: 39865.3867\n",
      "Epoch 96/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37001.6484 - val_loss: 40056.6680\n",
      "Epoch 97/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36956.5430 - val_loss: 40059.1602\n",
      "Epoch 98/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36933.6953 - val_loss: 39871.4219\n",
      "Epoch 99/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36913.6953 - val_loss: 39905.5117\n",
      "Epoch 100/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36869.6953 - val_loss: 40080.7461\n",
      "Epoch 101/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36865.9961 - val_loss: 39967.5430\n",
      "Epoch 102/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36811.9570 - val_loss: 39877.3750\n",
      "Epoch 103/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36811.8086 - val_loss: 39991.9219\n",
      "Epoch 104/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36763.1875 - val_loss: 40060.8867\n",
      "Epoch 105/10000\n",
      "1164/1164 [==============================] - 0s 20us/step - loss: 36753.0156 - val_loss: 39913.6250\n",
      "Epoch 106/10000\n",
      "1164/1164 [==============================] - 0s 19us/step - loss: 36719.4961 - val_loss: 39918.3789\n",
      "Epoch 107/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36692.6523 - val_loss: 40047.8789\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00107: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience = 50, mode='min', restore_best_weights=True, verbose=1)\n",
    "newAdam = Adam(learning_rate=0.01)\n",
    "Final_Model.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = Final_Model.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=10000,validation_split=0.2,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>119825.898438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>230804.328125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>199695.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>192461.093750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>172301.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>90912.945312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>98049.890625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>198322.968750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>112107.187500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>219405.875000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  119825.898438\n",
       "1     1462  230804.328125\n",
       "2     1463  199695.843750\n",
       "3     1464  192461.093750\n",
       "4     1465  172301.296875\n",
       "...    ...            ...\n",
       "1454  2915   90912.945312\n",
       "1455  2916   98049.890625\n",
       "1456  2917  198322.968750\n",
       "1457  2918  112107.187500\n",
       "1458  2919  219405.875000\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(pred(NN_500E_Adam31_LReLU05))\n",
    "y_df = y_df.rename(columns={0:'SalePrice'})\n",
    "out = Id.copy()\n",
    "out = out.join(y_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(r'~/Datas/KaggleHouse/NN_500E_Adam31_LReLU05.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
