{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.losses import mean_squared_error\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1456, 405)"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.read_csv('~/Datas/KaggleHouse/X_train_V2.csv',header=None).to_numpy()\n",
    "y = pd.read_csv('~/Datas/KaggleHouse/Y_train_V2.csv',header=None).to_numpy()\n",
    "X_final = pd.read_csv('~/Datas/KaggleHouse/X_test_V2.csv',header=None).to_numpy()\n",
    "Id = pd.read_csv('~/Datas/KaggleHouse/Id.csv',header=None,names=['Id'])\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "IN_DIM = X.shape[1]\n",
    "TRAIN_LEN = X.shape[0]\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotHistory(history):\n",
    "    import matplotlib.pyplot as plt\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred(model):\n",
    "    return model.predict(X_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "        return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Too much precision lost by using tensorflow\n",
    "### 2.718281828459045 -> 2.71828183\n",
    "def exp_root_mean_squared_error(y_true, y_pred):\n",
    "    return K.sqrt(K.mean(K.square(K.exp(y_pred)-K.exp(y_true))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_500E_Adam = Sequential()\n",
    "NN_500E_Adam.add(Dense(512,input_dim = IN_DIM,activation = 'relu'))\n",
    "NN_500E_Adam.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_Adam.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_Adam.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1019 samples, validate on 437 samples\n",
      "Epoch 1/500\n",
      "1019/1019 [==============================] - 0s 147us/step - loss: 165.5607 - val_loss: 1987.4303\n",
      "Epoch 2/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 2633.1904 - val_loss: 1525.4037\n",
      "Epoch 3/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 2033.4384 - val_loss: 668.5825\n",
      "Epoch 4/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 890.2421 - val_loss: 277.1201\n",
      "Epoch 5/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 391.6042 - val_loss: 451.3574\n",
      "Epoch 6/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 629.1174 - val_loss: 290.9961\n",
      "Epoch 7/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 404.4802 - val_loss: 44.2882\n",
      "Epoch 8/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 53.9037 - val_loss: 99.5665\n",
      "Epoch 9/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 135.4323 - val_loss: 22.1367\n",
      "Epoch 10/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 25.6896 - val_loss: 17.5037\n",
      "Epoch 11/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 28.9855 - val_loss: 73.3332\n",
      "Epoch 12/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 95.6729 - val_loss: 31.8994\n",
      "Epoch 13/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 39.3111 - val_loss: 107.5268\n",
      "Epoch 14/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 149.7910 - val_loss: 122.2324\n",
      "Epoch 15/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 170.1433 - val_loss: 44.7557\n",
      "Epoch 16/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 62.5181 - val_loss: 105.1678\n",
      "Epoch 17/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 144.6179 - val_loss: 151.1167\n",
      "Epoch 18/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 206.3639 - val_loss: 113.6083\n",
      "Epoch 19/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 153.2038 - val_loss: 9.6387\n",
      "Epoch 20/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 9.6272 - val_loss: 138.4801\n",
      "Epoch 21/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 194.8067 - val_loss: 184.8940\n",
      "Epoch 22/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 257.0802 - val_loss: 161.2875\n",
      "Epoch 23/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 223.5215 - val_loss: 81.2349\n",
      "Epoch 24/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 113.0498 - val_loss: 44.6908\n",
      "Epoch 25/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 59.9707 - val_loss: 97.0342\n",
      "Epoch 26/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 131.6384 - val_loss: 88.5601\n",
      "Epoch 27/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 119.5823 - val_loss: 31.2723\n",
      "Epoch 28/500\n",
      "1019/1019 [==============================] - 0s 25us/step - loss: 38.7834 - val_loss: 68.0726\n",
      "Epoch 29/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 99.6728 - val_loss: 104.0562\n",
      "Epoch 30/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 147.8035 - val_loss: 90.5330\n",
      "Epoch 31/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 128.6438 - val_loss: 36.9124\n",
      "Epoch 32/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 53.4418 - val_loss: 47.4022\n",
      "Epoch 33/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 64.9306 - val_loss: 79.4101\n",
      "Epoch 34/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 110.0969 - val_loss: 69.5798\n",
      "Epoch 35/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 97.1344 - val_loss: 25.1129\n",
      "Epoch 36/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 36.2857 - val_loss: 48.5733\n",
      "Epoch 37/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 64.7784 - val_loss: 78.1535\n",
      "Epoch 38/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 105.1826 - val_loss: 70.8000\n",
      "Epoch 39/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 95.2650 - val_loss: 32.8215\n",
      "Epoch 40/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 43.3932 - val_loss: 30.7280\n",
      "Epoch 41/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 43.6196 - val_loss: 56.5071\n",
      "Epoch 42/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 78.7148 - val_loss: 50.7181\n",
      "Epoch 43/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 70.0163 - val_loss: 18.3302\n",
      "Epoch 44/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 24.6609 - val_loss: 36.9121\n",
      "Epoch 45/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 51.8978 - val_loss: 58.9021\n",
      "Epoch 46/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 82.5052 - val_loss: 52.5229\n",
      "Epoch 47/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 73.8281 - val_loss: 21.7106\n",
      "Epoch 48/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 31.3352 - val_loss: 30.4910\n",
      "Epoch 49/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 41.1304 - val_loss: 50.0388\n",
      "Epoch 50/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 68.4447 - val_loss: 43.5021\n",
      "Epoch 51/500\n",
      "1019/1019 [==============================] - 0s 25us/step - loss: 59.1666 - val_loss: 15.6285\n",
      "Epoch 52/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 20.2276 - val_loss: 31.2656\n",
      "Epoch 53/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 43.3345 - val_loss: 48.9982\n",
      "Epoch 54/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 68.1817 - val_loss: 42.3718\n",
      "Epoch 55/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 60.0313 - val_loss: 16.8924\n",
      "Epoch 56/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 25.6968 - val_loss: 24.4028\n",
      "Epoch 57/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 31.1204 - val_loss: 40.5366\n",
      "Epoch 58/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 53.6118 - val_loss: 36.1348\n",
      "Epoch 59/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 47.7965 - val_loss: 14.4009\n",
      "Epoch 60/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 18.3544 - val_loss: 22.3742\n",
      "Epoch 61/500\n",
      "1019/1019 [==============================] - 0s 22us/step - loss: 31.1464 - val_loss: 36.8872\n",
      "Epoch 62/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 50.7094 - val_loss: 32.6457\n",
      "Epoch 63/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 45.0303 - val_loss: 12.6519\n",
      "Epoch 64/500\n",
      "1019/1019 [==============================] - 0s 24us/step - loss: 17.7598 - val_loss: 20.9081\n",
      "Epoch 65/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 27.4945 - val_loss: 34.1580\n",
      "Epoch 66/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 45.4640 - val_loss: 30.8774\n",
      "Epoch 67/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 40.8755 - val_loss: 13.6776\n",
      "Epoch 68/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 17.2352 - val_loss: 15.8237\n",
      "Epoch 69/500\n",
      "1019/1019 [==============================] - 0s 23us/step - loss: 22.7465 - val_loss: 27.5424\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00069: early stopping\n"
     ]
    }
   ],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience = 50, mode='min', restore_best_weights=True, verbose=1)\n",
    "NN_500E_Adam.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "history = NN_500E_Adam.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=500,validation_split=0.3,callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzt3Xl4lOXV+PHvmclkIQsJSVgDYQsgqCCigLjv2kVttUpV3FpaX6y2fbtoN/tq+6tdbOtWLVpc626t2KqIuKPIJrKK7BDWbJA9s53fH88zYZJMQgIJSZjzua5cmbmfZyaHGHNy3+deRFUxxhhjWsvT2QEYY4zpXixxGGOMaRNLHMYYY9rEEocxxpg2scRhjDGmTSxxGGOMaRNLHMa0ExEZLCIqIgmtuPdaEfnwUN/HmM5gicPEJRHZLCJ+Eclp1P6p+0t7cOdEZkzXZ4nDxLNNwNTIExE5BujReeEY0z1Y4jDx7ElgWtTza4Anom8QkZ4i8oSIFInIFhH5hYh43GteEfmTiBSLyEbgSzFe+w8R2Ski20XkNyLibWuQItJfRGaLSKmIrBeRb0ddO1FEFotIuYjsFpE/u+3JIvKUiJSIyF4RWSQifdr6tY2JxRKHiWcLgAwROcr9hX4F8FSje+4DegJDgdNwEs117rVvA18GjgMmAJc2eu1jQBAY7t5zLvCtg4jzWaAQ6O9+jf8nIme61+4B7lHVDGAY8Lzbfo0b90AgG/guUHMQX9uYJixxmHgX6XWcA6wBtkcuRCWT21S1QlU3A3cDV7u3fAP4q6puU9VS4HdRr+0DXAh8X1WrVHUP8Bf3/VpNRAYCU4Cfqmqtqi4DHmF/TykADBeRHFWtVNUFUe3ZwHBVDanqElUtb8vXNqY5ljhMvHsS+CZwLY2GqYAcwAdsiWrbAgxwH/cHtjW6FpHvvnanO1S0F/g70LuN8fUHSlW1opkYbgBGAJ+7w1Ffjvp3zQGeFZEdIvIHEfG18WsbE5MlDhPXVHULTpH8QuBfjS4X4/zlnh/VNoj9vZKdOENB0dcitgF1QI6qZrofGao6po0h7gB6iUh6rBhUdZ2qTsVJSL8HXhSRVFUNqOr/qepo4CScIbVpGNMOLHEY4/zVfqaqVkU3qmoIp2bwWxFJF5F84Ifsr4M8D9wsInkikgXcGvXancCbwN0ikiEiHhEZJiKntSUwVd0GfAT8zi14H+vG+xSAiFwlIrmqGgb2ui8Li8gZInKMO9xWjpMAw2352sY0xxKHiXuqukFVFzdz+XtAFbAR+BB4GpjlXnsYZzjoM2ApTXss04BEYDVQBrwI9DuIEKcCg3F6Hy8Dt6vqW+6184FVIlKJUyi/QlVrgL7u1yvHqd28hzN8ZcwhEzvIyRhjTFtYj8MYY0ybWOIwxhjTJpY4jDHGtIklDmOMMW1yRG7bnJOTo4MHD+7sMIwxpltZsmRJsarmHui+IzJxDB48mMWLm5tdaYwxJhYR2XLgu2yoyhhjTBtZ4jDGGNMmljiMMca0yRFZ44glEAhQWFhIbW1tZ4dy2CQnJ5OXl4fPZ5uiGmPaT9wkjsLCQtLT0xk8eDAi0tnhdDhVpaSkhMLCQoYMGdLZ4RhjjiBxM1RVW1tLdnZ2XCQNABEhOzs7rnpYxpjDI24SBxA3SSMi3v69xpjDI64Sx8GoC4SoqA10dhjGGNNlWOI4gKLKOraV1hzy+5SUlDBu3DjGjRtH3759GTBgQP1zv9/fqve47rrrWLt27SHHYowxhyJuiuMHSxWC4TBhVTyHMPSTnZ3NsmXLAPj1r39NWloaP/rRjxp9LUVV8Xhi5/NHH330oL++Mca0F+txHEDYPegqGOqYUzfXr1/P6NGjufLKKxkzZgw7d+5k+vTpTJgwgTFjxnDHHXfU33vyySezbNkygsEgmZmZ3HrrrYwdO5bJkyezZ8+eDonPGGMai8sex/+9uorVO8pbdW9tIEQorKQkelvscYzun8HtXxlzUPF8/vnnPPHEE0yYMAGAu+66i169ehEMBjnjjDO49NJLGT16dIPX7Nu3j9NOO4277rqLH/7wh8yaNYtbb7011tsbY0y7sh5HK3XkCbvDhg2rTxoAzzzzDOPHj2f8+PGsWbOG1atXN3lNSkoKF1xwAQDHH388mzdv7rgAjTEmSlz2ONrSM9hYVEllXZD+mSnkpCV1SDypqan1j9etW8c999zDwoULyczM5Kqrroq5FiMxMbH+sdfrJRgMdkhsxhjTmPU4DiDS0wiGOrDLEaW8vJz09HQyMjLYuXMnc+bMOSxf1xhjWqvDEoeIDBSRd0RktYisEpFb3PZfi8h2EVnmflwY9ZrbRGS9iKwVkfOi2s9329aLyGEdyI+ki0AHFccbGz9+PKNHj2bUqFFMmzaNKVOmHJava4wxrSXaQYP3ItIP6KeqS0UkHVgCXAx8A6hU1T81un808AxwItAfeAsY4V7+AjgHKAQWAVNVtenAv2vChAna+CCnNWvWcNRRR7X537FudwU1gRDpyT6G5KQe+AVdzMH+u40x8UdElqjqhAPd12E1DlXdCex0H1eIyBpgQAsvuQh4VlXrgE0ish4niQCsV9WNACLyrHtvs4mjPUXy6uHqcRhjTFd3WGocIjIYOA74xG26SUSWi8gsEcly2wYA26JeVui2Ndfe+GtMF5HFIrK4qKio3WJXIus4Dk+NwxhjuroOTxwikga8BHxfVcuBB4FhwDicHsnd7fF1VHWmqk5Q1Qm5uQc8a70N7+t8jqweN8aYeNehiUNEfDhJ45+q+i8AVd2tqiFVDQMPs384ajswMOrleW5bc+2HRRgQnIV/1uswxpiOnVUlwD+ANar656j2flG3XQKsdB/PBq4QkSQRGQIUAAtxiuEFIjJERBKBK9x7DwtVxed1E0fY6hzGGNORCwCnAFcDK0Rkmdv2M2CqiIzDmem6GfgOgKquEpHncYreQWCGqoYAROQmYA7gBWap6qoOjLsBVfD5PPhDYQLW4zDGmA6dVfUhEGtzp9daeM1vgd/GaH+tpdd1JAUSvR6qOLSNDktKSjjrrLMA2LVrF16vl0gtZuHChQ1Wgrdk1qxZXHjhhfTt2/egYzHGmEMRl1uOtFZkm3Of14PAIfU4WrOtemvMmjWL8ePHW+IwxnQaSxwtiKQJj4DX6+mwGsfjjz/OAw88gN/v56STTuL+++8nHA5z3XXXsWzZMlSV6dOn06dPH5YtW8bll19OSkpKm3oqxhjTXuIzcbx+K+xaccDbBGVoXYjEBA9p4bAzu8rnjX1z32PggrvaHMrKlSt5+eWX+eijj0hISGD69Ok8++yzDBs2jOLiYlascOLcu3cvmZmZ3Hfffdx///2MGzeuzV/LGGPaQ3wmjjYSwIMQpv2L42+99RaLFi2q31a9pqaGgQMHct5557F27VpuvvlmvvSlL3Huuee2+9c2xpiDEZ+Jo5U9g2AozMad5QzITKEmEKK8NsjofhntGoqqcv3113PnnXc2ubZ8+XJef/11HnjgAV566SVmzpzZrl/bGGMOhm2r3oLIBpAiQoLHQzAUpr03hTz77LN5/vnnKS4uBpzZV1u3bqWoqAhV5bLLLuOOO+5g6dKlAKSnp1NRUdGuMRhjTFvEZ4+jlSI5QoSoRYD7FwS2h2OOOYbbb7+ds88+m3A4jM/n46GHHsLr9XLDDTegqogIv//97wG47rrr+Na3vmXFcWNMp+mwbdU7U3ttq14bCPHF7goG9eqBiLClpIrhvdPokdh98q1tq26Maa3WbqtuQ1UtiB6qqu9x2OpxY0ycs8TRguihqgSP862yczmMMfEurhJHW4flIinCAyRE1Ti6iyNxGNIY0/niJnEkJydTUlLSpl+m0UNVHndmVXfpcagqJSUlJCcnd3YoxpgjTPep8h6ivLw8CgsLacvpgLWBEMWVfrQsicQED0XltZR6hIq0pA6MtP0kJyeTl5fX2WEYY44wcZM4fD4fQ4YMadNr5qzaxXdmL+G/N5/MUf17cteshZRV+5l9k233YYyJX3EzVHUwIsNSiV7n29Q7PYnd5bWdGZIxxnQ6SxwtiCQOn5s4+mQkU1zpJ9SNCuTGGNPeLHG0wB90E0eC2+PISCIUVkqr/J0ZljHGdCpLHC3wu4v9Iov/eqc7RXEbrjLGxDNLHC0IBBvVODKcqa1FFXWdFpMxxnQ2SxwtqC+OJ+wvjgPsqbAehzEmflniaEHj4nhu/VCV9TiMMfHLEkcLIjWOBI9T40hK8JLVw2c9DmNMXLPE0QJ/MEyi14PI/vM3eqcnW4/DGBPXLHG0IBAKNzm0qXdGEnusOG6MiWOWOFoQCIXr13BE9E5Ppsim4xpj4pgljhYEQuH6qbgRkR5H2FaPG2PilCWOFviDWj+jKqJ3ehLBsFJWbavHjTHxyRJHCwKhcP0ajog+7iJAq3MYY+KVJY4W+IMxiuO27YgxJs51WOIQkYEi8o6IrBaRVSJyi9veS0Tmisg693OW2y4icq+IrBeR5SIyPuq9rnHvXyci13RUzI05s6oafosiiwCLK22oyhgTnzqyxxEE/ldVRwOTgBkiMhq4FZinqgXAPPc5wAVAgfsxHXgQnEQD3A5MBE4Ebo8km47mj5E40pN9AFTWBg5HCMYY0+V0WOJQ1Z2qutR9XAGsAQYAFwGPu7c9DlzsPr4IeEIdC4BMEekHnAfMVdVSVS0D5gLnd1Tc0WLVOFKTvABU+UOHIwRjjOlyDkuNQ0QGA8cBnwB9VHWne2kX0Md9PADYFvWyQretufbGX2O6iCwWkcVtOVe8gYpdcP8JsOJFAAIhbTIdNynBi88rVNYFD+5rGGNMN9fhiUNE0oCXgO+rann0NVVVoF0WRKjqTFWdoKoTcnNzD+5NUrKgeJ3zQeyV4wBpSQlU1lriMMbEpw5NHCLiw0ka/1TVf7nNu90hKNzPe9z27cDAqJfnuW3Ntbe/hCTI6A97twCRWVVNv0WpSQlUWY/DGBOnOnJWlQD/ANao6p+jLs0GIjOjrgFeiWqf5s6umgTsc4e05gDnikiWWxQ/123rGJn5UOYmjhhbjoDb47DEYYyJUwkd+N5TgKuBFSKyzG37GXAX8LyI3ABsAb7hXnsNuBBYD1QD1wGoaqmI3Akscu+7Q1VLOyzqrHzY9D4Qe8sRcHocljiMMfGqwxKHqn4INC0QOM6Kcb8CM5p5r1nArPaLrgWZ+VC+A4J1BIJNi+Pg9Dj22pYjxpg4ZSvHG8vKBxT2Fbq74zZTHLcehzEmTlniaCwz3/lctjnmAkBw1nJY4jDGxCtLHI1luYlj75b6EwAbS0vyUVVnCwCNMfHJEkdj6f3A44OyLTH3qgJIS/JS5Q/ilGWMMSa+WOJozOOFzIGEy7YQVppdx6EK1bbtiDEmDlniiCVzUP1ajsZ7VQGkJTuT0azOYYyJR5Y4YsnMh31bAZrdcgQscRhj4pMljliy8vFUF9OD2pg9jtREJ3HYtiPGmHhkiSMWd0punhQ1W+MAbKNDY0xcssQRS9ZgAAbKnpiJI91qHMaYOGaJIxa3xzFQimLWOCI9jiq/JQ5jTPyxxBFLag7hhBQGShFJsWoc7imANlRljIlHljhiEcGfPrD5oaok99xxWz1ujIlDljiaUZuW5w5VNf0WJfs8eMRmVRlj4pMljmbU9MhzZlV5mtY4RMTO5DDGxC1LHM2oSs0jXWpICe2LeT3dEocxJk5Z4mhGVcoAAFKrYx9vbueOG2PilSWOZlQkO4mjRwuJw3ocxph4ZImjGfuS+wGQXFkY83p6siUOY0x8ssTRjBpPGns1laTKbTGvpybaUJUxJj5Z4miGPxhmm+biq2gmcSQl2CmAxpi4ZImjGYFQmG3am4TyrTGvpyV5qagNHOaojDGm81niaIaTOHLx7NsG4XCT62nJCVT5Q3Z8rDEm7ljiaIbf7XFIqA4qdze5npqUQCis1AWbJhVjjDmSWeJoRiCoFGqu82TvlibXI6cAVthGh8aYOGOJoxmBUJjt9HaelDWfOGxmlTEm3ljiaIY/FGa3100cMXocqXbuuDEmTlniaIY/GEa9yZDWt8UehyUOY0y86bDEISKzRGSPiKyMavu1iGwXkWXux4VR124TkfUislZEzotqP99tWy8it3ZUvI0FQmESvR7Iym+xxmFDVcaYeNORPY7HgPNjtP9FVce5H68BiMho4ApgjPuav4mIV0S8wAPABcBoYKp7b4cLhMIkJnicY2RtqMoYY+oldNQbq+r7IjK4lbdfBDyrqnXAJhFZD5zoXluvqhsBRORZ997V7RxuE4GQOoc4peZCdWmT6/t7HLZ63BgTXzqjxnGTiCx3h7Ky3LYBQPTeHoVuW3PtHc4fCuPzCqRkgb8SQg1XidefO15nq8eNMfHlcCeOB4FhwDhgJ3B3e72xiEwXkcUisrioqOiQ388fDDs9jpRMp6Fmb4PrqYmRoSrrcRhj4sthTRyqultVQ6oaBh5m/3DUdmBg1K15bltz7bHee6aqTlDVCbm5uYcca32NI8XtFNWUNbju8QipiV4rjhtj4s5hTRwi0i/q6SVAZMbVbOAKEUkSkSFAAbAQWAQUiMgQEUnEKaDPPhyxBkKNexxlTe5JTUqg0laOG2PiTIcVx0XkGeB0IEdECoHbgdNFZBygwGbgOwCqukpEnscpegeBGaoact/nJmAO4AVmqeqqjoo5WiCoznTcZnoc4Gx0WOm3xGGMiS8dOatqaozmf7Rw/2+B38Zofw14rR1DaxV/KExGom9/4qjd2+SeNDt33BgTh1o1VCUiw0QkyX18uojcLCKZHRta53IWAAoktzBUlWhDVcaY+NPaGsdLQEhEhgMzcQrWT3dYVF1A/ayq5J6AND9UZT0OY0ycaW3iCKtqEKegfZ+q/hjod4DXdGv1xXGP10kesRJHUgJVVuMwxsSZ1iaOgIhMBa4B/uO2+TompK6hfuU4OHWOmqY1jtQkr60cN8bEndYmjuuAycBvVXWTO2X2yY4Lq/P5I+s4wJmSa9NxjTEGaOWsKlVdDdwM4G4Tkq6qv+/IwDpbfXEc3B5H08SRnpSAPxTGH4xKMsYYc4Rr7ayqd0UkQ0R6AUuBh0Xkzx0bWucKRIrj0GziSLWt1Y0xcai1fyb3VNVy4GvAE6o6ETi748LqfP5QGF9C6xKHzawyxsST1iaOBHe7kG+wvzh+xFLVhsXx5ExnAWA43OC+dEscxpg41NrEcQfOth8bVHWRiAwF1nVcWJ0rEFKAhjUODYO/osF9NlRljIlHrS2OvwC8EPV8I/D1jgqqswVCTs8iMXqoCpzhquSe9fdFEkeFJQ5jTBxpbXE8T0Reds8Q3yMiL4lIXkcH11kiiaNBcRya1DnSk63HYYyJP60dqnoUZzvz/u7Hq27bEcnfJHE0c5iTDVUZY+JQaxNHrqo+qqpB9+Mx4NBPS+qi/EF3qOoAPY409xTAClsEaIyJI61NHCUicpWIeN2Pq4CSjgysM0WK476EqOI4NEkckXPHbdsRY0w8aW3iuB5nKu4unLPCLwWu7aCYOl2TGkczW6sneD0k+zy20aExJq60KnGo6hZV/aqq5qpqb1W9mCN4VlWToSpfMiSkNHuYk63jMMbEk0PZYOmH7RZFF1Pf44jef6qF1eO20aExJp4cSuKQdouii9m/ALBx4rDjY40x5lASh7ZbFF1MZKjK1yRxNNPjsMRhjIkjLa4cF5EKYicIAVI6JKIuYH9xPKpTlZIJpZua3JuWlMDu8trDFZoxxnS6FhOHqqYfrkC6kiYLAKHZw5zSkhLYaD0OY0wcsdOHYoj0OJJaWxy3dRzGmDhiiSOGJus4wEkcwRoINByWSkvyUlkXOJzhGWNMp7LEEUMgGFk5HvXtiSwCbLSWIy3JR20gTDDU8KwOY4w5UlniiKEuZnH8ANuO+G24yhgTHyxxxBBovHIcmt/o0E4BNMbEGUscMTRb44AYPQ7bWt0YE18sccTQ5ARAaPZMjrRk63EYY+JLhyUOEZnlnha4Mqqtl4jMFZF17ucst11E5F4RWS8iy0VkfNRrrnHvXyci13RUvNH87pYjCZ4D1zjSrMdhjIkzHdnjeAw4v1HbrcA8VS0A5rnPAS4ACtyP6cCD4CQa4HZgInAicHsk2XSkQChMoteDSFTiSMoA8TYdqnIPc7KNDo0x8aLDEoeqvg+UNmq+CHjcffw4cHFU+xPqWABkikg/4DxgrqqWqmoZMJemyajd+YPhhjOqAERirh5Pt6EqY0ycOdw1jj6qutN9vAvo4z4eAGyLuq/QbWuuvQkRmS4ii0VkcVFR0SEFGQiFG67hiEjObLKOw4rjxph402nFcVVV2nGHXVWdqaoTVHVCbu6hHYceCIUbzqiKiLHtSGQdh/U4jDHx4nAnjt3uEBTu5z1u+3ZgYNR9eW5bc+0dyh/Uhms4ImIkjqQEL4lej+1XZYyJG4c7ccwGIjOjrgFeiWqf5s6umgTsc4e05gDnikiWWxQ/123rUIFQuOFU3IhmNzr02lCVMSZutLit+qEQkWeA04EcESnEmR11F/C8iNwAbAG+4d7+GnAhsB6oBq4DUNVSEbkTWOTed4eqNi64tztnqCrGAYcpmTFPAbTDnIwx8aTDEoeqTm3m0lkx7lVgRjPvMwuY1Y6hHZAzq6qZHkftPgiHwOOtb06zxGGMiSO2cjwGf0vFcdRJHlHs3HFjTDyxxBFDZAFgEy3sV2WJwxgTLyxxxBAIaezieDNncqQnJ1BuK8eNMXHCEkcMzRfHY/c4+mYks3NfDU6pxhhjjmyWOGJosTgOTWZW9c9MoTYQprTKfxiiM8aYzmWJI4ZmtxxppscxICsFgB17axu/whhjjjiWOGLwN1scj30mx4BMJ3Fs31vd0aEZY0yns8QRQyCosWscXh8kpjXtcdQnDutxGGOOfJY4Ymh2yxGIue1IZg8fKT4v28tqDkN0xhjTuSxxxNDsAkCIeSaHiDAgK4Udey1xGGOOfJY4Ymh2ASDEPJMDnJlV2y1xGGPigCWOGAIhbaHHEXuH3AGZ1uMwxsQHSxyNhMJKKHwwiSOZkio/NX47l8MYc2SzxNFIIBQGwJcQY1YV7E8cjVaJ16/l2Ge9DmPMkc0SRyN+N3E0W+NIyYSQHwINE0T/nu6UXJtZZYw5wlniaCQQdBNHS9NxoYXV45Y4jDFHNkscjQRCzhBUizUOaJI4+mQk4xFsZpUx5ohniaOR+hpHGxOHz+uhb0ayJQ5jzBHPEkcjdcFI4mimON7MmRzgruWwGocx5ghniaORwIGK46m5zueKXU0uDchKsVlVxpgjniWORuoTR3PF8fS+zkaHJeubXOqfmcLOvbWEwnagkzHmyGWJo5ED1jhEIHsYFK9rcmlAZgrBsFJUUdfk2vLCvXyysaRdYzXGmM5giaMRf/AAs6oAsodDSezEAbHP5fjFv1dyy7PL7HhZY0y3Z4mjkf1DVc0UxwGyC2DvNgg0PH8jspaj8bkc5bUBVm7fx67yWjYUVbVvwMYYc5hZ4mjEHzzAUBVATgGgULqxQXP/zNirxxdvLiVS9pi/vrjdYjXGmM5giaORA9Y4wBmqgibDVWlJCfRM8TVZPb5gYymJXg/9eiZb4jDGdHsJnR1AV+M/0Kwq2J84YhTIY53LsWBjCeMGZTI0J5X/rthJMBQmoaXEZIwxXZj99moksuVIs+s4AJLSIL1/zCm5AxotAozUNyYNzWbK8BwqaoOs2L6v3eM2xpjDxRJHI60aqoJmp+TmNTpCNlLfmDS0FycNywbgow02LdcY0311SuIQkc0iskJElonIYretl4jMFZF17ucst11E5F4RWS8iy0VkfEfGtj9xtDCrCpwCecm6Judy9M9MpqIuyL6aALC/vjF+UBbZaUkc1S+DD9dZncMY0311Zo/jDFUdp6oT3Oe3AvNUtQCY5z4HuAAocD+mAw92ZFD1s6paqnGAMyW3dh9UN+w9DMjsAezfXj1S30j2eQE4eXg2S7aU2UmBxphuqysNVV0EPO4+fhy4OKr9CXUsADJFpF9HBXHAg5wicgqcz42Gq/pnJgPOlNzo+kbElOE5+ENhFm0ubb+gjTHmMOqsxKHAmyKyRESmu219VHWn+3gX0Md9PADYFvXaQretARGZLiKLRWRxUVHRQQcWaM3KcWh2Sm70EbLR9Y2IE4f0wucV5m+w4SpjTPfUWdNxT1bV7SLSG5grIp9HX1RVFZE27c2hqjOBmQATJkw46H09AqEwXo/g9RygxpE5CLxJTXocOalJJHo9bC+robCspr6+EdEjMYHjBmXZeg5jTLfVKT0OVd3uft4DvAycCOyODEG5n/e4t28HBka9PM9t6xCBUPjAhXEAjxd6DYWSDQ2bPUL/zGT2lJY1qW9EnDw8h1U7yimr8rdn6MYYc1gc9sQhIqkikh55DJwLrARmA9e4t10DvOI+ng1Mc2dXTQL2RQ1ptTt/KHzgYaqI7GExNzuckFrE79d/heyd7zWob0RMGZ6NKnxsu+UaY7qhzuhx9AE+FJHPgIXAf1X1DeAu4BwRWQec7T4HeA3YCKwHHgb+pyOD8wfDBy6MR+QUQOkmCAUbNF8Qfp9EgnzTM69BfSPi2LxM0pIS+NCGq4wx3dBhr3Go6kZgbIz2EuCsGO0KzDgMoQGRoarW9jgKIByAvVuc3geAKsdXvgPA6Z5lhLKbTrv1eT1MGtrL6hzGmG6pK03H7RICIW15n6posabk7lxGZm0hjwQvwCchkj9/OeZLpwzPYUtJNdtKm57dYYwxXZkljkb8rS2OQ+wpuStfIiwJ3Be8hF2pR8FnT8d86YlDnCGsZdv2Hkq4xhhz2FniaCQQbMNQVY9e0CN7/2aH4TCs+jehIWdQkD8Qz7grYOdnsHt1k5cOy03DI7Bud0U7Rm+MMR3PEkcjgVC49UNV4NQ5it3EUbgI9m3DN/ZSXrzxJHqfdBV4EmL2OpJ9XgbnpLLWEocxppuxxNFIm6bjQsPzx1f9y1kUOPJC53lqDhScB8ufbzLzCmBE73TW7a5sh6iNMebwscTRSCCora9xAOQMh8rdUFMGq/4NBedAcsb+6+OmOtc3vtPkpSP6pLG5pIragG14aIzpPixxNOIQB3J2AAAeyklEQVQPhUlM8B74xohsd2bV0iehchcc/bWG1wvOg5Qs+OyZJi8d0TedsMKGIut1GGO6D0scjQRCYRLb1ONwE8dH94KvB4w4v+H1hEQ4+lL4/L/ONuxRRvRJB7DhKmNMt2KJo5E2LQAEyBoC4oWqIidpJKY2vWfcVAjWwqqGazoGZ6fi84oVyI0x3YoljkYCIW1b4khIhKx853HjYaqI/uMhZyQsf6FBc2KChyE5qTYl1xjTrVjiaMTflnUcETkjIDEdhp8T+7oIjDgPtn0CgZoGlwr6pPOFDVUZY7oRSxyNOMXxNtQ4AM76FVz+JPiSm78nf4qzr1Xh4gbNI/uks7W0mmp/0+m68SIcVl5cUsim4qrODsUY0wqddZBTl+UUx9uYT/uMcT5aMmgiILD1YxhySn3ziD5pAKzfU8mxeZltjLb7U1V+8cpKnv5kKz6vMG3yYG4+s4CePXydHZoxphnW42ikTVuOtEVKlpNctsxv0ByZWRWPw1Wqym//u4anP9nK9VOG8PXxecyav4nT/vQOj83fRMA9/90Y07VY4mgkEFJ8bdlypC3yT4JtiyAU2N+UnUpigocv4rBA/te31vHIh5u49qTB/PLLR3HX14/lv987hdH9Mvj1q6u5YuYCQuGDPgXYGNNBLHFEUdW2bznSFoMmQ6AKdi6vb/J6hGG5ad0ycQRCYe5+cy0fbWj7uSJ/f28D98xbxzcm5PGrL49GxKkrje6fwT+/NZE7LxrDki1l/PvTDjsl2BhzkCxxRAmEnL9u27QAsC3yT3I+NxquGtknjS92da/EEQyF+f6zy7jv7fVM+8dCXv60sNWvfWXZdn73+ud8ZWx/fve1Y/F4Gn6/RYQrJ+ZzzICe/HnuF7YlizFdjCWOKJEx9Q7rcaT3hV7DnAJ5lII+6ezYV0tFbaCZF3Yt4bDykxeX898VO/nRuSM4cUgvfvDcZzz03gacAxtbUFNG39duYFnKjfw1459496yKeZvHI/z0/FFs31vDUwu2dMC/whhzsCxxRIkkjjZtq95W+ZNhy0fO2R2ukZGtR/Yc/gJ5VV2QV5Zt59tPLOaCez6grMrf4v2qys//vZJ/fbqdH507gpvOLODR607gy8f2467XP+f/Xl1NuLm6xK4VBB86jePqFlKWdSzepU/AQ1Pg4bOcvb5CDRPnyQU5nFKQw/3vrKe8myRVY+KBJY4owbCSk5ZIalIHzlLOnwK1e6FoTX1T/cyqwzhctXBTKTOeXsrxv5nLLc8uY3nhXtbuKufet9c1+xot2cCrj93FSws3MOOMYdx0prNPV1KCl3uvOI4bTh7CYx9t5gfPL2va8/jsWXjkHPy11Vzh/yWhK56F//0czvsd1FXA7Jvgvz9s8jV/ev4o9lYHmPnexnb997dEVdlYVGmzuoxphq3jiJKTlsTiXzSz+ru9DJrsfN7yUf3aj7ysFFJ83iZTcsNh5Ys9FYzsk15fPG4PO/bWcNU/PiE9KYHLjh/IV8b2Z0J+Fj//9wqeWrCFa08aTH52oz23Vs8m8NJ3+WqoismZ+eSMfLDBZY9H+OWXR5Ps8/DAOxu4bsoQxg3MBFV441b45CEYfAo/rJ1BeVIPhvd21q8w+X9g0o3w1u0w/x4Ychocc2n9+x49oCdfGdufRz7cyLTJ+fTOaGGRJc4v/UP5XlXVBfnRC5/x+spdZCQncPboPpw/pi+njsgl2deGXZONOYJZj+NwyxoM6f2dxOHyeISCPk1nVt0zbx3n//UD/r2sfWcW3f/mCiawhlevH8mdFx/NiUN64fEIPzh7BD6vhz+8sXb/zaEgvPkLeP5qVgf68lifn5GTrMhjF8Lsm51zSKJ897RhpCZ6efJjty6x+UMnaZzwLUq//jxztyrnj+nbMCAROPNXMHASvHoLlGxocPlH544gGFLumdd8bwhgwcYSjv/NW5z+x3e4+ZlPeeSDjXyysaTVxfWtJdV8/cGPmLNqFzeePoxzRvdl3po9TH9yCePvnMsD76xv1fsYc6SzHsfhJuLMrtr8ofPXuPvXcUHvdD5YV1R/24KNJdz39jq8HuFPc77ggqP7HfxfvKqw6X3Y/AE1X7zH7TuXkpQQhNkvwQ1zIbEHAL0zkpl+6lD++tY6rt9SxvG96uDF62HLfF5OuIB7vNfxyrVnIt6b4N3fwccPwNrX4fKn3JXxkJ7s45LxA3h+cSG/+NJRZM2/B1Jz4dzf8NZnxYTCyvlH920aozcBLv0HPHQyvHAN3PBW/RYu+dmpfHPiIP75yVaunJjP6P4ZTV6+taSaG59aQs8UH6P6ZrBkSxmzP9sBODWkl2ecRI/E5n/c568vZsbTS1GFx68/kVMKcgGn7rVgYwmPf7SFP85Zy6BePfjK2P6t+rbvqwnw4bpihuamMqJPOl7PwfeEDrUnZUx7sh5HZ8if7Bz6VLapvmlk3zQmVb1D1acvUVbl5wfPLWNQrx787crxbZ5ZtKm4iuLKuv0Nc38JT3wVPvgzO8vK+ScXUn36HbB7Fbz24wav/fYpQ8lNT2LW7HnozDNg+1Ke6v9z/rfqav409QR6pvicrePP/Q1Mf9f55f7KjAaF7asm5eMPhpn33jxYPxcmfgd8KcxZuYsBmSmMifGLH4CeeXDxg7BrhRNzlO+dWUCv1ESmPryARZtLncbidfDZc1Stn8+PHptLOKw8dt2JPHT18cy/9UwW/+Js/njpsXyxp4L/m7069tfcuZxd955N/yen8Efv33j79E2cklFUP3nB5/VwSkEuf7tyPBPys/jJi8tZ24pa1PLCvXzp3g+Y8fRSLrjnA4799RymzlzAH974nM1t2JOrtMrPjKeXMuIXr3Pm3e/y3SeXcPeba3n1sx1U1cXv/mamc1ni6Az5U5zPkeEqVc4peox7E++nx+xv8/BTT1FcWcd9U8dz3pi+9TOL9tUceGbRsm17ufCeD7jwng+c7dpX/gs+ug+Ov5al31zGmft+Rc3pt9Pj9Fvg1B/DsqecGU2u1KQEfjUlhZ8X/5hAXQ1vn/wkv9g4hlvOGsGEwb0afrF+Y+GCPzhnri96pL55VN8MThzci7QlD6G+VJhwA5V1QT5YV8z5R/dt+S/nkRfApP+BhTNh9Sv1zbnpSfzrxpPITk3kqkc+YfHc5+Dvp8HL00l96kKer5jGEt/1DH75q7BtIeDUrC6bMJAZpw/nucXbeCV6yK92H7z2E8J/P42EkrXsSx3CWb4VZL/zE3hwMvxpOKx7q/72xAQPf7tyPGnJCXznycWx/1uEAmiwjic/3sylD36MKjx67Qn85fKxfP34PCrrgsx8fyOXPvQR6/ccOPnMXb2bc//yHm+u2sWlx+dR0NsZznzgnfV875lP+drfPmLH3poDvk+0Zme8tcG20mo+WFfEttJqW9kfp+SA8+67oQkTJujixYsPfGNnCYfhj8Ng5IXw1ftgzs/gkwd5KXQykxI3khCs5q1TXuTKs08AYNWOfXzp3g+58fRh/PT8UQ3fq2QDLJ4FE7/L5mAvvvbgR/RI9OIPhhkY3MLz3l/g7XcMes2rXPbwEraWVvPuj093hm3CIXjqa7B1gTNk1e9YKNuCPnYhFeX7+J7v/1hcO4AxA3ryzLcnxR5qUYUnL4EdS+HmZdDDSS5zP17M6W+cy65RVzNw6j3M/mwHNz/zKS98dzInNE5AjQX9MOtcp+dx0s1OgnOH00qr/Dzz4J18p+J+9vUcyey8n/DestXMONbDCRllsPYNqC6Bbz5Xv5lkMBTmipkL+HxXBf+5aQqDd74Gc36OVhXxZPBslgybwZ+mnYbPI1C60Uk8Hz/gzHy76AEYe0V9aIs3l3LFzAWcNiKXh6dNcBYvButg0SPo+38iXLOXTeG+lKUN4+ixJ5KSP8FJhm6y3FBUyeV/X4AIPDd9EkNz0/b/u/3VsORRwvPvo7qmmo2BXpQn9eOoo44me9jxcMxl4PFSGwjx4bpifvDcMnokeZl17QmM6d+zxW/p6h3l/PrVVSzZUsbArBQG56QyJCeVoblpfPXY/q3aVHL73hrum7eOF5YU1ieMRK+HQdk9GJ6bxowzhnNMXstxRNuxt4aSSj9Dc1MPaSbjttJqqv0hBvXqQUqiTWA4FCKyRFUnHPA+Sxyd5Jlvwu6VTu/js6fRiTdy7ILT6O/fzOzk20nMPxG5+t/O2D/w/Wc/5fWVu3j3x6fTr2eK8x67V8ETF0PVHsJJGfxWb+Bfgcm89D9T8PorkEfOpEe4iu2Xz6GIXnzricX89pKjuXJi/v44qorhoVMgIQmu+Cc8MxVq97Lo1Me5bHY1PVN8vH7LKfTPTGn+37J7tbMe44RvwYV/BCD02q3owr/zs4FP8ocbvsyMfy7lk02lfPKzs1o31l9V4hTlP3saMgfBhX+CgnPhnd/C+39kRcoJXF52I9UkM21yPndcdLTzuopd8MRFULYZrngahp8FOL/0bvzrs9yZMIuxweWUZB7DNbsvJ3fERB66+niSGp8zX1sOz13p1IbOuROm3Fx/6YmPN/OrV1bxnVPyOSf4HsNX3UumfxfzdSzLQoO5oM8+hlCIlG4EDcOIC+Div9Un1XW7K7hi5gJ8Xg/PfWcS+ekCi2cRnn8Pnqo9LJKjWRfsw+TsagZ7S5B9W50TJPsf5/yh0fcYAD7fVc71jy5iX02AB64cz+kje++PPxSADW9TWRvgyVW1PL68llBKDl85bhC7K2rZVFTFpuIqagIh8rJS+NuV45vuzqwKu5ZTufxVPtpSxRtbPewim4ljx3DC2GPZujfAppIqNhVVsXRrGftqAvzo3JF8+5ShTXYDiLaicB9/f38Dr63YSaTD0q9nMsNy0xjVN51vnzqUPgeYPRcMhZn3+R6e/HgLH67fv+VN7/Qk8rN7MLJvOt85dRgDe/Vo8X2ilVX5CanSq0dii/EfySxxdPXE8dH98ObPncdn/BxO/THf+PsCNpVU8fZZ20l/42Y45X+dsz5w/qo66+73uPi4/vzh0rHOuR5PfR18Pai94C9s+tevOSq4htIhX6bXZffB7O+ha1/nB8l3MqdyGL1SE0lK8DDnB6c2XRm/dQE89iXnl1xiOkz7N9r/OB58bwPjB2UxaWj2gf89//khLHkMbpwPaX3gL0ezquepfGX71cz94Wl85b4Pufi4Afy/S45p2/dp84fOexevdQ7MKv4Cjrua4AV38//mbKC0qo4/Xja24b+pqthJqMVr4RtPwNAz4MM/E/7gz1SGfLyQeQO/3TORKcN78/C0Cc1POgjWwcvfcY78nXwTnPlLKFmP7l7F2++/y4CiDxjl2cZKHcrzmTcQHnIaF48bsH9IL1ALSx6FN3/p7Bpw6aMw0OlFrtlZzg9mvsplnveYlvg2vpoiFnAMd9ddQuKwKfzkvFGMHej+IleFlS8505qrS2HKLXDaT8CXwu7yWq57dBFrd1fwvTOHMzi5mqFbX2T4lufoUbenwT9HxYPkHgVn3AajvowCS7aUcfMzn1Jc6eeXXxnNVRMHIZV7YMXz+Jf8k8SSNcSU2tv5+ZxwHSQkUVbl57Z/reCNVbuYMjybuy8bR9+eyc6svC3z8a94mcCG99hRk8QXtemUenLonz+MlKFT+FQL2LCnkg1FlazZWUGSz8MvvzSayybk7R/WDPphy3yqV/2XfesXsq4yiU3+nlQm9WH48FFI/iTW1mSytbSaLaXVLC/cS1hh+ilDufH0YU17NEE/7NtGXUIab20O8MLSHbz/RRFhhQSPkJOWRO+MJIblpnHDyUM4ekDLPalQWFm0uZTXV+zk7bV78IjQOz2J3PQkeqcnU9AnjUuOG9DiBI3o91q6tYyP1pfgSxCyeiSS1SORXqmJDM1NJSct6YDvcbCOuMQhIucD9wBe4BFVvau5e7tF4iheB7POg9N+6hSPgcKyagDysnrA7O/B0idg6nMw8nwA7vzPah6dv4k3vgrD3/4WgZRcVp/9JH9dXMf8dbt54/glDF99HyQkg78SzvsdRUffwNX/+ITPd1Xw4JXjueCYfrHjWfgwfHC3M0Mq74A/N01VFcO94yHveBh0ErzzG3Zf+TaTZ+1idP8MVm4v54nrT+TUEbltf++gHz6+Hz78K0ye4fzSPNAMo5oyJ7Hu/Awy+sPerXDMZfxep/Hg4gomD81m1rUnHHhoIxyGObc5U4oRwPn/RT0+ynuOoGrCDHpPvJyEhBZ+IWxfAi9cC+U74KzbodcQWPI4uv4tFHg/dCwPBC+i56hT+Z8zhjN+UFbs96kudZLQsqecrWtGnAcJydRJEi+vKEGKv+Bi73ySJMD7oWN4LHQeffvlcePxqQz0lUPFbqduVLwWBk6Ec+6AQZMoq/Jzx9NzSd08l6t6rmBkzaeIhvg0PJxX9FTk6K9z3anDGZSwz/k3lG93FnRu/gAy8pz/HuO+iXoSePHjtTz9+rsUJOzksl6bGFn2LhnhfVRrEh+HR5OeEGRESgU9A3uQgPPzzqCT4OQfQME5bCqp5qcvLWfhplIuHhLmV2OKyNz+LuF1b5EQrKJOfXymQ+mXWEdfKcEXcGtF4nGGfid+FwafzK7yOn7/xue8/Ol2+mQk8dPzRnJyrzICX8wjact79Nz9Cb5Qdf23tpxUQslZlGQdx+LM81kiY9hdGeDTrWVU1AY5a1Rvbjm7wOmVBWph+2Iq1r6Hf/37yL4t7PanUBzqQYWkkZLZm82p43hXj6OwysOeijoqaoP0TPExbXI+15w02Pnlrwr7CtFtC6ndtIC9u7eypcrL+r1QHEyiXFNZEi5guQ5F3XK01yOcNao3UycO4tSC3Ka994rdUFMKvY9q+ee6GUdU4hARL/AFcA5QCCwCpqpqzKky3SJxQIPpuE0EauEf5zhDLvknQY9sanyZPLlwB9fwHzZrH67y30YRzi+Z/3fJMXxz4iDYscxZhd1vnDOsIcK+mgBLtpRyxsjeLRemW4qnNT5+wKnXJKTA4Clw1Ut8+4nFzF29m4zkBBb/4pxD286lrfHVlsOz34SKnc5Q17AzqAuGeH3FLs4d06dVf/3Vf91lT0PJemfRZu/RkFMA3jYcNlWz1/nvsuZV53l6fzjuKtb2v4in18IVJw7iqH7NzDZrbOO7MOfnULYFgjUQdmZXaUIK1UddRtW4GwjljMTrEXLTkhr+Nw8FncTzzu+cmX3Dz3YS0o6lAGwI9+P18Im8mXA6J086iWtPGhx70aUqbHoP5t0J2xdDxgCnrWJH/S1VmsTipIlsyj0b/9AzGT6gNycPz3V+BlSd5L78OWfyRvl26D0Gxk9Di9ZSseYtMqq3AlBEFnOD41iUOJEB48/jkokjGBapDdVVOH8UrHgBljzu/tIcA8d8HWr3Ubp9PcWF68gO7CRbnCSzOdyHD8LHsEqGc3y/RE7srQxMqcVTuRs2vAP+CichHnsZVXknM3/pCjZ8sYo+oZ0cm1JCvn89PgKEVVijg9hEHvlpIfKSa+lJJZ6qIqjb5/x/MOJcGHMJK3Qob85fSPHWzxni3cNJPcsYWLOGnkFnmK1GE9mh2aR7asnw1JIc3j/pIZSSTXne6ezqfSoflPfmvVXbqK2pIi9NOHNIMn1q1tNz32r6Va0lM1TC5uSjGHzrgtb9LDVypCWOycCvVfU89/ltAKr6u1j3d5vEcSBlm+GNn8G+bc7/3NXFEKxlT8+xfDLpbyRm5JCelEDfnskNi6ydJeiHv02C0g1wzasw5FTe/6KIabMW8rXxA/jzN8Yd/pgiP99dYQ2EKnz+H/AmwrCz6utXhywUdBKIx9fy8cXR/FWw4G+w4EHIGgKjvgSjvsyS6lw27KnkwmP7kdaagrUqfPGG80u7Ry/oNRSyh0P2cDR7GOJroTYWEfTDyhedHmXxWkhMg/wplPWdzANb8tjsyeeyEwZx5qjeLW9AGqiBFS/CJ3+H3Suc70fmQDQzn23hHHaljqJu0Gmk9RtOTpozjNRkmDJQA2tfc3pU6+eBOotHFaEyqQ9r/dms8xZQ3udEUoefzOhhgxjdL6Ph+4TDzkamq152enhVDYcMg+JjaziXz2UoO9OPpabveNIGjWXMwByOG5hJgtfjTFypKnaS87o3Yf1bTRbbRoRU2CR5bPIVsKvHSHTA8Uz7xmUH/r7HcKQljkuB81X1W+7zq4GJqnpTrPuPmMQRi78afCld4xdhLNsWOWs3Tr8NRAiHlT/P/YKvjO3PyL7pnR2d6crCYeePjqzBbevNNabqzKxL6QWeQ+jhVu5xJrD0HASZA50JJG0VDjnHKJRucv5dvYZAxgCCKng90vpFneGQM+S5d6vz/39CMvhSqNIkEvuOxJfcPn84xl3iEJHpwHSAQYMGHb9li23FbYwxbdHaxNFdFgBuBwZGPc9z2+qp6kxVnaCqE3JzD6IAa4wxplW6S+JYBBSIyBARSQSuAGZ3ckzGGBOXusUmh6oaFJGbgDk403FnqWrso+OMMcZ0qG6ROABU9TXgtc6Owxhj4l13GaoyxhjTRVjiMMYY0yaWOIwxxrSJJQ5jjDFt0i0WALaViBQBh7ICMAcoPuBdXUd3ixcs5sOlu8Xc3eKFIyvmfFU94EK4IzJxHCoRWdya1ZNdRXeLFyzmw6W7xdzd4oX4jNmGqowxxrSJJQ5jjDFtYokjtpmdHUAbdbd4wWI+XLpbzN0tXojDmK3GYYwxpk2sx2GMMaZNLHEYY4xpE0scUUTkfBFZKyLrReTWzo4nFhGZJSJ7RGRlVFsvEZkrIuvcz1mdGWNjIjJQRN4RkdUiskpEbnHbu2TcIpIsIgtF5DM33v9z24eIyCfuz8dz7hb/XYqIeEXkUxH5j/u8S8csIptFZIWILBORxW5bl/y5iBCRTBF5UUQ+F5E1IjK5q8YsIiPd723ko1xEvn+o8VricImIF3gAuAAYDUwVkdGdG1VMjwHnN2q7FZinqgXAPPd5VxIE/ldVRwOTgBnu97arxl0HnKmqY4FxwPkiMgn4PfAXVR0OlAE3dGKMzbkFWBP1vDvEfIaqjotaV9BVfy4i7gHeUNVRwFic73eXjFlV17rf23HA8UA18DKHGq+q2oczQWAyMCfq+W3AbZ0dVzOxDgZWRj1fC/RzH/cD1nZ2jAeI/xXgnO4QN9ADWApMxFlpmxDr56UrfOCcjDkPOBP4DyDdIObNQE6jti77cwH0BDbhTizqDjFHxXguML894rUex34DgG1Rzwvdtu6gj6rudB/vAvp0ZjAtEZHBwHHAJ3ThuN0hn2XAHmAusAHYq6pB95au+PPxV+AnQNh9nk3Xj1mBN0VkiYhMd9u67M8FMAQoAh51hwQfEZFUunbMEVcAz7iPDyleSxxHGHX+hOiSc6xFJA14Cfi+qpZHX+tqcatqSJ3ufR5wIjCqk0NqkYh8Gdijqks6O5Y2OllVx+MMEc8QkVOjL3a1nwucw+/GAw+q6nFAFY2GebpgzLi1ra8CLzS+djDxWuLYbzswMOp5ntvWHewWkX4A7uc9nRxPEyLiw0ka/1TVf7nNXT5uVd0LvIMzzJMpIpFTM7vaz8cU4Ksishl4Fme46h66dsyo6nb38x6csfcT6do/F4VAoap+4j5/ESeRdOWYwUnMS1V1t/v8kOK1xLHfIqDAnYWSiNOtm93JMbXWbOAa9/E1ODWELkNEBPgHsEZV/xx1qUvGLSK5IpLpPk7BqceswUkgl7q3dZl4AVT1NlXNU9XBOD+7b6vqlXThmEUkVUTSI49xxuBX0kV/LgBUdRewTURGuk1nAavpwjG7prJ/mAoONd7OLth0pQ/gQuALnPHsn3d2PM3E+AywEwjg/PVzA85Y9jxgHfAW0Kuz42wU88k4XeHlwDL348KuGjdwLPCpG+9K4Fdu+1BgIbAep8uf1NmxNhP/6cB/unrMbmyfuR+rIv/PddWfi6i4xwGL3Z+PfwNZXTlmIBUoAXpGtR1SvLbliDHGmDaxoSpjjDFtYonDGGNMm1jiMMYY0yaWOIwxxrSJJQ5jjDFtYonDmIMkIqFGO4+228Z2IjI4egdkY7qShAPfYoxpRo0625IYE1esx2FMO3PPmPiDe87EQhEZ7rYPFpG3RWS5iMwTkUFuex8Redk9/+MzETnJfSuviDzsngnypruK3ZhOZ4nDmIOX0mio6vKoa/tU9RjgfpxdawHuAx5X1WOBfwL3uu33Au+pc/7HeJxV1AAFwAOqOgbYC3y9g/89xrSKrRw35iCJSKWqpsVo34xzENRGd3PHXaqaLSLFOGcgBNz2naqaIyJFQJ6q1kW9x2BgrjoH7SAiPwV8qvqbjv+XGdMy63EY0zG0mcdtURf1OITVJE0XYYnDmI5xedTnj93HH+HsXAtwJfCB+3gecCPUHyDV83AFaczBsL9gjDl4Ke4pgRFvqGpkSm6WiCzH6TVMddu+h3Ny3I9xTpG7zm2/BZgpIjfg9CxuxNkB2ZguyWocxrQzt8YxQVWLOzsWYzqCDVUZY4xpE+txGGOMaRPrcRhjjGkTSxzGGGPaxBKHMcaYNrHEYYwxpk0scRhjjGmT/w/PQ2tNShiOoAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN_500E_Adam.save('/home/louisliu2096/Models/KaggleHouse/NN_500E_Adam_V2_Es.h5')\n",
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_500E_NewAdam = Sequential()\n",
    "NN_500E_NewAdam.add(Dense(512,input_dim = IN_DIM,activation = 'relu'))\n",
    "NN_500E_NewAdam.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_NewAdam.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_NewAdam.add(Dense(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1164/1164 [==============================] - 0s 300us/step - loss: 148543.6407 - val_loss: 92017.5689\n",
      "Epoch 2/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 95069.6305 - val_loss: 63413.2130\n",
      "Epoch 3/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 65197.5775 - val_loss: 50614.4189\n",
      "Epoch 4/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 49147.6446 - val_loss: 45944.5010\n",
      "Epoch 5/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 45520.8952 - val_loss: 43939.2352\n",
      "Epoch 6/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 42889.4300 - val_loss: 42513.9320\n",
      "Epoch 7/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 41225.1446 - val_loss: 41413.3865\n",
      "Epoch 8/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 40358.3156 - val_loss: 40452.0210\n",
      "Epoch 9/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 40023.0346 - val_loss: 40025.8317\n",
      "Epoch 10/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 39262.7343 - val_loss: 40795.7306\n",
      "Epoch 11/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38845.6364 - val_loss: 41281.1888\n",
      "Epoch 12/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 38921.7134 - val_loss: 39276.1199\n",
      "Epoch 13/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39902.0201 - val_loss: 39431.3181\n",
      "Epoch 14/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39023.4593 - val_loss: 39311.4817\n",
      "Epoch 15/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38523.3484 - val_loss: 40710.1223\n",
      "Epoch 16/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38435.5028 - val_loss: 41224.1091\n",
      "Epoch 17/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39382.4933 - val_loss: 39034.8523\n",
      "Epoch 18/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 39083.4810 - val_loss: 40083.7702\n",
      "Epoch 19/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 38276.1268 - val_loss: 39471.7453\n",
      "Epoch 20/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38490.2226 - val_loss: 39158.4069\n",
      "Epoch 21/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37866.7621 - val_loss: 38976.2904\n",
      "Epoch 22/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 39217.8846 - val_loss: 39937.6647\n",
      "Epoch 23/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37562.8151 - val_loss: 38906.1183\n",
      "Epoch 24/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 38547.1749 - val_loss: 44772.6740\n",
      "Epoch 25/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 38359.1716 - val_loss: 38902.8941\n",
      "Epoch 26/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 37437.8380 - val_loss: 39369.8074\n",
      "Epoch 27/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 38780.1477 - val_loss: 39765.8765\n",
      "Epoch 28/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 38136.3702 - val_loss: 38837.2691\n",
      "Epoch 29/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 37836.2283 - val_loss: 39282.1423\n",
      "Epoch 30/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 37926.9797 - val_loss: 39032.9026\n",
      "Epoch 31/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36902.6873 - val_loss: 39039.8823\n",
      "Epoch 32/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37227.0302 - val_loss: 40036.3962\n",
      "Epoch 33/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 37928.4655 - val_loss: 39225.7449\n",
      "Epoch 34/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 37492.5398 - val_loss: 38522.2016\n",
      "Epoch 35/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36842.5385 - val_loss: 38929.6638\n",
      "Epoch 36/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 38402.8919 - val_loss: 39227.8980\n",
      "Epoch 37/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 37161.3837 - val_loss: 39874.2767\n",
      "Epoch 38/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 36695.6533 - val_loss: 41370.5740\n",
      "Epoch 39/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36582.1180 - val_loss: 39463.2577\n",
      "Epoch 40/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 36489.3075 - val_loss: 38714.8542\n",
      "Epoch 41/500\n",
      "1164/1164 [==============================] - 0s 166us/step - loss: 36088.6029 - val_loss: 38751.5040\n",
      "Epoch 42/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 35763.8288 - val_loss: 40399.1791\n",
      "Epoch 43/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 36290.8722 - val_loss: 38053.2480\n",
      "Epoch 44/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 35873.0835 - val_loss: 38673.2439\n",
      "Epoch 45/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36202.9673 - val_loss: 38114.6500\n",
      "Epoch 46/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 35960.6971 - val_loss: 39544.1947\n",
      "Epoch 47/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 35727.2730 - val_loss: 38042.1552\n",
      "Epoch 48/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 36216.6497 - val_loss: 38525.1109\n",
      "Epoch 49/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 36043.3691 - val_loss: 37936.2178\n",
      "Epoch 50/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 36563.9113 - val_loss: 38400.9868\n",
      "Epoch 51/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 35705.7379 - val_loss: 39139.7305\n",
      "Epoch 52/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 35224.4780 - val_loss: 37751.9561\n",
      "Epoch 53/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 36192.1846 - val_loss: 40176.4567\n",
      "Epoch 54/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 35057.2067 - val_loss: 37912.3700\n",
      "Epoch 55/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 35133.7500 - val_loss: 37912.6365\n",
      "Epoch 56/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34982.5675 - val_loss: 38237.6900\n",
      "Epoch 57/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 36790.6518 - val_loss: 39853.9280\n",
      "Epoch 58/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 36181.3465 - val_loss: 38740.7345\n",
      "Epoch 59/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 36185.8245 - val_loss: 38897.7032\n",
      "Epoch 60/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34944.9452 - val_loss: 37749.9111\n",
      "Epoch 61/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 34693.2834 - val_loss: 37907.7400\n",
      "Epoch 62/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 34457.6854 - val_loss: 37919.2896\n",
      "Epoch 63/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 34075.1614 - val_loss: 37768.0381\n",
      "Epoch 64/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34045.9859 - val_loss: 38013.8103\n",
      "Epoch 65/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34399.5015 - val_loss: 38595.2138\n",
      "Epoch 66/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34619.7333 - val_loss: 38058.4076\n",
      "Epoch 67/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 35054.9943 - val_loss: 37847.6618\n",
      "Epoch 68/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 34108.9022 - val_loss: 38620.2705\n",
      "Epoch 69/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 35001.6549 - val_loss: 40650.9433\n",
      "Epoch 70/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 34941.5694 - val_loss: 38526.1198\n",
      "Epoch 71/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 33604.5765 - val_loss: 37610.9923\n",
      "Epoch 72/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 160us/step - loss: 34520.3321 - val_loss: 40756.9881\n",
      "Epoch 73/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33801.9173 - val_loss: 38212.9342\n",
      "Epoch 74/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 34622.5953 - val_loss: 38874.0683\n",
      "Epoch 75/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34560.0943 - val_loss: 38130.1095\n",
      "Epoch 76/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 34121.2530 - val_loss: 37705.3349\n",
      "Epoch 77/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 33761.6682 - val_loss: 37678.3809\n",
      "Epoch 78/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 34488.6496 - val_loss: 38135.3879\n",
      "Epoch 79/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33566.9425 - val_loss: 38152.0231\n",
      "Epoch 80/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 33350.5716 - val_loss: 38915.0794\n",
      "Epoch 81/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 33038.3593 - val_loss: 38205.9915\n",
      "Epoch 82/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 34341.5155 - val_loss: 38641.3517\n",
      "Epoch 83/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 33748.2984 - val_loss: 38016.7200\n",
      "Epoch 84/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 33769.5689 - val_loss: 37541.2317\n",
      "Epoch 85/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 33614.7244 - val_loss: 38043.5220\n",
      "Epoch 86/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 33272.2463 - val_loss: 42478.0633\n",
      "Epoch 87/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 34613.9309 - val_loss: 37512.6988\n",
      "Epoch 88/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32826.2375 - val_loss: 39375.6263\n",
      "Epoch 89/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33698.2653 - val_loss: 37521.9827\n",
      "Epoch 90/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32910.8246 - val_loss: 37170.9016\n",
      "Epoch 91/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 33721.2737 - val_loss: 37575.0855\n",
      "Epoch 92/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 32659.7575 - val_loss: 38470.1016\n",
      "Epoch 93/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32569.3491 - val_loss: 37513.2313\n",
      "Epoch 94/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 32050.2923 - val_loss: 37525.5856\n",
      "Epoch 95/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 32377.1063 - val_loss: 37358.9418\n",
      "Epoch 96/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32529.3891 - val_loss: 37247.0393\n",
      "Epoch 97/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 33198.9818 - val_loss: 37721.2317\n",
      "Epoch 98/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32704.7623 - val_loss: 37749.6313\n",
      "Epoch 99/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 32517.4010 - val_loss: 38106.7773\n",
      "Epoch 100/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32910.8036 - val_loss: 37371.7634\n",
      "Epoch 101/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32515.7649 - val_loss: 39225.5967\n",
      "Epoch 102/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 32207.3082 - val_loss: 37370.9420\n",
      "Epoch 103/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32702.7573 - val_loss: 38246.3472\n",
      "Epoch 104/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 32885.4731 - val_loss: 37240.7846\n",
      "Epoch 105/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 32354.9823 - val_loss: 38681.4775\n",
      "Epoch 106/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32272.6336 - val_loss: 37330.1860\n",
      "Epoch 107/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31557.8731 - val_loss: 37596.4870\n",
      "Epoch 108/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31771.4983 - val_loss: 37970.8753\n",
      "Epoch 109/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31837.1466 - val_loss: 37395.7945\n",
      "Epoch 110/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 31532.9808 - val_loss: 37257.8425\n",
      "Epoch 111/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 32811.4897 - val_loss: 39368.4255\n",
      "Epoch 112/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 32102.6311 - val_loss: 37603.1785\n",
      "Epoch 113/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 31319.1244 - val_loss: 37314.5582\n",
      "Epoch 114/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 31453.7761 - val_loss: 36976.5889\n",
      "Epoch 115/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 31558.1958 - val_loss: 38135.7990\n",
      "Epoch 116/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32455.6183 - val_loss: 36996.3074\n",
      "Epoch 117/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32172.0743 - val_loss: 42368.0679\n",
      "Epoch 118/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 32261.6823 - val_loss: 37889.8272\n",
      "Epoch 119/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 30942.8186 - val_loss: 37583.5994\n",
      "Epoch 120/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 31511.3067 - val_loss: 37898.9994\n",
      "Epoch 121/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31120.5221 - val_loss: 37585.5035\n",
      "Epoch 122/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 30844.8691 - val_loss: 37264.2702\n",
      "Epoch 123/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 31156.6265 - val_loss: 37917.1937\n",
      "Epoch 124/500\n",
      "1164/1164 [==============================] - 0s 171us/step - loss: 30775.7900 - val_loss: 37664.2216\n",
      "Epoch 125/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 31925.2584 - val_loss: 37133.4248\n",
      "Epoch 126/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 32461.5698 - val_loss: 37262.7513\n",
      "Epoch 127/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 29857.2676 - val_loss: 37198.6891\n",
      "Epoch 128/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 30454.8552 - val_loss: 37630.1721\n",
      "Epoch 129/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 31486.7042 - val_loss: 38385.0885\n",
      "Epoch 130/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30630.0513 - val_loss: 37383.9728\n",
      "Epoch 131/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30267.7791 - val_loss: 37583.3065\n",
      "Epoch 132/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 29524.0867 - val_loss: 37170.0038\n",
      "Epoch 133/500\n",
      "1164/1164 [==============================] - ETA: 0s - loss: 29289.508 - 0s 177us/step - loss: 29741.0795 - val_loss: 38355.5923\n",
      "Epoch 134/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 29871.0136 - val_loss: 37729.5106\n",
      "Epoch 135/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 31201.3171 - val_loss: 38585.5762\n",
      "Epoch 136/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 30824.2771 - val_loss: 37027.9282\n",
      "Epoch 137/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 30050.3028 - val_loss: 38903.7132\n",
      "Epoch 138/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 29629.9388 - val_loss: 39083.8024\n",
      "Epoch 139/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 30560.7771 - val_loss: 38496.6070\n",
      "Epoch 140/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30635.0331 - val_loss: 37577.1777\n",
      "Epoch 141/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 29067.5359 - val_loss: 37348.1656\n",
      "Epoch 142/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 29043.3603 - val_loss: 40615.0231\n",
      "Epoch 143/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 173us/step - loss: 30300.9578 - val_loss: 37120.4281\n",
      "Epoch 144/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28803.7107 - val_loss: 39837.7526\n",
      "Epoch 145/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 29633.7259 - val_loss: 37700.4394\n",
      "Epoch 146/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 28818.7375 - val_loss: 37649.8676\n",
      "Epoch 147/500\n",
      "1164/1164 [==============================] - 0s 178us/step - loss: 28790.7979 - val_loss: 37344.8975\n",
      "Epoch 148/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 29145.6958 - val_loss: 37731.7047\n",
      "Epoch 149/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 30112.2863 - val_loss: 37618.3568\n",
      "Epoch 150/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 28796.9386 - val_loss: 38545.3280\n",
      "Epoch 151/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28313.8811 - val_loss: 38097.6434\n",
      "Epoch 152/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28885.6029 - val_loss: 37625.0024\n",
      "Epoch 153/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28253.2433 - val_loss: 37185.0954\n",
      "Epoch 154/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 27724.2884 - val_loss: 38256.5359\n",
      "Epoch 155/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 28850.9137 - val_loss: 38470.9125\n",
      "Epoch 156/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 28216.8561 - val_loss: 37623.4275\n",
      "Epoch 157/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 27330.7649 - val_loss: 37937.8291\n",
      "Epoch 158/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 27638.0963 - val_loss: 37460.5820\n",
      "Epoch 159/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 28751.4052 - val_loss: 38402.1224\n",
      "Epoch 160/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 28888.2668 - val_loss: 37348.5605\n",
      "Epoch 161/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 28056.0472 - val_loss: 37844.0356\n",
      "Epoch 162/500\n",
      "1164/1164 [==============================] - 0s 174us/step - loss: 27831.7652 - val_loss: 38193.5613\n",
      "Epoch 163/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 27959.6307 - val_loss: 39414.2193\n",
      "Epoch 164/500\n",
      "1164/1164 [==============================] - 0s 172us/step - loss: 28421.7049 - val_loss: 37000.8839\n",
      "Epoch 165/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 28009.7382 - val_loss: 37130.0997\n",
      "Epoch 166/500\n",
      "1164/1164 [==============================] - 0s 175us/step - loss: 26813.2289 - val_loss: 37958.7974\n",
      "Epoch 167/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 27250.3008 - val_loss: 37674.2099\n",
      "Epoch 168/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 26603.1641 - val_loss: 37406.6289\n",
      "Epoch 169/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 26598.3796 - val_loss: 38686.6198\n",
      "Epoch 170/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 27769.1186 - val_loss: 37508.6566\n",
      "Epoch 171/500\n",
      "1164/1164 [==============================] - 0s 176us/step - loss: 27427.4461 - val_loss: 37678.8554\n",
      "Epoch 172/500\n",
      "1164/1164 [==============================] - 0s 177us/step - loss: 26865.5808 - val_loss: 38421.6253\n",
      "Epoch 173/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 26033.3942 - val_loss: 38736.1558\n",
      "Epoch 174/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25849.9523 - val_loss: 37709.4021\n",
      "Epoch 175/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 26305.7334 - val_loss: 37959.3617\n",
      "Epoch 176/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 27214.1016 - val_loss: 37284.7237\n",
      "Epoch 177/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25991.2694 - val_loss: 37799.5433\n",
      "Epoch 178/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 26112.0083 - val_loss: 39340.2163\n",
      "Epoch 179/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25494.6472 - val_loss: 39185.8844\n",
      "Epoch 180/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 25974.2881 - val_loss: 38645.5101\n",
      "Epoch 181/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25556.5378 - val_loss: 37559.9154\n",
      "Epoch 182/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 25280.5966 - val_loss: 37396.2794\n",
      "Epoch 183/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 25650.2815 - val_loss: 39637.0727\n",
      "Epoch 184/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 26955.1238 - val_loss: 37523.6091\n",
      "Epoch 185/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 25887.3478 - val_loss: 38265.5188\n",
      "Epoch 186/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25122.6399 - val_loss: 37925.8438\n",
      "Epoch 187/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25835.5196 - val_loss: 38689.2182\n",
      "Epoch 188/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 24850.3151 - val_loss: 38273.2514\n",
      "Epoch 189/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 25316.4020 - val_loss: 37641.4595\n",
      "Epoch 190/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25028.0693 - val_loss: 37845.3955\n",
      "Epoch 191/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25611.9346 - val_loss: 37793.2922\n",
      "Epoch 192/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 24731.9866 - val_loss: 37716.8632\n",
      "Epoch 193/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 25211.2186 - val_loss: 38131.8334\n",
      "Epoch 194/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 24206.1303 - val_loss: 37929.9528\n",
      "Epoch 195/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 25052.9708 - val_loss: 41099.1232\n",
      "Epoch 196/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25461.4176 - val_loss: 39361.3773\n",
      "Epoch 197/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 26418.5227 - val_loss: 37472.1597\n",
      "Epoch 198/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 25267.8392 - val_loss: 39730.6871\n",
      "Epoch 199/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 25600.6047 - val_loss: 37666.6244\n",
      "Epoch 200/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 25616.1147 - val_loss: 40511.6755\n",
      "Epoch 201/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 24358.1464 - val_loss: 37225.3182\n",
      "Epoch 202/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 23208.9980 - val_loss: 37616.1899\n",
      "Epoch 203/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23981.9591 - val_loss: 37815.3722\n",
      "Epoch 204/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23663.9609 - val_loss: 37665.8900\n",
      "Epoch 205/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23900.9642 - val_loss: 38211.8616\n",
      "Epoch 206/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23300.2330 - val_loss: 38359.6709\n",
      "Epoch 207/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 24213.9648 - val_loss: 38123.5437\n",
      "Epoch 208/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 24132.9672 - val_loss: 37427.4294\n",
      "Epoch 209/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23567.7269 - val_loss: 38903.8411\n",
      "Epoch 210/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 24840.0212 - val_loss: 38310.6309\n",
      "Epoch 211/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23766.9105 - val_loss: 39170.4030\n",
      "Epoch 212/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23726.8965 - val_loss: 37463.7416\n",
      "Epoch 213/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 24165.2890 - val_loss: 39359.5987\n",
      "Epoch 214/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 162us/step - loss: 24401.0984 - val_loss: 39681.9774\n",
      "Epoch 215/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 23616.3471 - val_loss: 38754.3473\n",
      "Epoch 216/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23108.1409 - val_loss: 37380.7876\n",
      "Epoch 217/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22562.6091 - val_loss: 37394.2743\n",
      "Epoch 218/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22837.3616 - val_loss: 38803.2634\n",
      "Epoch 219/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 23034.9099 - val_loss: 37851.3889\n",
      "Epoch 220/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 22472.5834 - val_loss: 38274.6259\n",
      "Epoch 221/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 23944.4890 - val_loss: 38943.7165\n",
      "Epoch 222/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23138.5698 - val_loss: 37759.1853\n",
      "Epoch 223/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 22763.8303 - val_loss: 37454.0975\n",
      "Epoch 224/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 24394.0377 - val_loss: 37517.4260\n",
      "Epoch 225/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 25066.4371 - val_loss: 38181.1810\n",
      "Epoch 226/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 23701.7663 - val_loss: 39166.5416\n",
      "Epoch 227/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 21775.2738 - val_loss: 37486.1422\n",
      "Epoch 228/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 22067.9933 - val_loss: 37151.0756\n",
      "Epoch 229/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 22845.1926 - val_loss: 40254.9620\n",
      "Epoch 230/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 22841.6374 - val_loss: 38087.3340\n",
      "Epoch 231/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 23795.0349 - val_loss: 38458.2839\n",
      "Epoch 232/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21987.0046 - val_loss: 39307.5137\n",
      "Epoch 233/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21667.5782 - val_loss: 37730.6135\n",
      "Epoch 234/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21725.4113 - val_loss: 39489.9604\n",
      "Epoch 235/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 22551.8857 - val_loss: 37456.6099\n",
      "Epoch 236/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21766.8432 - val_loss: 38089.5209\n",
      "Epoch 237/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 21450.9326 - val_loss: 39590.1899\n",
      "Epoch 238/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22647.9046 - val_loss: 40827.3250\n",
      "Epoch 239/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 23612.6508 - val_loss: 37786.8867\n",
      "Epoch 240/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 22085.3160 - val_loss: 37980.4027\n",
      "Epoch 241/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 21139.3389 - val_loss: 38093.7318\n",
      "Epoch 242/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21476.7382 - val_loss: 37647.7593\n",
      "Epoch 243/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 20727.4548 - val_loss: 38788.3942\n",
      "Epoch 244/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 20733.8662 - val_loss: 39366.0122\n",
      "Epoch 245/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 21793.5414 - val_loss: 38913.1234\n",
      "Epoch 246/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21168.6101 - val_loss: 38176.9354\n",
      "Epoch 247/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20983.9584 - val_loss: 38002.9442\n",
      "Epoch 248/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 20550.9601 - val_loss: 37834.4754\n",
      "Epoch 249/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 22429.8972 - val_loss: 37803.7357\n",
      "Epoch 250/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 21513.6880 - val_loss: 39095.9978\n",
      "Epoch 251/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 22123.1730 - val_loss: 37385.3510\n",
      "Epoch 252/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 21329.6272 - val_loss: 38259.5141\n",
      "Epoch 253/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20689.1722 - val_loss: 37801.4029\n",
      "Epoch 254/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 21106.6295 - val_loss: 37518.5321\n",
      "Epoch 255/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 20835.7314 - val_loss: 37438.0989\n",
      "Epoch 256/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 21129.6880 - val_loss: 37915.1560\n",
      "Epoch 257/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21246.4216 - val_loss: 39090.9880\n",
      "Epoch 258/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20713.5809 - val_loss: 41687.4096\n",
      "Epoch 259/500\n",
      "1164/1164 [==============================] - 0s 154us/step - loss: 21996.1363 - val_loss: 37931.6601\n",
      "Epoch 260/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 21125.0885 - val_loss: 38291.9193\n",
      "Epoch 261/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 23932.0272 - val_loss: 38044.9541\n",
      "Epoch 262/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 20368.8539 - val_loss: 38416.1695\n",
      "Epoch 263/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21099.3296 - val_loss: 37414.1254\n",
      "Epoch 264/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 21702.3986 - val_loss: 38152.4858\n",
      "Epoch 265/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 19863.1092 - val_loss: 37645.1324\n",
      "Epoch 266/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19217.9720 - val_loss: 37539.5672\n",
      "Epoch 267/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 20202.2089 - val_loss: 39647.9312\n",
      "Epoch 268/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 20887.0760 - val_loss: 37111.4004\n",
      "Epoch 269/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 21620.5414 - val_loss: 37603.1153\n",
      "Epoch 270/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20163.5765 - val_loss: 38587.8747\n",
      "Epoch 271/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19923.9087 - val_loss: 37614.7137\n",
      "Epoch 272/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19569.1868 - val_loss: 38325.1320\n",
      "Epoch 273/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19681.5746 - val_loss: 37763.4749\n",
      "Epoch 274/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 19105.3479 - val_loss: 38980.0793\n",
      "Epoch 275/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 20857.6504 - val_loss: 37648.0350\n",
      "Epoch 276/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 19100.0269 - val_loss: 39036.2216\n",
      "Epoch 277/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19652.6779 - val_loss: 38913.7207\n",
      "Epoch 278/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18983.3385 - val_loss: 37693.3802\n",
      "Epoch 279/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 18430.1724 - val_loss: 37902.4269\n",
      "Epoch 280/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19392.6579 - val_loss: 38478.9181\n",
      "Epoch 281/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19146.8597 - val_loss: 37224.9610\n",
      "Epoch 282/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18781.4567 - val_loss: 40000.4411\n",
      "Epoch 283/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19076.2832 - val_loss: 38050.7733\n",
      "Epoch 284/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19080.7690 - val_loss: 39219.9004\n",
      "Epoch 285/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 160us/step - loss: 19223.7877 - val_loss: 38843.5109\n",
      "Epoch 286/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18868.5739 - val_loss: 37600.3272\n",
      "Epoch 287/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18485.2802 - val_loss: 38107.1495\n",
      "Epoch 288/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18463.5082 - val_loss: 40261.4744\n",
      "Epoch 289/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19218.3906 - val_loss: 40034.7559\n",
      "Epoch 290/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19275.4340 - val_loss: 38474.6564\n",
      "Epoch 291/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 18869.3455 - val_loss: 37746.1772\n",
      "Epoch 292/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 20223.0720 - val_loss: 37732.5231\n",
      "Epoch 293/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 20127.6555 - val_loss: 37881.6365\n",
      "Epoch 294/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 20102.7116 - val_loss: 37571.3051\n",
      "Epoch 295/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 18875.3649 - val_loss: 38576.2475\n",
      "Epoch 296/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18821.4416 - val_loss: 38011.4563\n",
      "Epoch 297/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 19650.7209 - val_loss: 38353.2147\n",
      "Epoch 298/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 19720.5634 - val_loss: 37847.6343\n",
      "Epoch 299/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 17759.6121 - val_loss: 37856.1966\n",
      "Epoch 300/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18638.2556 - val_loss: 37898.9138\n",
      "Epoch 301/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 17435.6651 - val_loss: 38000.3481\n",
      "Epoch 302/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17191.8700 - val_loss: 37770.5442\n",
      "Epoch 303/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 18646.6198 - val_loss: 40796.6436\n",
      "Epoch 304/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 19026.9206 - val_loss: 41464.2781\n",
      "Epoch 305/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18678.4735 - val_loss: 38931.1043\n",
      "Epoch 306/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18044.2267 - val_loss: 38389.7065\n",
      "Epoch 307/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 17442.1048 - val_loss: 39335.8297\n",
      "Epoch 308/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18827.4116 - val_loss: 39772.3940\n",
      "Epoch 309/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17324.6472 - val_loss: 38547.4348\n",
      "Epoch 310/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18413.5673 - val_loss: 38043.9988\n",
      "Epoch 311/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 20688.6768 - val_loss: 38838.3060\n",
      "Epoch 312/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18593.1461 - val_loss: 40592.2323\n",
      "Epoch 313/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 18379.1248 - val_loss: 38968.5753\n",
      "Epoch 314/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 18270.8143 - val_loss: 38675.8415\n",
      "Epoch 315/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 19632.3524 - val_loss: 38623.8301\n",
      "Epoch 316/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 17586.1073 - val_loss: 40241.1493\n",
      "Epoch 317/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 17398.4044 - val_loss: 37696.6504\n",
      "Epoch 318/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16544.6381 - val_loss: 39740.0193\n",
      "Epoch 319/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17537.4563 - val_loss: 38654.3097\n",
      "Epoch 320/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 18039.6075 - val_loss: 38779.5340\n",
      "Epoch 321/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 17608.3681 - val_loss: 40690.8045\n",
      "Epoch 322/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 17210.4401 - val_loss: 38471.6595\n",
      "Epoch 323/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17388.5854 - val_loss: 39548.6759\n",
      "Epoch 324/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16895.4578 - val_loss: 38807.2674\n",
      "Epoch 325/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16660.2819 - val_loss: 39207.6022\n",
      "Epoch 326/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16515.8929 - val_loss: 38792.1101\n",
      "Epoch 327/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16183.1116 - val_loss: 40038.3401\n",
      "Epoch 328/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16553.9980 - val_loss: 38404.0399\n",
      "Epoch 329/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16844.2291 - val_loss: 39725.0360\n",
      "Epoch 330/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 16406.5155 - val_loss: 39450.1851\n",
      "Epoch 331/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16618.7569 - val_loss: 39087.1997\n",
      "Epoch 332/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 18734.0362 - val_loss: 39386.1453\n",
      "Epoch 333/500\n",
      "1164/1164 [==============================] - 0s 155us/step - loss: 16880.3776 - val_loss: 39326.3884\n",
      "Epoch 334/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16235.9938 - val_loss: 38325.6917\n",
      "Epoch 335/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16987.4169 - val_loss: 39240.1021\n",
      "Epoch 336/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16183.6002 - val_loss: 38397.3274\n",
      "Epoch 337/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 16206.6551 - val_loss: 40297.5967\n",
      "Epoch 338/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 16493.8527 - val_loss: 39056.5242\n",
      "Epoch 339/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 16576.2259 - val_loss: 39242.8662\n",
      "Epoch 340/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 16157.8047 - val_loss: 39509.8276\n",
      "Epoch 341/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 16155.5570 - val_loss: 43999.5747\n",
      "Epoch 342/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 17659.4900 - val_loss: 39872.9080\n",
      "Epoch 343/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16041.9440 - val_loss: 39447.9648\n",
      "Epoch 344/500\n",
      "1164/1164 [==============================] - 0s 172us/step - loss: 15673.1208 - val_loss: 39720.3668\n",
      "Epoch 345/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 15723.7619 - val_loss: 41059.1589\n",
      "Epoch 346/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 15659.5345 - val_loss: 39314.5558\n",
      "Epoch 347/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 17793.4485 - val_loss: 38946.9845\n",
      "Epoch 348/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 17895.6257 - val_loss: 40081.7414\n",
      "Epoch 349/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 19470.7780 - val_loss: 38344.0825\n",
      "Epoch 350/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16654.1534 - val_loss: 40551.8722\n",
      "Epoch 351/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 16154.9522 - val_loss: 38950.8379\n",
      "Epoch 352/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 15783.5996 - val_loss: 39842.1618\n",
      "Epoch 353/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 15364.7552 - val_loss: 39696.4467\n",
      "Epoch 354/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 16348.9560 - val_loss: 40639.8262\n",
      "Epoch 355/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 16932.3546 - val_loss: 39019.6820\n",
      "Epoch 356/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 172us/step - loss: 15884.4802 - val_loss: 39214.2293\n",
      "Epoch 357/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 16879.8036 - val_loss: 39219.8838\n",
      "Epoch 358/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 15991.7709 - val_loss: 39620.2223\n",
      "Epoch 359/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 15310.2276 - val_loss: 40906.7883\n",
      "Epoch 360/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 18096.1780 - val_loss: 41176.5976\n",
      "Epoch 361/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15911.4205 - val_loss: 39136.8223\n",
      "Epoch 362/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 15799.2978 - val_loss: 39932.8179\n",
      "Epoch 363/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15710.5642 - val_loss: 39267.6482\n",
      "Epoch 364/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 14945.9055 - val_loss: 40002.8494\n",
      "Epoch 365/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 15202.7582 - val_loss: 39705.6155\n",
      "Epoch 366/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 15532.0875 - val_loss: 40776.6758\n",
      "Epoch 367/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15719.4604 - val_loss: 40346.9848\n",
      "Epoch 368/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 16742.3957 - val_loss: 40549.4224\n",
      "Epoch 369/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 16026.7432 - val_loss: 38870.5817\n",
      "Epoch 370/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 14611.4658 - val_loss: 40391.4457\n",
      "Epoch 371/500\n",
      "1164/1164 [==============================] - 0s 166us/step - loss: 14564.3721 - val_loss: 39229.4577\n",
      "Epoch 372/500\n",
      "1164/1164 [==============================] - 0s 173us/step - loss: 15510.9331 - val_loss: 40573.2604\n",
      "Epoch 373/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 15812.8844 - val_loss: 39948.1112\n",
      "Epoch 374/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 15604.5701 - val_loss: 40308.5121\n",
      "Epoch 375/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 14015.1608 - val_loss: 39699.2989\n",
      "Epoch 376/500\n",
      "1164/1164 [==============================] - 0s 169us/step - loss: 14978.8721 - val_loss: 40573.9130\n",
      "Epoch 377/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 13895.4619 - val_loss: 41221.4015\n",
      "Epoch 378/500\n",
      "1164/1164 [==============================] - 0s 168us/step - loss: 14631.4050 - val_loss: 39479.7165\n",
      "Epoch 379/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 14707.3950 - val_loss: 40691.0412\n",
      "Epoch 380/500\n",
      "1164/1164 [==============================] - 0s 170us/step - loss: 14559.2160 - val_loss: 40187.4277\n",
      "Epoch 381/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14275.8056 - val_loss: 39656.5324\n",
      "Epoch 382/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 15039.0907 - val_loss: 41235.5670\n",
      "Epoch 383/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 15476.3939 - val_loss: 43451.1041\n",
      "Epoch 384/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 15079.5113 - val_loss: 40103.2007\n",
      "Epoch 385/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 14770.9515 - val_loss: 39697.1952\n",
      "Epoch 386/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14142.5222 - val_loss: 41700.0600\n",
      "Epoch 387/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16187.0358 - val_loss: 40964.1201\n",
      "Epoch 388/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14780.9313 - val_loss: 40297.9461\n",
      "Epoch 389/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 14936.2823 - val_loss: 42972.8662\n",
      "Epoch 390/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14895.1076 - val_loss: 39739.1348\n",
      "Epoch 391/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 15707.6732 - val_loss: 40071.4268\n",
      "Epoch 392/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 15542.7464 - val_loss: 42189.0947\n",
      "Epoch 393/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 14705.1113 - val_loss: 38787.2683\n",
      "Epoch 394/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 14134.2575 - val_loss: 40374.2666\n",
      "Epoch 395/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 17462.5428 - val_loss: 39805.8993\n",
      "Epoch 396/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 15576.3146 - val_loss: 39487.0349\n",
      "Epoch 397/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 13837.6611 - val_loss: 39335.2313\n",
      "Epoch 398/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 15341.9047 - val_loss: 40784.2808\n",
      "Epoch 399/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 15018.2918 - val_loss: 40199.1077\n",
      "Epoch 400/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14319.0199 - val_loss: 40958.8238\n",
      "Epoch 401/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 14340.1669 - val_loss: 41162.9404\n",
      "Epoch 402/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 13210.1901 - val_loss: 40457.6597\n",
      "Epoch 403/500\n",
      "1164/1164 [==============================] - 0s 167us/step - loss: 13862.2007 - val_loss: 39547.8804\n",
      "Epoch 404/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13677.2392 - val_loss: 41091.0568\n",
      "Epoch 405/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13497.2910 - val_loss: 41295.4080\n",
      "Epoch 406/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 16136.6917 - val_loss: 40396.4076\n",
      "Epoch 407/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13348.2018 - val_loss: 40516.7990\n",
      "Epoch 408/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13212.9638 - val_loss: 40736.4814\n",
      "Epoch 409/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13018.9031 - val_loss: 41725.0881\n",
      "Epoch 410/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 15133.7006 - val_loss: 41138.9313\n",
      "Epoch 411/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13922.3944 - val_loss: 40220.1880\n",
      "Epoch 412/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12791.4169 - val_loss: 40093.1999\n",
      "Epoch 413/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12926.0074 - val_loss: 40673.8010\n",
      "Epoch 414/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13763.5508 - val_loss: 41442.7875\n",
      "Epoch 415/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 14844.3035 - val_loss: 41575.0179\n",
      "Epoch 416/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 15052.7153 - val_loss: 39805.4062\n",
      "Epoch 417/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13690.6358 - val_loss: 41202.6159\n",
      "Epoch 418/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13240.6836 - val_loss: 39461.4615\n",
      "Epoch 419/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13353.3957 - val_loss: 41960.9988\n",
      "Epoch 420/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13432.8167 - val_loss: 39793.3657\n",
      "Epoch 421/500\n",
      "1164/1164 [==============================] - 0s 156us/step - loss: 14099.8454 - val_loss: 42768.1267\n",
      "Epoch 422/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 14344.7231 - val_loss: 41009.1168\n",
      "Epoch 423/500\n",
      "1164/1164 [==============================] - 0s 157us/step - loss: 13294.0246 - val_loss: 41272.8337\n",
      "Epoch 424/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12981.1915 - val_loss: 40183.4171\n",
      "Epoch 425/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12785.8488 - val_loss: 40887.3032\n",
      "Epoch 426/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 14137.0451 - val_loss: 41753.9664\n",
      "Epoch 427/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 158us/step - loss: 14523.6876 - val_loss: 43397.1966\n",
      "Epoch 428/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13716.2277 - val_loss: 41724.9843\n",
      "Epoch 429/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13037.9280 - val_loss: 40554.8354\n",
      "Epoch 430/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13784.9459 - val_loss: 40448.4096\n",
      "Epoch 431/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13848.4145 - val_loss: 42653.8635\n",
      "Epoch 432/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13699.0293 - val_loss: 40299.0116\n",
      "Epoch 433/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12071.9218 - val_loss: 40882.2461\n",
      "Epoch 434/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13329.4149 - val_loss: 40500.4345\n",
      "Epoch 435/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13399.1613 - val_loss: 41515.7128\n",
      "Epoch 436/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12139.7064 - val_loss: 39964.3952\n",
      "Epoch 437/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12492.2845 - val_loss: 40996.6708\n",
      "Epoch 438/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 13216.6302 - val_loss: 40405.4676\n",
      "Epoch 439/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12384.8943 - val_loss: 41735.8005\n",
      "Epoch 440/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12183.7744 - val_loss: 40200.0452\n",
      "Epoch 441/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 13289.3649 - val_loss: 41797.8221\n",
      "Epoch 442/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12110.8267 - val_loss: 41346.9310\n",
      "Epoch 443/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 13496.3509 - val_loss: 41270.8564\n",
      "Epoch 444/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12977.6221 - val_loss: 42602.7133\n",
      "Epoch 445/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 13413.1574 - val_loss: 41457.1247\n",
      "Epoch 446/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13786.4421 - val_loss: 40575.0891\n",
      "Epoch 447/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 12596.0192 - val_loss: 41243.4157\n",
      "Epoch 448/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12690.7170 - val_loss: 41137.9997\n",
      "Epoch 449/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12243.9426 - val_loss: 40923.2136\n",
      "Epoch 450/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11777.9568 - val_loss: 41085.5662\n",
      "Epoch 451/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12157.9833 - val_loss: 41052.5875\n",
      "Epoch 452/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12615.7354 - val_loss: 41664.9140\n",
      "Epoch 453/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11981.8073 - val_loss: 41212.2406\n",
      "Epoch 454/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11677.4469 - val_loss: 41101.9188\n",
      "Epoch 455/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12259.8033 - val_loss: 41345.0915\n",
      "Epoch 456/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11849.1602 - val_loss: 40411.4397\n",
      "Epoch 457/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14455.9818 - val_loss: 41332.6718\n",
      "Epoch 458/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12712.6012 - val_loss: 40931.2631\n",
      "Epoch 459/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 13065.7323 - val_loss: 45985.9686\n",
      "Epoch 460/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 14444.2488 - val_loss: 39517.3395\n",
      "Epoch 461/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12357.7335 - val_loss: 41919.5432\n",
      "Epoch 462/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11412.1529 - val_loss: 40308.6211\n",
      "Epoch 463/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11422.9647 - val_loss: 41414.0864\n",
      "Epoch 464/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12482.5546 - val_loss: 40244.0855\n",
      "Epoch 465/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13277.9149 - val_loss: 41419.1516\n",
      "Epoch 466/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 12674.3513 - val_loss: 40451.3993\n",
      "Epoch 467/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11708.2323 - val_loss: 41640.4858\n",
      "Epoch 468/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12769.9747 - val_loss: 40133.0025\n",
      "Epoch 469/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11936.4459 - val_loss: 40729.1540\n",
      "Epoch 470/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12336.1627 - val_loss: 41100.9954\n",
      "Epoch 471/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 11579.1097 - val_loss: 41493.7196\n",
      "Epoch 472/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11258.9750 - val_loss: 40930.2070\n",
      "Epoch 473/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11994.1730 - val_loss: 41697.8211\n",
      "Epoch 474/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 11835.7246 - val_loss: 41206.8512\n",
      "Epoch 475/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11982.2333 - val_loss: 41689.4320\n",
      "Epoch 476/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 12396.6449 - val_loss: 40007.4434\n",
      "Epoch 477/500\n",
      "1164/1164 [==============================] - 0s 158us/step - loss: 11323.8524 - val_loss: 42641.9654\n",
      "Epoch 478/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 12158.9246 - val_loss: 40723.4179\n",
      "Epoch 479/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11218.3632 - val_loss: 41915.6149\n",
      "Epoch 480/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 12726.1265 - val_loss: 39920.1422\n",
      "Epoch 481/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 10879.9961 - val_loss: 41786.0981\n",
      "Epoch 482/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 12085.6461 - val_loss: 40185.9988\n",
      "Epoch 483/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 11708.8747 - val_loss: 40460.8083\n",
      "Epoch 484/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 10982.3738 - val_loss: 42510.6145\n",
      "Epoch 485/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 13200.6554 - val_loss: 39975.2106\n",
      "Epoch 486/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 12919.2656 - val_loss: 40477.4426\n",
      "Epoch 487/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 12355.5672 - val_loss: 40031.9141\n",
      "Epoch 488/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11300.3811 - val_loss: 42109.0684\n",
      "Epoch 489/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 10973.9273 - val_loss: 41111.4598\n",
      "Epoch 490/500\n",
      "1164/1164 [==============================] - 0s 165us/step - loss: 11971.8062 - val_loss: 41790.1564\n",
      "Epoch 491/500\n",
      "1164/1164 [==============================] - 0s 164us/step - loss: 11364.2863 - val_loss: 41920.8279\n",
      "Epoch 492/500\n",
      "1164/1164 [==============================] - 0s 163us/step - loss: 10829.0715 - val_loss: 40846.4341\n",
      "Epoch 493/500\n",
      "1164/1164 [==============================] - 0s 160us/step - loss: 13952.9954 - val_loss: 41342.0292\n",
      "Epoch 494/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11444.7508 - val_loss: 40826.4385\n",
      "Epoch 495/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11279.7510 - val_loss: 42676.8892\n",
      "Epoch 496/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 10620.3781 - val_loss: 40238.4325\n",
      "Epoch 497/500\n",
      "1164/1164 [==============================] - 0s 162us/step - loss: 10827.0776 - val_loss: 41711.3166\n",
      "Epoch 498/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 163us/step - loss: 12505.8379 - val_loss: 42329.2046\n",
      "Epoch 499/500\n",
      "1164/1164 [==============================] - 0s 161us/step - loss: 11581.0677 - val_loss: 41041.9296\n",
      "Epoch 500/500\n",
      "1164/1164 [==============================] - 0s 159us/step - loss: 11132.4347 - val_loss: 42397.6687\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "newAdam = Adam(learning_rate=0.0005)\n",
    "NN_500E_NewAdam.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = NN_500E_NewAdam.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN_500E_NewAdam.save('NN_5000E_NewAdam_V2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_5 (Dense)              (None, 512)               169472    \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 695,297\n",
      "Trainable params: 695,297\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_Sig = Sequential()\n",
    "NN_5000E_Adam_Sig.add(Dense(512,input_dim = 330,activation = 'sigmoid'))\n",
    "NN_5000E_Adam_Sig.add(Dense(512,activation = 'sigmoid'))\n",
    "NN_5000E_Adam_Sig.add(Dense(512,activation = 'sigmoid'))\n",
    "NN_5000E_Adam_Sig.add(Dense(1))\n",
    "NN_5000E_Adam_Sig.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 364us/step - loss: 37765465534.6290 - val_loss: 38856311962.3014\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37760839083.3248 - val_loss: 38851632927.5616\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37756176252.3805 - val_loss: 38846826748.4931\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37751193261.2991 - val_loss: 38841367622.1370\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37745548971.5441 - val_loss: 38835620204.7123\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37739652777.7892 - val_loss: 38829458361.8630\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37733663901.0660 - val_loss: 38823570894.9041\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37727911614.8483 - val_loss: 38817877006.0274\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37722319242.8586 - val_loss: 38812273734.1370\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37716798786.0291 - val_loss: 38806748174.0274\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37711336034.7147 - val_loss: 38801228168.7671\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37705886606.8072 - val_loss: 38795753387.8356\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37700468985.1997 - val_loss: 38790304978.4110\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37695077452.3393 - val_loss: 38784852697.4247\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37689687407.6572 - val_loss: 38779443452.4931\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37684336289.0146 - val_loss: 38773981127.8904\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 37678964479.7806 - val_loss: 38768613291.8356\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37673629230.9443 - val_loss: 38763233897.2055\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37668293715.3590 - val_loss: 38757846759.4521\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37662964371.8526 - val_loss: 38752457489.5342\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37657637643.1877 - val_loss: 38747105195.8356\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37652331194.4610 - val_loss: 38741736013.1507\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37647024177.1380 - val_loss: 38736370926.4658\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37641713795.6195 - val_loss: 38731015658.9589\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37636415628.3942 - val_loss: 38725684518.5753\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 37631123020.7781 - val_loss: 38720334525.3699\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 37625830946.6598 - val_loss: 38714992836.3836\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37620546639.8492 - val_loss: 38709638915.5069\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37615245229.5184 - val_loss: 38704294140.4931\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37609942443.3248 - val_loss: 38698978486.3562\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37604684702.6015 - val_loss: 38693630008.1096\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37599393523.4961 - val_loss: 38688331186.8493\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37594135414.2382 - val_loss: 38682998924.2740\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37588857109.2785 - val_loss: 38677657515.8356\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 37583581963.1877 - val_loss: 38672350278.1370\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37578291987.9623 - val_loss: 38667061724.9315\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 37573038374.8278 - val_loss: 38661679749.2603\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37567736295.8698 - val_loss: 38656375036.4931\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37562480097.7275 - val_loss: 38651053715.2877\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37557219478.9237 - val_loss: 38645727063.6712\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37551939696.3153 - val_loss: 38640386833.5342\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37546659359.1500 - val_loss: 38635104620.7123\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37541417953.2888 - val_loss: 38629788517.6986\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37536160465.2751 - val_loss: 38624502489.4247\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37530913073.3573 - val_loss: 38619158107.1781\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37525648042.6667 - val_loss: 38613881112.5479\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37520407607.2802 - val_loss: 38608596430.9041\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37515154127.5201 - val_loss: 38603312198.1370\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37509925992.4182 - val_loss: 38597969891.9452\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37504646111.5338 - val_loss: 38592705465.8630\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37499414807.0334 - val_loss: 38587396600.9863\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37494167399.3213 - val_loss: 38582122131.2877\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37488929294.4781 - val_loss: 38576804625.5342\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37483697800.4456 - val_loss: 38571488298.0822\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37478438696.1440 - val_loss: 38566246876.9315\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37473214530.6872 - val_loss: 38560950580.6027\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37467961073.7412 - val_loss: 38555669994.9589\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37462729218.1937 - val_loss: 38550376167.4521\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37457483360.0823 - val_loss: 38545103549.3699\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37452250307.6744 - val_loss: 38539755295.5616\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 37446980712.4182 - val_loss: 38534491767.2329\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37441751536.6444 - val_loss: 38529196368.6575\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 37436511606.6769 - val_loss: 38523888682.0822\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37431263393.4533 - val_loss: 38518638339.5069\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37426032510.1354 - val_loss: 38513364374.7945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37420806614.3205 - val_loss: 38508010622.2466\n",
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37415536322.3582 - val_loss: 38502743671.2329\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 37410309570.1388 - val_loss: 38497450797.5890\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37405076672.1645 - val_loss: 38492161066.0822\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37399840706.5776 - val_loss: 38486900736.0000\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37394613480.5278 - val_loss: 38481583735.2329\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37389368189.2579 - val_loss: 38476314427.6164\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37384131820.0377 - val_loss: 38471026547.7260\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37378893752.0480 - val_loss: 38465764871.0137\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 37373654076.5450 - val_loss: 38460444840.3288\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37368407909.5664 - val_loss: 38455184285.8082\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37363190939.3111 - val_loss: 38449917334.7945\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37357977947.4756 - val_loss: 38444616044.7123\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 232us/step - loss: 37352746407.8149 - val_loss: 38439353806.9041\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37347513144.8158 - val_loss: 38434082198.7945\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37342292452.3599 - val_loss: 38428770079.5616\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37337063094.0737 - val_loss: 38423503857.9726\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 37331837698.4130 - val_loss: 38418243303.4521\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37326635123.8252 - val_loss: 38412945828.8219\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37321405409.2888 - val_loss: 38407698403.9452\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37316182065.1380 - val_loss: 38402445368.1096\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37310959214.1217 - val_loss: 38397139982.0274\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37305726830.3410 - val_loss: 38391896877.5890\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37300516069.0180 - val_loss: 38386602601.2055\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37295286014.0257 - val_loss: 38381376441.8630\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37290090327.5270 - val_loss: 38376056916.1644\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37284859151.1362 - val_loss: 38370802926.4658\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37279636545.8098 - val_loss: 38365580582.5753\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37274431644.1885 - val_loss: 38360293376.0000\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 261us/step - loss: 37269211490.4953 - val_loss: 38355026424.9863\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37263998726.8003 - val_loss: 38349771313.0959\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37258781661.7789 - val_loss: 38344476419.5069\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37253551715.5921 - val_loss: 38339209412.3836\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37248341363.1671 - val_loss: 38333921981.3699\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37243107001.5835 - val_loss: 38328704182.3562\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37237913622.8141 - val_loss: 38323423877.2603\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37232689752.1851 - val_loss: 38318160012.2740\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37227471353.4190 - val_loss: 38312884925.3699\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37222256734.7661 - val_loss: 38307633685.0411\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37217034966.5398 - val_loss: 38302391983.3425\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 37211833859.0711 - val_loss: 38297087214.4658\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37206617750.4850 - val_loss: 38291842819.5069\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37201403226.5981 - val_loss: 38286597021.8082\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37196193325.1894 - val_loss: 38281309366.3562\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37190956275.0574 - val_loss: 38276083150.9041\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 37185759844.4696 - val_loss: 38270775239.8904\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37180552915.9075 - val_loss: 38265488706.6301\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 37175335580.6273 - val_loss: 38260260190.6849\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37170134476.6684 - val_loss: 38255014841.8630\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37164942464.9871 - val_loss: 38249735434.5205\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 37159723299.3179 - val_loss: 38244519655.4521\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37154513106.5913 - val_loss: 38239258652.0548\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37149297175.2528 - val_loss: 38233980310.7945\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37144087101.8612 - val_loss: 38228714481.9726\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 37138891680.3565 - val_loss: 38223466888.7671\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 266us/step - loss: 37133700274.1251 - val_loss: 38218225635.9452\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 279us/step - loss: 37128497918.9032 - val_loss: 38212981016.5479\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 37123279497.3231 - val_loss: 38207658012.0548\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 37118050898.9203 - val_loss: 38202443242.9589\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 37112870410.0908 - val_loss: 38197196996.3836\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37107671666.5090 - val_loss: 38191969883.1781\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 37102483220.8398 - val_loss: 38186710170.3014\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37097282608.2605 - val_loss: 38181456629.4795\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37092088992.5758 - val_loss: 38176209316.8219\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 37086897475.7841 - val_loss: 38170950782.2466\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37081676500.7849 - val_loss: 38165743924.6027\n",
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 37076492333.6281 - val_loss: 38160474673.0959\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37071288535.8560 - val_loss: 38155209068.7123\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 37066082467.2082 - val_loss: 38149985097.6438\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 37060892203.4344 - val_loss: 38144735708.9315\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 37055698521.0626 - val_loss: 38139490808.9863\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37050510521.1448 - val_loss: 38134242766.9041\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 37045310631.5955 - val_loss: 38129019974.1370\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 37040122110.4644 - val_loss: 38123787867.1781\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37034932345.0900 - val_loss: 38118537075.7260\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 37029733970.0428 - val_loss: 38113249364.1644\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 37024533102.9991 - val_loss: 38108021128.7671\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37019349769.4327 - val_loss: 38102786328.5479\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37014152413.9983 - val_loss: 38097569483.3973\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 37008977656.7609 - val_loss: 38092310219.3973\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 37003781055.9452 - val_loss: 38087067844.3836\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36998602095.6572 - val_loss: 38081815201.3151\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36993409782.1285 - val_loss: 38076591959.6712\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36988220771.3727 - val_loss: 38071369615.7808\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36983046335.2871 - val_loss: 38066106985.2055\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36977850176.7129 - val_loss: 38060896704.8767\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36972665072.4250 - val_loss: 38055629753.8630\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36967467817.0214 - val_loss: 38050398600.7671\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36962292052.4559 - val_loss: 38045169860.3836\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 36957100470.7318 - val_loss: 38039933601.3151\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36951917293.3539 - val_loss: 38034705983.1233\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36946739851.0780 - val_loss: 38029463608.1096\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36941544127.7258 - val_loss: 38024268196.8219\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36936383102.7935 - val_loss: 38019013982.6849\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36931187955.0574 - val_loss: 38013806171.1781\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36926004456.5278 - val_loss: 38008553584.2192\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36920812358.8552 - val_loss: 38003322318.9041\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36915638272.0000 - val_loss: 37998049700.8219\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36910428823.3625 - val_loss: 37992847388.0548\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36905246484.8398 - val_loss: 37987606415.7808\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36900053135.9040 - val_loss: 37982389290.0822\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36894877671.4310 - val_loss: 37977123012.3836\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36889692790.0189 - val_loss: 37971896291.9452\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36884513408.5484 - val_loss: 37966680120.1096\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36879343073.7275 - val_loss: 37961446666.5205\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36874157518.4233 - val_loss: 37956237059.5069\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36868993311.8081 - val_loss: 37950990307.9452\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36863806082.3033 - val_loss: 37945746810.7397\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36858624101.7858 - val_loss: 37940518294.7945\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36853450601.0763 - val_loss: 37935292808.7671\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36848265182.6564 - val_loss: 37930089766.5753\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36843104766.6838 - val_loss: 37924861755.6164\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36837918342.6907 - val_loss: 37919636494.0274\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36832744552.4182 - val_loss: 37914377005.5890\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36827554623.8355 - val_loss: 37909184455.8904\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36822380243.9075 - val_loss: 37903976700.4931\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36817219738.4336 - val_loss: 37898697405.3699\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36812023181.4910 - val_loss: 37893487125.0411\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36806838556.2982 - val_loss: 37888244020.6027\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36801664183.3899 - val_loss: 37883020610.6301\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36796487246.5330 - val_loss: 37877820317.8082\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36791324728.1577 - val_loss: 37872539002.7397\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36786132021.5253 - val_loss: 37867385224.7671\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36780971103.6435 - val_loss: 37862153959.4521\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36775797185.2614 - val_loss: 37856912257.7534\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36770605436.8192 - val_loss: 37851695580.9315\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36765436755.1397 - val_loss: 37846427283.2877\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36760254213.0454 - val_loss: 37841238380.7123\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36755090378.4747 - val_loss: 37836006947.0685\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36749910679.3625 - val_loss: 37830795544.5479\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36744732220.9837 - val_loss: 37825564503.6712\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36739562358.2382 - val_loss: 37820348331.8356\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36734393114.9820 - val_loss: 37815105507.9452\n",
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36729206982.3068 - val_loss: 37809914360.9863\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36724067048.0891 - val_loss: 37804649654.3562\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36718885762.5227 - val_loss: 37799498289.0959\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36713746523.2562 - val_loss: 37794242952.7671\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36708551731.7703 - val_loss: 37789052030.2466\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36703393546.3102 - val_loss: 37783822392.1096\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36698241883.9143 - val_loss: 37778583383.6712\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 36693065987.7292 - val_loss: 37773407947.3973\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36687915295.8081 - val_loss: 37768155809.3151\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36682738381.7652 - val_loss: 37762989687.2329\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36677585582.1765 - val_loss: 37757770036.6027\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36672410975.8629 - val_loss: 37752545392.2192\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36667245460.0720 - val_loss: 37747282312.7671\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36662063354.9546 - val_loss: 37742092231.8904\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 36656903969.1243 - val_loss: 37736917749.4795\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36651756141.2442 - val_loss: 37731698996.6027\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36646592547.0985 - val_loss: 37726504707.5069\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36641443396.0034 - val_loss: 37721255262.6849\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 36636271746.7421 - val_loss: 37716065013.4795\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36631113720.9803 - val_loss: 37710830830.4658\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36625949259.9006 - val_loss: 37705623411.7260\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 36620790837.5253 - val_loss: 37700415936.8767\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36615629305.4190 - val_loss: 37695241847.2329\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36610491094.5398 - val_loss: 37690007383.6712\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36605325087.3693 - val_loss: 37684822352.6575\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36600165927.9246 - val_loss: 37679626885.2603\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36595020141.0248 - val_loss: 37674372671.1233\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36589855451.8046 - val_loss: 37669180345.8630\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36584700900.7986 - val_loss: 37664016524.2740\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 36579564657.1928 - val_loss: 37658818026.9589\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36574417766.4439 - val_loss: 37653606400.0000\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36569262629.2922 - val_loss: 37648405882.7397\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36564100437.3333 - val_loss: 37643202952.7671\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36558933061.3196 - val_loss: 37638020166.1370\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36553779938.8243 - val_loss: 37632784355.9452\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36548625498.3787 - val_loss: 37627556569.4247\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36543486457.4190 - val_loss: 37622326650.7397\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36538324284.7644 - val_loss: 37617155759.3425\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36533164733.9709 - val_loss: 37611957767.0137\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36528019554.2759 - val_loss: 37606744961.7534\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36522864875.1602 - val_loss: 37601571096.5479\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36517725995.6538 - val_loss: 37596363790.0274\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36512575803.8869 - val_loss: 37591126857.6438\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36507421358.1765 - val_loss: 37585962026.0822\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36502278017.6452 - val_loss: 37580745237.0411\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36497121678.3685 - val_loss: 37575553865.6438\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36491978123.7361 - val_loss: 37570356939.3973\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 263us/step - loss: 36486824550.2245 - val_loss: 37565189021.8082\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36481680878.0120 - val_loss: 37559959327.5616\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36476527276.4216 - val_loss: 37554780973.5890\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36471390760.8021 - val_loss: 37549559695.7808\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36466234447.8492 - val_loss: 37544397894.1370\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36461102354.6461 - val_loss: 37539197208.5479\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36455948382.7661 - val_loss: 37533989397.0411\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36450784237.5733 - val_loss: 37528814577.9726\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36445656654.5330 - val_loss: 37523602277.6986\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 36440516439.5270 - val_loss: 37518405856.4384\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36435368501.0865 - val_loss: 37513224248.1096\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36430228265.0214 - val_loss: 37508043088.6575\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36425086474.0908 - val_loss: 37502852615.0137\n",
      "Epoch 259/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 247us/step - loss: 36419954521.2819 - val_loss: 37497654173.8082\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36414807691.9554 - val_loss: 37492467964.4931\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36409664979.6881 - val_loss: 37487280240.2192\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36404526596.8260 - val_loss: 37482079274.0822\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36399381996.2571 - val_loss: 37476889922.6301\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36394242837.7172 - val_loss: 37471721556.1644\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36389099426.9889 - val_loss: 37466531755.8356\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36383964840.0343 - val_loss: 37461280178.8493\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36378782161.0557 - val_loss: 37456092005.6986\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36373650673.3025 - val_loss: 37450890815.1233\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36368516725.1414 - val_loss: 37445692093.3699\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36363371208.5004 - val_loss: 37440529618.4110\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36358231825.3299 - val_loss: 37435357997.5890\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36353113188.9083 - val_loss: 37430156582.5753\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36347975021.9023 - val_loss: 37424993883.1781\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36342831395.3179 - val_loss: 37419799032.9863\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36337682840.8980 - val_loss: 37414648228.8219\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 36332569081.4190 - val_loss: 37409404619.3973\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36327413933.7378 - val_loss: 37404236926.2466\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36322267120.2057 - val_loss: 37399056215.6712\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 36317144593.1105 - val_loss: 37393850255.7808\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 36312007049.1037 - val_loss: 37388702537.6438\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36306888967.2391 - val_loss: 37383502300.9315\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 36301742188.8055 - val_loss: 37378355256.1096\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36296630752.8500 - val_loss: 37373142001.9726\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36291493768.6650 - val_loss: 37367950350.0274\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36286346165.4156 - val_loss: 37362801958.5753\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36281229922.7147 - val_loss: 37357618105.8630\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36276085910.0463 - val_loss: 37352403505.0959\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36270953609.7618 - val_loss: 37347220718.4658\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 36265826140.7918 - val_loss: 37342053923.0685\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36260716063.1500 - val_loss: 37336876579.0685\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36255573484.2571 - val_loss: 37331739409.5342\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36250453226.2828 - val_loss: 37326506965.9178\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36245300365.2716 - val_loss: 37321396167.8904\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 36240192421.6213 - val_loss: 37316161760.4384\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36235063878.6358 - val_loss: 37310972128.4384\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36229932043.4070 - val_loss: 37305803930.3014\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36224804581.4567 - val_loss: 37300599317.0411\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36219663954.0428 - val_loss: 37295469890.6301\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36214559337.7344 - val_loss: 37290272739.9452\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36209424078.6427 - val_loss: 37285109535.5616\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36204313581.5733 - val_loss: 37279950932.1644\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 36199197549.4636 - val_loss: 37274787615.5616\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 36194080402.9752 - val_loss: 37269625308.9315\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36188970758.8003 - val_loss: 37264434105.8630\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36183820054.5947 - val_loss: 37259303052.2740\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36178706607.4927 - val_loss: 37254086824.3288\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36173576018.2622 - val_loss: 37248916325.6986\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36168465096.5004 - val_loss: 37243720633.8630\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36163333740.3668 - val_loss: 37238573140.1644\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36158227646.4096 - val_loss: 37233417454.4658\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36153112465.4396 - val_loss: 37228241513.2055\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36147989039.8218 - val_loss: 37223084032.0000\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36142873855.3419 - val_loss: 37217873976.1096\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36137743181.8749 - val_loss: 37212752966.1370\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36132629762.8518 - val_loss: 37207592679.4521\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36127516654.4507 - val_loss: 37202380323.0685\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 36122391075.5373 - val_loss: 37197232604.9315\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36117283753.1311 - val_loss: 37192073103.7808\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36112177306.4336 - val_loss: 37186916295.8904\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36107069568.1097 - val_loss: 37181765435.6164\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36101961087.4516 - val_loss: 37176594488.1096\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36096839739.6675 - val_loss: 37171463378.4110\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 36091744656.1234 - val_loss: 37166295685.2603\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 36086628704.7404 - val_loss: 37161114750.2466\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36081514371.4002 - val_loss: 37155955641.8630\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 36076391449.4464 - val_loss: 37150796589.5890\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36071294519.7189 - val_loss: 37145628391.4521\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36066187707.9966 - val_loss: 37140466758.1370\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36061061009.4396 - val_loss: 37135335480.1096\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36055966892.8603 - val_loss: 37130164083.7260\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36050849756.9015 - val_loss: 37125011147.3973\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 36045751152.9734 - val_loss: 37119852039.0137\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 36040645327.5201 - val_loss: 37114682045.3699\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 36035530141.2853 - val_loss: 37109530680.1096\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 36030408038.8826 - val_loss: 37104350923.3973\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 36025288042.3925 - val_loss: 37099204103.0137\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 36020182773.2511 - val_loss: 37094054140.4931\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 36015077832.2811 - val_loss: 37088886840.1096\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 36009970546.7284 - val_loss: 37083726609.5342\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 36004866179.6195 - val_loss: 37078559589.6986\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35999757114.5707 - val_loss: 37073407719.4521\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 35994657141.7995 - val_loss: 37068249340.4931\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35989544349.2853 - val_loss: 37063137420.2740\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35984452171.9006 - val_loss: 37057950481.5342\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 35979335090.3445 - val_loss: 37052804166.1370\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35974242051.2905 - val_loss: 37047664696.1096\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35969122398.7661 - val_loss: 37042488025.4247\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35964023993.1448 - val_loss: 37037292277.4795\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 35958906844.9015 - val_loss: 37032160494.4658\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 35953812694.9786 - val_loss: 37027038362.3014\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35948723669.4430 - val_loss: 37021897208.9863\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35943625869.2716 - val_loss: 37016731311.3425\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35938517441.2614 - val_loss: 37011590943.5616\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35933419521.7549 - val_loss: 37006443225.4247\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35928322862.2862 - val_loss: 37001298424.9863\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35923230899.0026 - val_loss: 36996116592.2192\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 35918111632.5621 - val_loss: 36990960008.7671\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35912997932.7506 - val_loss: 36985822502.5753\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35907907494.4987 - val_loss: 36980661149.8082\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35902813186.6324 - val_loss: 36975506361.8630\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35897707810.4404 - val_loss: 36970397808.2192\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35892624390.1422 - val_loss: 36965209578.9589\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35887507780.6615 - val_loss: 36960097097.6438\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35882401549.8200 - val_loss: 36954953531.6164\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35877304241.0283 - val_loss: 36949805589.0411\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35872211302.0051 - val_loss: 36944665614.0274\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35867123887.4927 - val_loss: 36939525863.4521\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35862041369.2271 - val_loss: 36934352671.5616\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35856934506.6118 - val_loss: 36929213145.4247\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35851843320.7609 - val_loss: 36924054766.4658\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35846749421.7926 - val_loss: 36918922534.5753\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35841644092.9837 - val_loss: 36913796194.1918\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35836550499.3727 - val_loss: 36908629623.2329\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35831462735.6298 - val_loss: 36903483700.6027\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 237us/step - loss: 35826379034.5433 - val_loss: 36898340527.3425\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35821295631.7943 - val_loss: 36893202852.8219\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35816205391.8492 - val_loss: 36888106587.1781\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35811131095.4173 - val_loss: 36882930870.3562\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35806033077.6350 - val_loss: 36877802958.9041\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 35800944176.6992 - val_loss: 36872693283.0685\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35795864071.4584 - val_loss: 36867565203.2877\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35790777920.4936 - val_loss: 36862427977.6438\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35785704459.4070 - val_loss: 36857280652.2740\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35780618383.9040 - val_loss: 36852153245.8082\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35775528967.0197 - val_loss: 36847042784.4384\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35770445692.3805 - val_loss: 36841861176.1096\n",
      "Epoch 387/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 251us/step - loss: 35765347483.3111 - val_loss: 36836729617.5342\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35760273351.8423 - val_loss: 36831582572.7123\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35755194403.0985 - val_loss: 36826471101.3699\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35750124121.9400 - val_loss: 36821362547.7260\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35745035900.1611 - val_loss: 36816217578.9589\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 35739942648.7609 - val_loss: 36811075527.8904\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35734834733.1894 - val_loss: 36805947392.0000\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35729741021.9983 - val_loss: 36800814711.2329\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35724663213.9572 - val_loss: 36795629904.6575\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 35719579098.7078 - val_loss: 36790486058.0822\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35714489130.7764 - val_loss: 36785385023.1233\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35709415539.8252 - val_loss: 36780254811.1781\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35704327802.4062 - val_loss: 36775135260.0548\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35699258725.1277 - val_loss: 36769954100.6027\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 259us/step - loss: 35694158269.7515 - val_loss: 36764865185.3151\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35689083655.6778 - val_loss: 36759717467.1781\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35683996683.4070 - val_loss: 36754556226.6301\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35678904488.4730 - val_loss: 36749473258.9589\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35673836708.0857 - val_loss: 36744308595.7260\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 233us/step - loss: 35668745008.0411 - val_loss: 36739190503.4521\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35663668275.7703 - val_loss: 36734087672.9863\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 35658611556.6889 - val_loss: 36728949942.3562\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 35653520755.1671 - val_loss: 36723832691.7260\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35648457407.7258 - val_loss: 36718686881.3151\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 35643369354.4199 - val_loss: 36713566881.3151\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35638291914.9135 - val_loss: 36708435547.1781\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 243us/step - loss: 35633221717.9914 - val_loss: 36703280254.2466\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35628150201.3642 - val_loss: 36698189094.5753\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35623083852.9974 - val_loss: 36693123801.4247\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35618030750.8209 - val_loss: 36687977822.6849\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35612937648.5895 - val_loss: 36682865678.0274\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35607880897.9194 - val_loss: 36677730696.7671\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35602806747.1465 - val_loss: 36672601887.5616\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35597713997.6555 - val_loss: 36667485141.9178\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35592657273.3093 - val_loss: 36662336638.2466\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35587587899.4482 - val_loss: 36657225166.9041\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35582514327.8012 - val_loss: 36652158471.0137\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35577454069.0317 - val_loss: 36647006937.4247\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35572397202.5364 - val_loss: 36641883570.8493\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35567322527.0403 - val_loss: 36636799817.6438\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35562259679.7532 - val_loss: 36631674655.5616\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35557192122.2416 - val_loss: 36626551906.1918\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35552137461.6898 - val_loss: 36621415353.8630\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35547059857.2202 - val_loss: 36616313196.7123\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35542006239.0951 - val_loss: 36611180010.9589\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35536926546.2622 - val_loss: 36606071906.1918\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35531850363.2836 - val_loss: 36601003022.0274\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35526801437.8338 - val_loss: 36595833589.4795\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35521726370.9889 - val_loss: 36590710952.3288\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35516652451.8663 - val_loss: 36585583938.6301\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35511587179.2699 - val_loss: 36580473308.9315\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35506514967.6915 - val_loss: 36575421874.8493\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35501458917.2374 - val_loss: 36570268209.0959\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35496384141.7104 - val_loss: 36565162909.8082\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35491323591.6230 - val_loss: 36560048184.1096\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35486271693.3265 - val_loss: 36554930933.4795\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35481207020.9152 - val_loss: 36549830066.8493\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35476159750.3616 - val_loss: 36544724038.1370\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35471099326.6290 - val_loss: 36539629343.5616\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35466039962.8723 - val_loss: 36534491164.0548\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35460967370.4747 - val_loss: 36529409935.7808\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35455926868.6752 - val_loss: 36524283819.8356\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35450855652.1405 - val_loss: 36519173919.5616\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35445799019.0506 - val_loss: 36514044829.8082\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 236us/step - loss: 35440737542.3616 - val_loss: 36508967473.0959\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 249us/step - loss: 35435686112.6307 - val_loss: 36503862342.1370\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35430633749.2785 - val_loss: 36498744362.0822\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 239us/step - loss: 35425568773.2648 - val_loss: 36493643776.0000\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 35420499499.4344 - val_loss: 36488551606.3562\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 246us/step - loss: 35415457114.5981 - val_loss: 36483418196.1644\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35410388198.7729 - val_loss: 36478330403.0685\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35405338076.4627 - val_loss: 36473246369.3151\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35400300394.8312 - val_loss: 36468112622.4658\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 258us/step - loss: 35395242656.1371 - val_loss: 36463035882.9589\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35390190262.9512 - val_loss: 36457930471.4521\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35385126452.2091 - val_loss: 36452814848.0000\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35380059610.7078 - val_loss: 36447673021.3699\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35375006833.1928 - val_loss: 36442592690.8493\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 240us/step - loss: 35369961266.6735 - val_loss: 36437487784.3288\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 35364904931.9212 - val_loss: 36432412223.1233\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35359863742.1902 - val_loss: 36427293401.4247\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35354813912.9529 - val_loss: 36422182098.4110\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35349770341.7858 - val_loss: 36417079885.1507\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35344718038.1011 - val_loss: 36412013357.5890\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 247us/step - loss: 35339678250.5570 - val_loss: 36406943126.7945\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35334641802.6392 - val_loss: 36401811736.5479\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35329583965.6692 - val_loss: 36396711150.4658\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35324544636.1611 - val_loss: 36391580265.2055\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35319490112.4936 - val_loss: 36386519516.9315\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35314464320.4936 - val_loss: 36381420501.9178\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 252us/step - loss: 35309412906.5570 - val_loss: 36376320084.1644\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 238us/step - loss: 35304359018.1731 - val_loss: 36371239304.7671\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 241us/step - loss: 35299298892.7781 - val_loss: 36366150108.9315\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35294251901.2579 - val_loss: 36361055414.3562\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 254us/step - loss: 35289203860.2913 - val_loss: 36355958419.2877\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35284155377.0831 - val_loss: 36350857384.3288\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35279121379.0437 - val_loss: 36345714603.8356\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 256us/step - loss: 35274053514.4199 - val_loss: 36340652733.3699\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35268997516.6135 - val_loss: 36335577396.6027\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 250us/step - loss: 35263954041.9674 - val_loss: 36330432371.7260\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35258905032.2811 - val_loss: 36325316074.9589\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35253852137.1859 - val_loss: 36320239111.0137\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 248us/step - loss: 35248827009.4259 - val_loss: 36315141218.1918\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35243785880.2399 - val_loss: 36310123057.0959\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 255us/step - loss: 35238769633.2888 - val_loss: 36305036835.0685\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35233713653.0317 - val_loss: 36299925027.0685\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35228673009.0831 - val_loss: 36294811872.4384\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 257us/step - loss: 35223626945.9194 - val_loss: 36289734010.7397\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35218598988.3393 - val_loss: 36284617657.8630\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 245us/step - loss: 35213552433.7961 - val_loss: 36279574752.4384\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 244us/step - loss: 35208533231.5476 - val_loss: 36274484378.3014\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 253us/step - loss: 35203491476.7301 - val_loss: 36269369877.0411\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 242us/step - loss: 35198437047.8286 - val_loss: 36264309970.4110\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 251us/step - loss: 35193409967.7121 - val_loss: 36259218025.2055\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0005)\n",
    "NN_5000E_Adam_Sig.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = NN_5000E_Adam_Sig.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_15 (Dense)             (None, 512)               169472    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 1,220,609\n",
      "Trainable params: 1,220,609\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_4H = Sequential()\n",
    "NN_5000E_Adam_4H.add(Dense(512,input_dim = 330,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(512,activation = 'relu'))\n",
    "NN_5000E_Adam_4H.add(Dense(1))\n",
    "NN_5000E_Adam_4H.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 1s 564us/step - loss: 36522277088.6307 - val_loss: 31151011924.1644\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 20591323523.4002 - val_loss: 10548069663.5616\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 14787735020.2571 - val_loss: 9263632727.6712\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 12293208674.7147 - val_loss: 7920128925.8082\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 9516187921.4396 - val_loss: 5965733312.8767\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 6733085789.2853 - val_loss: 4627476713.2055\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 4693404259.3728 - val_loss: 4056714520.5479\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 3240308142.8346 - val_loss: 3607858887.8904\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 2409489225.2956 - val_loss: 3407602991.3425\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 2126030431.5338 - val_loss: 3345038679.6712\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 1923661741.2991 - val_loss: 3390656392.7671\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1856447810.9066 - val_loss: 3235776711.8904\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1840494072.1028 - val_loss: 3213287765.9178\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1782692888.1302 - val_loss: 3306326983.8904\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1750170984.5004 - val_loss: 3425477902.0274\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 417us/step - loss: 1759172968.4730 - val_loss: 3297372075.8356\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1739661912.5141 - val_loss: 3182036495.7808\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1700264707.7292 - val_loss: 3237325718.7945\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1689910471.8423 - val_loss: 3233075962.7397\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1677819253.6898 - val_loss: 3279000328.7671\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1672347774.7935 - val_loss: 3259854074.7397\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1654582417.7686 - val_loss: 3256749399.6712\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1636965410.5501 - val_loss: 3173431262.6849\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1677840963.1260 - val_loss: 3256274823.0137\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1653054868.7301 - val_loss: 3350165290.0822\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 1s 455us/step - loss: 1613606679.4722 - val_loss: 3368744814.4658\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1626568677.2922 - val_loss: 3261663114.5205\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1598763332.2228 - val_loss: 3400512554.0822\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1600926150.6358 - val_loss: 3133992418.1918\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1589578693.8680 - val_loss: 3169283257.8630\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 1558678889.7344 - val_loss: 3343467656.7671\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 1577731098.2416 - val_loss: 3109704747.8356\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1572402602.5570 - val_loss: 3108685816.9863\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1556857881.7755 - val_loss: 3194043000.9863\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1531136180.0171 - val_loss: 3077415045.2603\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1559198172.5724 - val_loss: 3153962674.8493\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1611650858.1731 - val_loss: 3321784872.3288\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1597638399.6710 - val_loss: 3112523512.9863\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1533095930.1868 - val_loss: 3112745910.3562\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1507373505.8098 - val_loss: 3048931233.3151\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 1505514252.2296 - val_loss: 3165863196.0548\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1541081999.5750 - val_loss: 3124507476.1644\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1483151867.5030 - val_loss: 3368232248.1096\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1506677220.6341 - val_loss: 3230176112.2192\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1464773561.9949 - val_loss: 3109345416.7671\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1473269284.3599 - val_loss: 3029772579.0685\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1474278194.2896 - val_loss: 3350865646.4658\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1468301756.8740 - val_loss: 3040980655.3425\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1444248563.2219 - val_loss: 3273193668.3836\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 1441196837.0180 - val_loss: 3289186696.7671\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1465705852.3805 - val_loss: 3054998640.2192\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1445793033.4327 - val_loss: 3009686117.6986\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1448398886.1697 - val_loss: 3376718620.0548\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1442741500.4901 - val_loss: 3039573184.8767\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 1419594656.2468 - val_loss: 3103301109.4795\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1412442016.7952 - val_loss: 3072513343.1233\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1392019265.3710 - val_loss: 2961614932.1644\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1446132969.9263 - val_loss: 2986880175.3425\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1397039893.0591 - val_loss: 2973036999.8904\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 1436366527.5613 - val_loss: 3078874971.1781\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1374469245.4773 - val_loss: 2989103146.0822\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1401208453.1551 - val_loss: 3078489358.0274\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1404507123.8252 - val_loss: 2962517556.6027\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1373522788.4696 - val_loss: 3419958615.6712\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 1s 492us/step - loss: 1412739832.3222 - val_loss: 3066478935.6712\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1385106021.9777 - val_loss: 3052150110.6849\n",
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1385407327.5338 - val_loss: 3518912722.4110\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1387324834.6598 - val_loss: 3015115821.5890\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1360344916.7575 - val_loss: 3181765951.1233\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 414us/step - loss: 1356330097.4670 - val_loss: 2938336606.6849\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1418422828.9152 - val_loss: 3055152359.4521\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 408us/step - loss: 1389436894.1354 - val_loss: 3155283982.0274\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 1375963933.8338 - val_loss: 3211531123.7260\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1515055199.0951 - val_loss: 2916702348.2740\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1378849899.9554 - val_loss: 2923911280.2192\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 413us/step - loss: 1347275776.6581 - val_loss: 2924796110.9041\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 1422471533.0797 - val_loss: 2990133830.1370\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1365481410.7969 - val_loss: 2926225425.5342\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1343566834.6187 - val_loss: 3069888519.0137\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1397603578.7901 - val_loss: 2906680456.7671\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1361736407.0883 - val_loss: 3027433657.8630\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1297740856.8706 - val_loss: 2982782909.3699\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1337794199.0883 - val_loss: 2964935013.6986\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1344212505.8852 - val_loss: 2905368421.6986\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1337217342.8483 - val_loss: 3134342459.6164\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1304985361.0009 - val_loss: 3129265088.8767\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1320477071.2459 - val_loss: 2910153934.9041\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 1357981463.9657 - val_loss: 3150098537.2055\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1373008714.5844 - val_loss: 3004727278.4658\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1327203684.3599 - val_loss: 2886679432.7671\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1354050444.8329 - val_loss: 2916040125.3699\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1443282470.4987 - val_loss: 2858416924.0548\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1274284851.5510 - val_loss: 3335619359.5616\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1332225653.0865 - val_loss: 2850390545.5342\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1297488776.7198 - val_loss: 3218708767.5616\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1288654257.4670 - val_loss: 2883222969.8630\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1315380586.3925 - val_loss: 2911319446.7945\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1277727962.2691 - val_loss: 3091833638.5753\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1351574174.6015 - val_loss: 2860897763.9452\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1283803369.4053 - val_loss: 2917407512.5479\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1269597817.5287 - val_loss: 2836882137.4247\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1277439467.7635 - val_loss: 3185674250.5205\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 1328414660.8260 - val_loss: 2819263295.1233\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1267059175.5955 - val_loss: 3268857722.7397\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1286872024.7335 - val_loss: 2884893134.9041\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1269414652.7644 - val_loss: 2836079342.4658\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1277289148.8740 - val_loss: 3226905796.3836\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1337045110.6769 - val_loss: 2912508700.0548\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1257782190.2314 - val_loss: 2805235824.2192\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 411us/step - loss: 1253752236.4216 - val_loss: 3031019754.9589\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1265631103.5613 - val_loss: 2966189385.6438\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1254794863.3282 - val_loss: 3039772307.2877\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 1273395907.7841 - val_loss: 3015003164.0548\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1253965045.6350 - val_loss: 2833160090.3014\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1248249266.6598 - val_loss: 2793115721.6438\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1235316038.3068 - val_loss: 2814190970.7397\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1242431011.5921 - val_loss: 2917721915.6164\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1215784993.4533 - val_loss: 2791357678.4658\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1227567860.0583 - val_loss: 2799876092.4932\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1204748677.7035 - val_loss: 2772784054.3562\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1241430347.6812 - val_loss: 2829291463.8904\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1203923034.9820 - val_loss: 2980978354.8493\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 1227110923.5167 - val_loss: 2726641320.3288\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1224827167.3693 - val_loss: 2758921773.5890\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1216114306.7695 - val_loss: 2849170474.0822\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1208462259.2219 - val_loss: 2765550206.2466\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1178334835.6058 - val_loss: 3064930535.4521\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1242277517.9846 - val_loss: 2776714061.1507\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1168476084.4284 - val_loss: 2790439487.1233\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1180584411.1465 - val_loss: 2937083875.9452\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1196678562.0566 - val_loss: 2905820500.1644\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1228621927.9246 - val_loss: 2713756279.2329\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 410us/step - loss: 1170014581.2511 - val_loss: 2879766373.6986\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1151639312.4524 - val_loss: 2739778556.4932\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1161729214.9032 - val_loss: 2823046936.5479\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1136940189.0660 - val_loss: 2735173772.2740\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1374524349.3402 - val_loss: 2740995797.9178\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1256290006.7592 - val_loss: 2676047721.2055\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1219013163.5441 - val_loss: 3056892068.8219\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1175807710.8209 - val_loss: 2801659167.5616\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1141791732.3188 - val_loss: 2757831231.1233\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 1127327780.1680 - val_loss: 2737458295.2329\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1209805676.1474 - val_loss: 3516409617.5342\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1162274225.1380 - val_loss: 2675946969.4247\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1152418114.4130 - val_loss: 2801360590.9041\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1134924492.1200 - val_loss: 2961354320.6575\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1133271640.2674 - val_loss: 2716638965.4795\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1167033444.5793 - val_loss: 2647884600.1096\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1125002850.0566 - val_loss: 2801871847.4521\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1130881244.7918 - val_loss: 2723554237.3699\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1104355664.7266 - val_loss: 2765979483.1781\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 1s 455us/step - loss: 1124271407.1637 - val_loss: 2821367613.3699\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 1114570797.5733 - val_loss: 3038661454.9041\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1147617141.0317 - val_loss: 2604635970.6301\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1111880157.6692 - val_loss: 3401820396.7123\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1170500358.4713 - val_loss: 2696339080.7671\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 1086342238.9032 - val_loss: 2697793108.1644\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 1108077620.6478 - val_loss: 2832108351.1233\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1104968992.8775 - val_loss: 2569963520.0000\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1103239603.8800 - val_loss: 2687750305.3151\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1120879070.4370 - val_loss: 2821650165.4795\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1129409480.7198 - val_loss: 2820961201.0959\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1197193376.5210 - val_loss: 2704260963.9452\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1163431229.3128 - val_loss: 2805474561.7534\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 418us/step - loss: 1068764773.4019 - val_loss: 2606075844.3836\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1049542469.6761 - val_loss: 2823917047.2329\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 1102754081.4259 - val_loss: 2579630518.3562\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1079847800.1851 - val_loss: 2649085930.9589\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1125941684.7849 - val_loss: 2754391592.3288\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 1065836692.9494 - val_loss: 2620755960.9863\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1030397758.6290 - val_loss: 2701324843.8356\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1029007056.8912 - val_loss: 2569067470.9041\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1057597332.0720 - val_loss: 2654585500.0548\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 1112998012.1611 - val_loss: 2549296867.9452\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1046072091.4207 - val_loss: 2772417765.6986\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1060361687.9109 - val_loss: 2562131704.9863\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1083178665.1585 - val_loss: 2557425727.1233\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1038767162.1868 - val_loss: 2552461753.8630\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 1058269719.6915 - val_loss: 2641547583.1233\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1017577372.6547 - val_loss: 2596697991.0137\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 1033077963.7087 - val_loss: 2538548981.4795\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 1006965434.2965 - val_loss: 2590699842.6301\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 1018358084.7164 - val_loss: 2573801826.1918\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 1003363456.4936 - val_loss: 2688863068.9315\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 412us/step - loss: 1014420952.3496 - val_loss: 2551687518.6849\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 1018725788.8466 - val_loss: 2528942500.8219\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1052294238.1080 - val_loss: 2584050958.0274\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 1031049205.5253 - val_loss: 2567224334.0274\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 1025497806.8620 - val_loss: 2614238016.8767\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 1003398974.9580 - val_loss: 2518541908.1644\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1057644201.5150 - val_loss: 2615751445.0411\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 981876905.2408 - val_loss: 2585163630.4658\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 1s 451us/step - loss: 1028487203.5373 - val_loss: 2502089194.9589\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 1019378153.5698 - val_loss: 2575126538.5205\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 1s 446us/step - loss: 1022293632.3839 - val_loss: 2593028273.0959\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 976082292.1817 - val_loss: 2573779561.2055\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 1023460718.6701 - val_loss: 2541961191.4521\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 980955752.5278 - val_loss: 2521318952.3288\n",
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 965736293.1277 - val_loss: 2500116050.4110\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 970083816.7472 - val_loss: 2444630431.5616\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 1067123853.9023 - val_loss: 2674072440.9863\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 983837678.5878 - val_loss: 2897060513.3151\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1115245671.2665 - val_loss: 2622692625.5342\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 975434417.5767 - val_loss: 2688465379.9452\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 983308236.1200 - val_loss: 2503049694.6849\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 951119468.0925 - val_loss: 2766748196.8219\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 997028387.5373 - val_loss: 2981307234.1918\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 1028715966.8209 - val_loss: 2464263634.4110\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 958872795.0368 - val_loss: 2495568682.0822\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 985832051.2768 - val_loss: 2447847767.6712\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 957202617.5287 - val_loss: 2474049039.7808\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 977453249.3710 - val_loss: 2503193084.4932\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 924376434.8380 - val_loss: 2492283058.8493\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 945514387.5784 - val_loss: 2546149814.3562\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 955767238.1422 - val_loss: 2794607095.2329\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 1s 457us/step - loss: 931387452.5450 - val_loss: 2565829263.7808\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 922580138.2965 - val_loss: 2511601230.9041\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 1012442514.9203 - val_loss: 2570016177.0959\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 920826590.2177 - val_loss: 2515948172.2740\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 910687443.0848 - val_loss: 2527533694.2466\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 946499936.6307 - val_loss: 2582226616.1096\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 944178932.6478 - val_loss: 2501055575.6712\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 948697753.0077 - val_loss: 2560802458.3014\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 900709444.2228 - val_loss: 2679332709.6986\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 911544116.8535 - val_loss: 2415079790.4658\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 908810597.2922 - val_loss: 2644379144.7671\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 906620429.2716 - val_loss: 2421240640.8767\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 894227365.2374 - val_loss: 2449338320.6575\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 865456966.4165 - val_loss: 2567168552.3288\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 918058290.1251 - val_loss: 2475005134.9041\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 893168682.9957 - val_loss: 2458523854.9041\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 959278870.4850 - val_loss: 2403692305.5342\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 890078937.2819 - val_loss: 2483831921.9726\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 898361193.2134 - val_loss: 2510960057.8630\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 1s 452us/step - loss: 970780942.3685 - val_loss: 2408832478.6849\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 943779464.6650 - val_loss: 2549861414.5753\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 984472130.1114 - val_loss: 2493290639.7808\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 994768016.8912 - val_loss: 2533802001.5342\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 878162922.3925 - val_loss: 2454324343.2329\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 877589596.6821 - val_loss: 2380361147.6164\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 911282183.6778 - val_loss: 2781867688.3288\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 961077939.0848 - val_loss: 2482427535.7808\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 901307280.8363 - val_loss: 2374933109.4795\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 839346405.7858 - val_loss: 2508189897.6438\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 846065842.2348 - val_loss: 2416079793.0959\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 876929966.2588 - val_loss: 2405825460.6027\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 881971499.5990 - val_loss: 2461471552.8767\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 1026497828.0308 - val_loss: 2580763316.6027\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 940188732.6547 - val_loss: 2563567966.6849\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 1033726922.0360 - val_loss: 2387850588.9315\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 871534964.1817 - val_loss: 2429442842.3014\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 909385648.9186 - val_loss: 2562697219.5068\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 866050949.8132 - val_loss: 2507730016.4384\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 840345883.2014 - val_loss: 2462088819.7260\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 887160160.9049 - val_loss: 2387740046.0274\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 885163780.9632 - val_loss: 2568999688.7671\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 881363563.9829 - val_loss: 2421478305.3151\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 831474608.9460 - val_loss: 2423309382.1370\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 832505091.6744 - val_loss: 2431839032.1096\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 846284255.7806 - val_loss: 2414436655.3425\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 821298057.5424 - val_loss: 2397715456.0000\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 876104139.5716 - val_loss: 2587999116.2740\n",
      "Epoch 263/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 426us/step - loss: 809903481.3093 - val_loss: 2481664671.5616\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 817842361.8578 - val_loss: 2401059868.0548\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 827466964.1542 - val_loss: 2538248945.9726\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 820705424.0686 - val_loss: 2347676878.9041\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 825997942.4850 - val_loss: 2434173759.1233\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 895748078.3959 - val_loss: 2428477632.8767\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 822585974.5124 - val_loss: 2452980557.1507\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 801043098.4884 - val_loss: 2391332821.9178\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 886114413.7652 - val_loss: 2459997108.6027\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 839023465.8440 - val_loss: 2453516280.9863\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 808831226.3513 - val_loss: 2446906611.7260\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 806534559.4516 - val_loss: 2489390183.4521\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 806042001.1654 - val_loss: 2429343568.6575\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 814260592.5347 - val_loss: 2389523440.2192\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 787634547.3865 - val_loss: 2407609998.0274\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 784523957.5527 - val_loss: 2423837594.3014\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 795087403.8732 - val_loss: 2404417618.4110\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 789063277.5733 - val_loss: 2452517842.4110\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 839851849.6247 - val_loss: 2455173042.8493\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 841785062.7181 - val_loss: 2529570693.2603\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 914266457.2819 - val_loss: 2575414896.2192\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 831771568.6992 - val_loss: 2453789255.8904\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 790015239.2391 - val_loss: 2360814688.4384\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 788207536.3702 - val_loss: 2400364689.5342\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 842039451.2014 - val_loss: 2380899142.1370\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 417us/step - loss: 803941542.1148 - val_loss: 2440020427.3973\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 769553763.8663 - val_loss: 2453919742.2466\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 800665857.0831 - val_loss: 2360788118.7945\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 866723271.1842 - val_loss: 2336134550.7945\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 786412193.4533 - val_loss: 2469033186.1918\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 774387014.5810 - val_loss: 2501145656.1096\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 759279214.9443 - val_loss: 2388313345.7534\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 768833315.2356 - val_loss: 2368530628.3836\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 417us/step - loss: 745396218.1868 - val_loss: 2483869504.8767\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 740539419.3659 - val_loss: 2339098997.4795\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 414us/step - loss: 770241684.8398 - val_loss: 2390654704.2192\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 747615910.5261 - val_loss: 2397503480.9863\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 793495796.1817 - val_loss: 2334234552.1096\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 801730959.5201 - val_loss: 2345727675.6164\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 747542744.1851 - val_loss: 2498170655.5616\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 1s 480us/step - loss: 790751367.3488 - val_loss: 2389159960.5479\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 743665199.4379 - val_loss: 2553739458.6301\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 762377482.5844 - val_loss: 2390248981.0411\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 750283890.0428 - val_loss: 2323665294.0274\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 787193796.1131 - val_loss: 2705346779.1781\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 815940878.4233 - val_loss: 2566856213.0411\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 415us/step - loss: 784603573.2237 - val_loss: 2365077779.2877\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 724795739.8320 - val_loss: 2409498073.4247\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 765429001.7069 - val_loss: 2467798670.0274\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 772722542.5056 - val_loss: 2446749320.7671\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 851150413.6555 - val_loss: 2352762944.8767\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 764232299.9143 - val_loss: 2384771352.5479\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 742819693.8201 - val_loss: 2428034414.4658\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 715300996.8260 - val_loss: 2491498972.9315\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 733766702.9991 - val_loss: 2359009779.7260\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 694370764.8329 - val_loss: 2477721403.6164\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 742151629.6281 - val_loss: 2688429918.6849\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 749009446.8826 - val_loss: 2313789971.2877\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 706460612.0034 - val_loss: 2383975294.2466\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 1s 445us/step - loss: 727507675.3111 - val_loss: 2582766311.4521\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 821800171.7087 - val_loss: 2506657118.6849\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 819613654.5947 - val_loss: 2310187758.4658\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 745200026.4336 - val_loss: 2436599951.7808\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 752391363.0163 - val_loss: 2345429190.1370\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 738840697.9400 - val_loss: 2316779960.1096\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 1s 437us/step - loss: 732201030.4713 - val_loss: 2504809985.7534\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 709335316.8398 - val_loss: 2438000248.9863\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 751711049.6247 - val_loss: 2343722653.8082\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 701395589.7035 - val_loss: 2370005360.2192\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 677976394.5844 - val_loss: 2420657094.1370\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 700541107.2219 - val_loss: 2480075974.1370\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 665851731.3316 - val_loss: 2451428702.6849\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 678998788.0308 - val_loss: 2371011647.1233\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 698614768.1508 - val_loss: 2317950434.1918\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 675414115.5647 - val_loss: 2542269987.0685\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 729897242.7626 - val_loss: 2656059246.4658\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 711171766.2382 - val_loss: 2323855822.9041\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 687465815.4447 - val_loss: 2380172303.7808\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 667565321.4602 - val_loss: 2360235961.8630\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 672494633.0763 - val_loss: 2280649303.6712\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 653599729.9606 - val_loss: 2542033481.6438\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 652296081.4944 - val_loss: 2357716036.3836\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 698488538.9820 - val_loss: 2469881154.6301\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 699070963.6058 - val_loss: 2367242375.0137\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 666605859.0985 - val_loss: 2313763520.8767\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 641228013.9023 - val_loss: 2376246275.5068\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 648157152.9871 - val_loss: 2290733813.4795\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 648029400.2399 - val_loss: 2482713542.1370\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 676783162.9546 - val_loss: 2253589304.1096\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 687731260.4353 - val_loss: 2551187357.8082\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 693525812.6478 - val_loss: 2263384802.1918\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 1s 457us/step - loss: 649249442.8243 - val_loss: 2436736422.5753\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 630722665.3505 - val_loss: 2339113026.6301\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 631198143.6984 - val_loss: 2449198979.5068\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 660726545.2202 - val_loss: 2276557955.5068\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 635180405.9914 - val_loss: 2411087189.9178\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 415us/step - loss: 646835009.1791 - val_loss: 2376843516.4932\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 635925468.1063 - val_loss: 2278746971.1781\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 730552981.5801 - val_loss: 2407490644.1644\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 660039041.1791 - val_loss: 2309187825.9726\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 624311635.0300 - val_loss: 2276247792.2192\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 651175797.7584 - val_loss: 2285672430.4658\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 625971637.5253 - val_loss: 2301243157.0411\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 651141655.4173 - val_loss: 2284701497.8630\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 610031888.0137 - val_loss: 2266322361.8630\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 614614797.6007 - val_loss: 2308266020.8219\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 634982278.9649 - val_loss: 2225863843.0685\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 607783560.9940 - val_loss: 2366102224.6575\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 608315344.6307 - val_loss: 2311168033.3151\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 637371135.3967 - val_loss: 2220983750.1370\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 704354636.6684 - val_loss: 2210336911.7808\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 662216000.7129 - val_loss: 2249227704.1096\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 671174157.4910 - val_loss: 2508978645.9178\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 681831503.8492 - val_loss: 2270667134.2466\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 801104037.2374 - val_loss: 2259342562.1918\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 600770757.0180 - val_loss: 2209300872.7671\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 606772524.9152 - val_loss: 2260240164.8219\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 598738966.7044 - val_loss: 2365470788.3836\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 581708820.7849 - val_loss: 2277087009.3151\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 589641918.3548 - val_loss: 2193358307.9452\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 594558691.2631 - val_loss: 2206415970.1918\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 656423462.5536 - val_loss: 2602849858.6301\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 639719073.6727 - val_loss: 2368870259.7260\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 646062660.7164 - val_loss: 2277231956.1644\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 612187533.1620 - val_loss: 2401172110.0274\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 590911884.9974 - val_loss: 2243274266.3014\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 584990705.1928 - val_loss: 2282490723.9452\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 599397960.3907 - val_loss: 2252540701.8082\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 615689960.3633 - val_loss: 2325972318.6849\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 577663039.8081 - val_loss: 2354439639.6712\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 593966307.3728 - val_loss: 2181883719.8904\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 552265748.2913 - val_loss: 2190654048.4384\n",
      "Epoch 395/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 419us/step - loss: 637857781.4156 - val_loss: 2214793082.7397\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 583126219.6264 - val_loss: 2182806457.8630\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 628089033.9263 - val_loss: 2403850236.4932\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 599288422.6358 - val_loss: 2366905063.4521\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 626103832.1302 - val_loss: 2407291341.1507\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 570548071.0746 - val_loss: 2214153749.0411\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 600729730.2211 - val_loss: 2782370617.8630\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 616668776.5553 - val_loss: 2386182559.5616\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 627390685.6144 - val_loss: 2224731155.2877\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 557893881.9400 - val_loss: 2292105396.6027\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 545144828.6547 - val_loss: 2192441014.3562\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 558283691.4344 - val_loss: 2155195441.0959\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 544811436.0377 - val_loss: 2201508706.1918\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 550698000.5621 - val_loss: 2233732707.9452\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 591090273.2065 - val_loss: 2172412410.7397\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 1s 441us/step - loss: 562197502.7935 - val_loss: 2129002520.5479\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 1s 443us/step - loss: 624774251.9280 - val_loss: 2283998595.5068\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 1s 458us/step - loss: 597099676.4353 - val_loss: 2149478075.6164\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 1s 467us/step - loss: 546849668.2228 - val_loss: 2303420338.8493\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 1s 472us/step - loss: 527472307.1945 - val_loss: 2259343105.7534\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 1s 469us/step - loss: 522604062.8483 - val_loss: 2153799485.3699\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 1s 465us/step - loss: 533793090.4267 - val_loss: 2258412345.8630\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 1s 475us/step - loss: 519035235.7841 - val_loss: 2166524107.3973\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 1s 461us/step - loss: 542402578.3171 - val_loss: 2244855183.7808\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 1s 469us/step - loss: 569013728.3016 - val_loss: 2332655921.0959\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 1s 459us/step - loss: 538103436.0925 - val_loss: 2310023907.9452\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 1s 462us/step - loss: 576505642.6941 - val_loss: 2210883284.1644\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 1s 459us/step - loss: 542632405.9366 - val_loss: 2129097684.1644\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 1s 438us/step - loss: 554214285.9023 - val_loss: 2197056727.6712\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 515045332.5656 - val_loss: 2158640757.4795\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 584308170.8997 - val_loss: 2138831093.4795\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 508733226.8860 - val_loss: 2285831792.2192\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 589167336.1440 - val_loss: 2079473627.1781\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 522222990.7524 - val_loss: 2168033316.8219\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 571651492.4696 - val_loss: 2291274958.9041\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 603015320.4593 - val_loss: 2229901936.2192\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 524343174.9786 - val_loss: 2101377988.3836\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 539728373.8543 - val_loss: 2122909390.9041\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 511874972.2228 - val_loss: 2186072272.6575\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 562759148.3119 - val_loss: 2117983684.3836\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 519693291.8183 - val_loss: 2158459200.8767\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 493881214.0805 - val_loss: 2166377168.6575\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 502078934.1834 - val_loss: 2098739897.8630\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 424us/step - loss: 529026566.9923 - val_loss: 2395801175.6712\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 530601616.3016 - val_loss: 2187134965.4795\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 422us/step - loss: 487350926.5330 - val_loss: 2291397042.8493\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 486936561.5767 - val_loss: 2138136269.1507\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 413us/step - loss: 493159076.7986 - val_loss: 2202595319.2329\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 531578425.1997 - val_loss: 2168301138.4110\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 415us/step - loss: 487667499.3796 - val_loss: 2122696525.1507\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 518301955.1260 - val_loss: 2126776128.8767\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 498038780.9837 - val_loss: 2188954511.7808\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 500780114.0017 - val_loss: 2085714360.1096\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 425us/step - loss: 517685615.8355 - val_loss: 2329906233.8630\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 1s 447us/step - loss: 551063976.7061 - val_loss: 2312218701.1507\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 490981971.3590 - val_loss: 2148451112.3288\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 528969558.9237 - val_loss: 2170177642.9589\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 423us/step - loss: 523604603.7361 - val_loss: 2373947397.2603\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 1s 428us/step - loss: 499428918.2382 - val_loss: 2314304887.2329\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 419us/step - loss: 493898900.0171 - val_loss: 2212574897.0959\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 492465896.5827 - val_loss: 2152268365.1507\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 497642579.1397 - val_loss: 2111886814.6849\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 454758872.5141 - val_loss: 2128725640.7671\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 472131492.2502 - val_loss: 2209085664.4384\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 466152114.4267 - val_loss: 2141905867.3973\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 469449370.2416 - val_loss: 2094584314.7397\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 471798480.6718 - val_loss: 2066637643.3973\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 1s 448us/step - loss: 474862000.7266 - val_loss: 2130776227.0685\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 488551699.7429 - val_loss: 2306299700.6027\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 475858674.9751 - val_loss: 2053234994.8493\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 504993967.1088 - val_loss: 2120908228.3836\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 486189729.8509 - val_loss: 2164837248.0000\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 466126397.9434 - val_loss: 2137085981.8082\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 443284154.0771 - val_loss: 2146301299.7260\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 420us/step - loss: 455077509.4567 - val_loss: 2115810872.1096\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 466901363.0643 - val_loss: 2240322144.4384\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 428us/step - loss: 493800170.4473 - val_loss: 2065612896.4384\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 455589020.4353 - val_loss: 2319207637.9178\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 427us/step - loss: 445601691.5030 - val_loss: 2126078651.6164\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 441519543.4996 - val_loss: 2352376386.6301\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 431745276.0788 - val_loss: 2092260366.0274\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 416us/step - loss: 430217983.4516 - val_loss: 2081458086.5753\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 468114833.5561 - val_loss: 2017135800.1096\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 453311539.0574 - val_loss: 2151033261.5890\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 523021385.1585 - val_loss: 2098038508.7123\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 1s 436us/step - loss: 465422913.1928 - val_loss: 2426776540.9315\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 1s 439us/step - loss: 550481967.5201 - val_loss: 2151403662.0274\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 1s 432us/step - loss: 470275424.1371 - val_loss: 2125608796.9315\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 434303994.8175 - val_loss: 2263325247.1233\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 488932887.2734 - val_loss: 2025614132.6027\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 453335345.7686 - val_loss: 2259035151.7808\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 446069468.8055 - val_loss: 2068731272.7671\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 1s 431us/step - loss: 436329514.4747 - val_loss: 2057267622.5753\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 421us/step - loss: 431849700.5244 - val_loss: 2130841768.3288\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 457425081.9949 - val_loss: 2139316381.8082\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 1s 450us/step - loss: 448494086.4987 - val_loss: 2002187051.8356\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 448693023.7532 - val_loss: 2117843424.4384\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 1s 434us/step - loss: 455686421.2374 - val_loss: 2070145285.2603\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 1s 433us/step - loss: 416661227.8183 - val_loss: 2164533696.8767\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 1s 442us/step - loss: 495962642.2622 - val_loss: 2005678027.3973\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 1s 430us/step - loss: 426483906.4953 - val_loss: 2140407115.3973\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 426us/step - loss: 429844164.6067 - val_loss: 2062513083.6164\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 414us/step - loss: 433833588.0994 - val_loss: 2207829184.8767\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 1s 435us/step - loss: 607598219.4070 - val_loss: 1982930601.2055\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 1s 429us/step - loss: 430218705.5081 - val_loss: 1972770870.3562\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 1s 440us/step - loss: 399483474.8518 - val_loss: 2035059394.6301\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_5000E_Adam_4H.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_5000E_Adam_4H.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_21 (Dense)             (None, 32)                10592     \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 12,737\n",
      "Trainable params: 12,737\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_32 = Sequential()\n",
    "NN_5000E_Adam_32.add(Dense(32,input_dim = 330,activation = 'relu'))\n",
    "NN_5000E_Adam_32.add(Dense(32,activation = 'relu'))\n",
    "NN_5000E_Adam_32.add(Dense(32,activation = 'relu'))\n",
    "NN_5000E_Adam_32.add(Dense(1))\n",
    "NN_5000E_Adam_32.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 38706398633.5698 - val_loss: 39777723770.7397\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 38636421785.1174 - val_loss: 39698240722.4110\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 38542318570.0634 - val_loss: 39589462240.4384\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 38415337147.3385 - val_loss: 39443433948.9315\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 38248806998.4302 - val_loss: 39249987275.3973\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 38010765052.2708 - val_loss: 38963096646.1370\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 37672731139.9486 - val_loss: 38570601149.3699\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 37220937980.7095 - val_loss: 38052890708.1644\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 36598502590.4096 - val_loss: 37321046773.4795\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 35805219818.0634 - val_loss: 36460606842.7397\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 34872381573.3745 - val_loss: 35419379571.7260\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 33742581523.0848 - val_loss: 34171426282.9589\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 32438129647.3282 - val_loss: 32714959409.0959\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 30893797028.5244 - val_loss: 30992828584.3288\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 29121939647.2871 - val_loss: 29113267073.7534\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 27253701934.7249 - val_loss: 27067186218.0822\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 25301668003.2082 - val_loss: 24952807003.1781\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 23338820777.3505 - val_loss: 22845223921.9726\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 21412865454.8346 - val_loss: 20753600960.8767\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 19594083869.3950 - val_loss: 18759517422.4658\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 17935180062.9306 - val_loss: 16933127967.5616\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 16475649507.4824 - val_loss: 15357747775.1233\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 15268909963.2973 - val_loss: 13884133572.3836\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 14237559091.1123 - val_loss: 12753926494.6849\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 13452300094.9580 - val_loss: 11853913466.7397\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 12808024805.4567 - val_loss: 11034024693.4795\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 12319047574.2656 - val_loss: 10391662409.6438\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 11900296191.1225 - val_loss: 9917871608.9863\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 11603727163.0094 - val_loss: 9430912631.2329\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 11313423176.6101 - val_loss: 9087031239.8904\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 11068894077.2579 - val_loss: 8854256345.4247\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 10853735214.2862 - val_loss: 8725884857.8630\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 10660931209.3231 - val_loss: 8466526853.2603\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 10486329650.2348 - val_loss: 8288456963.5068\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 10292422791.5681 - val_loss: 8086953212.4932\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 10128356060.6821 - val_loss: 7959175459.0685\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 9976599358.5193 - val_loss: 7823500351.1233\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 9809923101.8338 - val_loss: 7669834352.2192\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 9703149746.1251 - val_loss: 7552567930.7397\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 9534539733.4430 - val_loss: 7462937049.4247\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 9383754433.9195 - val_loss: 7324561692.0548\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 9245272851.7429 - val_loss: 7246285809.9726\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 9122447823.5201 - val_loss: 7161663526.5753\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 8975761924.6067 - val_loss: 7036752208.6575\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 8848736644.7164 - val_loss: 6965342043.1781\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 8735802418.6735 - val_loss: 6901014952.3288\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 8607325949.3676 - val_loss: 6790326380.7123\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 8477249712.1508 - val_loss: 6724546279.4521\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 8336319042.6872 - val_loss: 6644477012.1644\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 8235009435.5304 - val_loss: 6559246392.1096\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 8093321635.6470 - val_loss: 6502898835.2877\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 7982328565.4704 - val_loss: 6442004297.6438\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 7857201790.7935 - val_loss: 6377018550.3562\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 7735639634.9203 - val_loss: 6300772453.6986\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 7639975582.8209 - val_loss: 6226937982.2466\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 7512915052.8055 - val_loss: 6183156981.4795\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 7406318823.7601 - val_loss: 6127344668.0548\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 7282975436.4490 - val_loss: 6049111706.3014\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 7178417607.1842 - val_loss: 6007947253.4795\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 7063475700.1542 - val_loss: 5945050960.6575\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 6953709088.4662 - val_loss: 5879241931.3973\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 6847420000.0823 - val_loss: 5820818323.2877\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 6753905585.6864 - val_loss: 5772550256.2192\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 6623230864.3428 - val_loss: 5707564978.8493\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 6525066252.7232 - val_loss: 5657811810.1918\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 6425273773.0797 - val_loss: 5607142968.1096\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 6317624499.0026 - val_loss: 5556530887.8904\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 6214892520.7472 - val_loss: 5502116827.1781\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 6116913004.8055 - val_loss: 5452863030.3562\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 6025396548.6615 - val_loss: 5398700131.9452\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 5953267253.3059 - val_loss: 5374689651.7260\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 5810141920.4113 - val_loss: 5317422718.2466\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 5734094331.1740 - val_loss: 5277504990.6849\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5636116918.2931 - val_loss: 5215588639.5616\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 5530374620.9015 - val_loss: 5179111062.7945\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 5449547373.6829 - val_loss: 5137831530.9589\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5341673511.0471 - val_loss: 5091163600.6575\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5265072582.7455 - val_loss: 5050199011.9452\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 5163992398.3136 - val_loss: 5004535776.4384\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 5080508594.6187 - val_loss: 4963202773.9178\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4997120554.1183 - val_loss: 4926502570.0822\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4913954565.2648 - val_loss: 4888727716.8219\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4815495078.9374 - val_loss: 4849002865.9726\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4736895983.3282 - val_loss: 4811981357.5890\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 4650270387.4413 - val_loss: 4771804938.5205\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 4571201740.9974 - val_loss: 4734898526.6849\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 4488653437.4773 - val_loss: 4699172348.4932\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 4418284794.2965 - val_loss: 4663280436.6027\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 4328052583.3213 - val_loss: 4628027493.6986\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 4247456842.1731 - val_loss: 4593235662.9041\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 4164169424.1782 - val_loss: 4560153840.2192\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 4087768613.5116 - val_loss: 4527197715.2877\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 4018893261.1071 - val_loss: 4496667085.1507\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 3943134419.2494 - val_loss: 4462239614.2466\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 3875217084.6547 - val_loss: 4430877392.6575\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 3805339698.2348 - val_loss: 4400140181.0411\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 3734887477.0865 - val_loss: 4373305508.8219\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 3670403773.6418 - val_loss: 4341492283.6164\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 3607399924.5930 - val_loss: 4312472546.1918\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3540711237.5390 - val_loss: 4283318605.1507\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3472353100.2296 - val_loss: 4259193168.6575\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 3411745480.3907 - val_loss: 4232893983.5616\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3361530084.5793 - val_loss: 4200075370.9589\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3285369875.8526 - val_loss: 4180581216.4384\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 3222169827.7018 - val_loss: 4155386408.3288\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 3173382167.3076 - val_loss: 4120383561.6438\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 3103955209.4327 - val_loss: 4099355448.1096\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 3050930032.0960 - val_loss: 4077304104.3288\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2990750478.3685 - val_loss: 4043948764.9315\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2940906004.4010 - val_loss: 4018824591.7808\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 2882607095.2254 - val_loss: 4009177217.7534\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2830177688.1851 - val_loss: 3965381930.0822\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2774880382.2451 - val_loss: 3952398397.3699\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 2730119532.8055 - val_loss: 3926675943.4521\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2679935106.1388 - val_loss: 3909341378.6301\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2640372572.1337 - val_loss: 3888944827.6164\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2588194538.3925 - val_loss: 3874660306.4110\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 2547997006.2039 - val_loss: 3858030118.5753\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2508642872.2674 - val_loss: 3837574517.4795\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2469528089.4464 - val_loss: 3817418906.3014\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 2427632317.7515 - val_loss: 3798146256.6575\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 2396896891.3933 - val_loss: 3799322247.0137\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 2362590387.0026 - val_loss: 3763367294.2466\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 2318120108.4216 - val_loss: 3753669028.8219\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2286852517.0728 - val_loss: 3738668835.0685\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2254323046.0051 - val_loss: 3722019175.4521\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2222012521.0763 - val_loss: 3713085522.4110\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2191895375.5201 - val_loss: 3695300153.8630\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2167147673.6658 - val_loss: 3681623038.2466\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 2139802497.3162 - val_loss: 3664703047.8904\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 2113456239.4379 - val_loss: 3662482817.7534\n",
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 2085059142.3068 - val_loss: 3639073692.0548\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 52us/step - loss: 2062470155.5167 - val_loss: 3629449210.7397\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 2039034682.6804 - val_loss: 3611442688.0000\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 2017470409.5973 - val_loss: 3597317581.1507\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1994395819.6538 - val_loss: 3595217275.6164\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1974552403.7978 - val_loss: 3586392843.3973\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - ETA: 0s - loss: 1971996256.00 - 0s 55us/step - loss: 1957795174.6632 - val_loss: 3578731761.9726\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1939415341.8475 - val_loss: 3568260553.6438\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1922744864.3290 - val_loss: 3552882700.2740\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1905478207.0677 - val_loss: 3553967503.7808\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1890238106.9820 - val_loss: 3538871984.2192\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1877667966.3822 - val_loss: 3536281866.5205\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1864408960.0000 - val_loss: 3516147306.9589\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1850358573.8475 - val_loss: 3512954774.7945\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1837450442.4747 - val_loss: 3505507644.4932\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1829624468.3462 - val_loss: 3491177894.5753\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1815536968.9392 - val_loss: 3500005283.0685\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1807436928.2194 - val_loss: 3495819360.4384\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1795641924.9906 - val_loss: 3479796967.4521\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1787179761.4122 - val_loss: 3471683278.0274\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1780303638.4302 - val_loss: 3469492081.9726\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1771548038.4713 - val_loss: 3463687374.9041\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1764629664.7952 - val_loss: 3461038145.7534\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1757342492.6272 - val_loss: 3456917567.1233\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 1751547446.5124 - val_loss: 3453712243.7260\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1746295093.9640 - val_loss: 3448716894.6849\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1739427787.5716 - val_loss: 3440058282.9589\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1734687408.1508 - val_loss: 3435727543.2329\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1730488431.8766 - val_loss: 3425369769.2055\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1727635935.7532 - val_loss: 3436082506.5205\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1722694381.6829 - val_loss: 3414917388.2740\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1718910985.8166 - val_loss: 3423651845.2603\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1716679538.6735 - val_loss: 3415767932.4932\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1711951244.3942 - val_loss: 3426715008.8767\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1707524151.4996 - val_loss: 3415765361.0959\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1705556316.1337 - val_loss: 3410250505.6438\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1702480068.5518 - val_loss: 3415080218.3014\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1702778834.8106 - val_loss: 3395565839.7808\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1697179953.7961 - val_loss: 3407484970.9589\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1694956684.9426 - val_loss: 3413392680.3288\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1692443871.9177 - val_loss: 3410999443.2877\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1692344402.0428 - val_loss: 3384032043.8356\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1687718659.7292 - val_loss: 3402425109.9178\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1684760729.3368 - val_loss: 3401716743.0137\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 1681913474.6324 - val_loss: 3391241259.8356\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 53us/step - loss: 1681247660.8603 - val_loss: 3390127026.8493\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1679080424.4182 - val_loss: 3398176941.5890\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 53us/step - loss: 1680472277.2648 - val_loss: 3392955113.2055\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1673838683.5853 - val_loss: 3386586562.6301\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1672412963.8663 - val_loss: 3384779902.2466\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1674562137.9949 - val_loss: 3386585903.3425\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 1670759076.8535 - val_loss: 3361329436.0548\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1666544476.0788 - val_loss: 3383822623.5616\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 63us/step - loss: 1666571945.1311 - val_loss: 3396075585.7534\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1664615394.1662 - val_loss: 3384505978.7397\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1666060725.7995 - val_loss: 3386037789.8082\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1661833314.7695 - val_loss: 3399095325.8082\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1659161578.7215 - val_loss: 3369155649.7534\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1659652241.6590 - val_loss: 3366439484.4932\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1655639092.5381 - val_loss: 3383871337.2055\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1655291298.1114 - val_loss: 3378685585.5342\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1653014404.9906 - val_loss: 3394723612.0548\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1651405891.8389 - val_loss: 3376431242.5205\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1650283011.6195 - val_loss: 3382085351.4521\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1647595548.0788 - val_loss: 3367060853.4795\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1645953143.8835 - val_loss: 3363220888.5479\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1646748258.7147 - val_loss: 3364070696.3288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1642946155.7635 - val_loss: 3378011390.2466\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1643436666.3239 - val_loss: 3388817851.6164\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1641126022.1422 - val_loss: 3370086361.4247\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1640486235.0368 - val_loss: 3368569985.7534\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1640567324.9015 - val_loss: 3365025458.8493\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1636792076.9152 - val_loss: 3356467892.6027\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1633960917.0043 - val_loss: 3374617394.8493\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1637340350.7798 - val_loss: 3348085214.6849\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1630970737.5219 - val_loss: 3367064267.3973\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1631393451.5441 - val_loss: 3367508408.1096\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1632107544.2399 - val_loss: 3358085837.1507\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1629815965.2853 - val_loss: 3360494528.8767\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1630682019.4276 - val_loss: 3344180395.8356\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1624866225.4670 - val_loss: 3376202373.2603\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1624640587.6812 - val_loss: 3369888859.1781\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1623777854.1902 - val_loss: 3369012103.0137\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1621197153.0420 - val_loss: 3353191995.6164\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1620273572.5244 - val_loss: 3350942855.0137\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1620673721.1448 - val_loss: 3344478467.5068\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1618500240.5073 - val_loss: 3357727873.7534\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1619188639.9177 - val_loss: 3358749336.5479\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1617198681.3093 - val_loss: 3338672303.3425\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1614507859.2768 - val_loss: 3354863195.1781\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1613196823.1431 - val_loss: 3362721052.0548\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1612524431.9040 - val_loss: 3349652231.0137\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1610517926.9374 - val_loss: 3338402808.9863\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1611139156.2365 - val_loss: 3354705194.0822\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1609206515.0574 - val_loss: 3348781113.8630\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1606634112.7129 - val_loss: 3352287654.5753\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1608319920.2605 - val_loss: 3366012263.4521\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1610905307.0368 - val_loss: 3346298688.8767\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1603822376.4182 - val_loss: 3356487969.3151\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1603342626.7695 - val_loss: 3338179043.9452\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1601124574.5193 - val_loss: 3338147429.6986\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1599499721.4876 - val_loss: 3341876406.3562\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1600386606.7249 - val_loss: 3361123033.4247\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1598714451.1397 - val_loss: 3335292542.2466\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1599877571.1260 - val_loss: 3325158875.1781\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1600471715.9760 - val_loss: 3368318337.7534\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1594547448.1028 - val_loss: 3327266288.2192\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1594258169.6932 - val_loss: 3330957715.2877\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1592677826.7969 - val_loss: 3351511466.0822\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1591808407.5818 - val_loss: 3329061919.5616\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1593877713.9332 - val_loss: 3352627929.4247\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1590893470.5467 - val_loss: 3312453696.8767\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1591728951.3899 - val_loss: 3349289852.4932\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1589881132.4764 - val_loss: 3330775594.0822\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1588658104.3770 - val_loss: 3320039490.6301\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1586497934.1491 - val_loss: 3331378533.6986\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1585370064.3428 - val_loss: 3342122816.8767\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1584601782.9512 - val_loss: 3329291313.0959\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1582765797.2374 - val_loss: 3339787418.3014\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1583465219.6195 - val_loss: 3319001585.9726\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1584579680.6855 - val_loss: 3333327921.0959\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1582124058.7626 - val_loss: 3325263484.4932\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1582069529.6658 - val_loss: 3357233393.9726\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1583155360.7952 - val_loss: 3328286083.5068\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1577047561.6521 - val_loss: 3339482282.0822\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1576756690.4816 - val_loss: 3342498053.2603\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1576185572.5244 - val_loss: 3324439883.3973\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1575592102.7181 - val_loss: 3334397031.4521\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1574631051.4070 - val_loss: 3341170154.9589\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1576035657.3779 - val_loss: 3306831628.2740\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1572128399.4105 - val_loss: 3337726525.3699\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1570996126.9306 - val_loss: 3317617194.0822\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1571672843.1877 - val_loss: 3326764831.5616\n",
      "Epoch 265/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 60us/step - loss: 1570279104.1097 - val_loss: 3324334576.2192\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1568328393.8166 - val_loss: 3333199596.7123\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1569793750.1560 - val_loss: 3337802536.3288\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1566240392.4456 - val_loss: 3336586560.8767\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1568994391.8423 - val_loss: 3313692328.3288\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1567468620.2296 - val_loss: 3312366188.7123\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1564970378.9683 - val_loss: 3312891207.8904\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1566892999.8423 - val_loss: 3317635817.2055\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1561978648.1851 - val_loss: 3338508084.6027\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1564405482.8312 - val_loss: 3323758125.5890\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1565620191.0951 - val_loss: 3338893227.8356\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1561465066.2828 - val_loss: 3313342681.4247\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1568981902.2039 - val_loss: 3330112536.5479\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1560113636.5244 - val_loss: 3294286199.2329\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1558503755.4619 - val_loss: 3318952651.3973\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1556752483.4824 - val_loss: 3303615663.3425\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1559867775.0129 - val_loss: 3317163043.0685\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1555163925.9640 - val_loss: 3310872523.3973\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1554209150.0257 - val_loss: 3302283106.1918\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1554028558.4781 - val_loss: 3298381620.6027\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1554974751.5887 - val_loss: 3307001003.8356\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1557085587.0848 - val_loss: 3302131852.2740\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1552921951.6435 - val_loss: 3312779860.1644\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1553246008.8706 - val_loss: 3301375992.9863\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1549916051.1945 - val_loss: 3301833533.3699\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1550725861.1277 - val_loss: 3305065864.7671\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1548908395.4893 - val_loss: 3305409518.4658\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1547857614.6427 - val_loss: 3300498784.4384\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1548983276.5861 - val_loss: 3314609166.0274\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1547381346.6050 - val_loss: 3294355559.4521\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1546236545.2614 - val_loss: 3310683700.6027\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1545287706.8723 - val_loss: 3297308366.9041\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1544330124.4490 - val_loss: 3294957266.4110\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1545272419.3728 - val_loss: 3303228505.4247\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1546049333.8543 - val_loss: 3294140303.7808\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1542332286.6838 - val_loss: 3306862090.5205\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1543987471.2734 - val_loss: 3297484133.6986\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1542748617.1585 - val_loss: 3326367018.0822\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1542180393.5698 - val_loss: 3288134673.5342\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1539066629.2648 - val_loss: 3276072177.9726\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1542246092.4490 - val_loss: 3305773776.6575\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1539218996.3188 - val_loss: 3279851672.5479\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1537352052.4833 - val_loss: 3308955306.0822\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1537320515.6744 - val_loss: 3291054977.7534\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1539481241.2271 - val_loss: 3305520399.7808\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1536118347.0231 - val_loss: 3288447644.0548\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1534592269.2168 - val_loss: 3284422559.5616\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1537395395.0985 - val_loss: 3270059772.4932\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1531610300.2159 - val_loss: 3312667781.2603\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1533855421.4225 - val_loss: 3305355228.9315\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1535321952.3016 - val_loss: 3298388953.4247\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1538843926.9237 - val_loss: 3288880587.3973\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1532028982.4027 - val_loss: 3284776916.1644\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1531159296.2742 - val_loss: 3282761985.7534\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1531414114.0291 - val_loss: 3274923909.2603\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1529698755.1808 - val_loss: 3292283521.7534\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1529307895.2254 - val_loss: 3278986851.9452\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1529553500.1885 - val_loss: 3279407056.6575\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1529385230.9169 - val_loss: 3287744361.2055\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1529842700.9426 - val_loss: 3311285730.1918\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1525120427.3248 - val_loss: 3271882197.9178\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1526953513.1859 - val_loss: 3286710014.2466\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1524520400.3702 - val_loss: 3291868356.3836\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1524830274.1388 - val_loss: 3299909972.1644\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1525849500.7369 - val_loss: 3275655671.2329\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1522915575.0060 - val_loss: 3283981357.5890\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1521478965.2511 - val_loss: 3273719729.0959\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1525159656.9666 - val_loss: 3279379178.9589\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1523251915.5716 - val_loss: 3289551121.5342\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1520413531.9143 - val_loss: 3284330848.4384\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1521950434.2759 - val_loss: 3275888320.8767\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1519656317.6967 - val_loss: 3271421639.8904\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 54us/step - loss: 1517919038.8483 - val_loss: 3275567707.1781\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1520154150.8278 - val_loss: 3277380253.8082\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1520201249.6727 - val_loss: 3269832893.3699\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1521117311.0129 - val_loss: 3273244414.2466\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1519509877.4704 - val_loss: 3266068923.6164\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1516237902.8895 - val_loss: 3281168811.8356\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1516469935.1637 - val_loss: 3282818002.4110\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1516114923.9280 - val_loss: 3283774890.0822\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1512970700.0651 - val_loss: 3274600013.1507\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1517373470.7112 - val_loss: 3294449583.3425\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1525440964.6615 - val_loss: 3250548818.4110\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1513587383.3899 - val_loss: 3260082093.5890\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1510653332.6204 - val_loss: 3282825591.2329\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1512875770.1868 - val_loss: 3263326285.1507\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 53us/step - loss: 1514542504.8021 - val_loss: 3268497383.4521\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1511196022.0189 - val_loss: 3301923797.9178\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1512069283.8663 - val_loss: 3261537280.0000\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1510187746.2759 - val_loss: 3270397724.0548\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1510388869.7309 - val_loss: 3270987470.9041\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1508622600.9392 - val_loss: 3250580078.4658\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1512035835.7224 - val_loss: 3248653578.5205\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1509030281.7618 - val_loss: 3275158685.8082\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1506152789.1962 - val_loss: 3267227232.4384\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1506064821.1962 - val_loss: 3271577901.5890\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1504672111.4379 - val_loss: 3249588017.0959\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1505568442.0771 - val_loss: 3269657901.5890\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1504521214.1354 - val_loss: 3257113496.5479\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1507002240.2194 - val_loss: 3253819136.0000\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1504684521.4053 - val_loss: 3248412649.2055\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1502914994.3719 - val_loss: 3253278495.5616\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1501579723.3522 - val_loss: 3267931127.2329\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1502474516.6204 - val_loss: 3271842645.9178\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1503408805.9503 - val_loss: 3287094917.2603\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1500035197.1482 - val_loss: 3248584376.1096\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1501230360.4045 - val_loss: 3270496112.2192\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1500860263.1020 - val_loss: 3249280582.1370\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1497934593.0968 - val_loss: 3270298659.0685\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1499776000.3290 - val_loss: 3256270998.7945\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1497232771.8389 - val_loss: 3263570677.4795\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1498850310.2519 - val_loss: 3262840221.8082\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1496281235.4139 - val_loss: 3241033584.2192\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1497488735.3145 - val_loss: 3245896626.8493\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1497479608.9254 - val_loss: 3262576419.0685\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1499584348.5176 - val_loss: 3262936691.7260\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1503448550.2245 - val_loss: 3256721062.5753\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1498932223.4516 - val_loss: 3257401715.7260\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1497135519.4790 - val_loss: 3265295628.2740\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1494110699.8183 - val_loss: 3256117975.6712\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1497377424.5621 - val_loss: 3256655160.1096\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1495445215.1500 - val_loss: 3263472839.8904\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1494065969.7961 - val_loss: 3259768546.1918\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1492116730.6804 - val_loss: 3246462972.4932\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1492950162.5364 - val_loss: 3254962712.5479\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1490286117.2922 - val_loss: 3264291832.9863\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1494607285.1414 - val_loss: 3229488913.5342\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1490344395.7361 - val_loss: 3267381917.8082\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1490028983.4996 - val_loss: 3251606594.6301\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1488713414.8003 - val_loss: 3241056436.6027\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1488494062.5056 - val_loss: 3250394033.0959\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1488652402.5090 - val_loss: 3253074198.7945\n",
      "Epoch 397/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 58us/step - loss: 1486969053.2305 - val_loss: 3231215961.4247\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1485900301.8201 - val_loss: 3240620473.8630\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1490707363.6470 - val_loss: 3247976372.6027\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1488909484.6410 - val_loss: 3274217538.6301\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1495862845.0934 - val_loss: 3234048885.4795\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1485780904.8569 - val_loss: 3238229065.6438\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1487108986.9546 - val_loss: 3245870991.7808\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1490039268.5793 - val_loss: 3216480452.3836\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1480992769.5356 - val_loss: 3258522483.7260\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1484978125.7104 - val_loss: 3243279184.6575\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1486306557.5321 - val_loss: 3250113153.7534\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1482138049.2614 - val_loss: 3218907947.8356\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1482320573.5870 - val_loss: 3224521633.3151\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1479427623.9246 - val_loss: 3247877453.1507\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1485569207.0608 - val_loss: 3228562154.9589\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1485710774.4027 - val_loss: 3276820548.3836\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1481066139.6401 - val_loss: 3236017283.5068\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1479683953.0831 - val_loss: 3231855323.1781\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1478071504.3976 - val_loss: 3243903042.6301\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1482730487.5544 - val_loss: 3248476898.1918\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1478934622.2177 - val_loss: 3222848634.7397\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1479707065.6384 - val_loss: 3217185758.6849\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1479066300.7644 - val_loss: 3234499347.2877\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1479548089.2545 - val_loss: 3219090284.7123\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1475982091.5167 - val_loss: 3255215668.6027\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1475937192.6924 - val_loss: 3239381507.5068\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1474830663.6778 - val_loss: 3232866833.5342\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1475361120.4662 - val_loss: 3245886232.5479\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1481581588.8398 - val_loss: 3225789113.8630\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1475648315.6675 - val_loss: 3253279921.0959\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1474589375.3967 - val_loss: 3225022891.8356\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1473099330.3582 - val_loss: 3230469945.8630\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1475045690.4610 - val_loss: 3217910180.8219\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1473758648.5416 - val_loss: 3231167386.3014\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1471057962.3376 - val_loss: 3233282987.8356\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1473022559.8629 - val_loss: 3220702179.9452\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1471570520.0754 - val_loss: 3220594302.2466\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1469428385.5081 - val_loss: 3233011775.1233\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1472438270.4644 - val_loss: 3222713854.2466\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1470033951.4790 - val_loss: 3241057792.0000\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1470120003.8389 - val_loss: 3212787189.4795\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1474844036.6067 - val_loss: 3229537560.5479\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1472377732.7164 - val_loss: 3205435207.8904\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 1468226268.7918 - val_loss: 3222889137.0959\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1467326716.3805 - val_loss: 3222296444.4932\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1466337237.7172 - val_loss: 3225696613.6986\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1467379493.6213 - val_loss: 3219830079.1233\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1466157290.6118 - val_loss: 3229535917.5890\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1466948386.5501 - val_loss: 3210768203.3973\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1464894467.7292 - val_loss: 3216125906.4110\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1465758578.5090 - val_loss: 3234856998.5753\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1465763920.6170 - val_loss: 3208414521.8630\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1464103865.1448 - val_loss: 3228899152.6575\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1465476036.1131 - val_loss: 3222152968.7671\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1463218183.1294 - val_loss: 3212645158.5753\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1463579299.9760 - val_loss: 3207096763.6164\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1463970871.1705 - val_loss: 3214238935.6712\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1463704969.7618 - val_loss: 3194173436.4932\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1460909807.2185 - val_loss: 3242586289.0959\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1462218206.6564 - val_loss: 3195322410.0822\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1460174671.4105 - val_loss: 3231075243.8356\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1460402533.1277 - val_loss: 3217991138.1918\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1462218801.4670 - val_loss: 3196542492.0548\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1461586358.9512 - val_loss: 3225326230.7945\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 52us/step - loss: 1457585468.8740 - val_loss: 3198147790.9041\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1457424939.3248 - val_loss: 3217194671.3425\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1458521830.3342 - val_loss: 3205390181.6986\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1456691644.4901 - val_loss: 3213111255.6712\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1460065529.7481 - val_loss: 3195573916.0548\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1456073851.9966 - val_loss: 3228109978.3014\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 55us/step - loss: 1456722377.5150 - val_loss: 3195960516.3836\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1456343124.6752 - val_loss: 3195000917.9178\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1455973650.8655 - val_loss: 3208429326.0274\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1455483148.7232 - val_loss: 3213158640.2192\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1457038310.1148 - val_loss: 3220148371.2877\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1454206584.1028 - val_loss: 3198300805.2603\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1454387145.5424 - val_loss: 3200413061.2603\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1452827117.5184 - val_loss: 3202696868.8219\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1454262713.9674 - val_loss: 3208549428.6027\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1454195866.1045 - val_loss: 3188978384.6575\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1454608477.0111 - val_loss: 3186756976.2192\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1451465321.1311 - val_loss: 3222083682.1918\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1453027177.6247 - val_loss: 3206809184.4384\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1450490898.7009 - val_loss: 3207057283.5068\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 56us/step - loss: 1450616565.7446 - val_loss: 3212872234.0822\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1450770115.0163 - val_loss: 3200044149.4795\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1453509328.5621 - val_loss: 3210151893.9178\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 61us/step - loss: 1452355236.9632 - val_loss: 3179999398.5753\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1449791432.3907 - val_loss: 3220240220.9315\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1447590060.8603 - val_loss: 3194245600.4384\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1447647414.8415 - val_loss: 3199469266.4110\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1449048873.8989 - val_loss: 3193018480.2192\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1446570931.8800 - val_loss: 3194738570.5205\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1448366469.0454 - val_loss: 3198471087.3425\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1449191750.9649 - val_loss: 3209340421.2603\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1450744712.4456 - val_loss: 3208845601.3151\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1446352680.2536 - val_loss: 3176652184.5479\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1446298082.1388 - val_loss: 3200123697.0959\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 57us/step - loss: 1444904612.8535 - val_loss: 3206175114.5205\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 58us/step - loss: 1446772943.5201 - val_loss: 3188714450.4110\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1446655241.8715 - val_loss: 3194649866.5205\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 60us/step - loss: 1443726001.1380 - val_loss: 3210666197.9178\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 62us/step - loss: 1443403879.5407 - val_loss: 3194751077.6986\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 59us/step - loss: 1448296398.5330 - val_loss: 3211983658.0822\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_5000E_Adam_32.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_5000E_Adam_32.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_25 (Dense)             (None, 128)               42368     \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 75,521\n",
      "Trainable params: 75,521\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_5000E_Adam_128 = Sequential()\n",
    "NN_5000E_Adam_128.add(Dense(128,input_dim = 330,activation = 'relu'))\n",
    "NN_5000E_Adam_128.add(Dense(128,activation = 'relu'))\n",
    "NN_5000E_Adam_128.add(Dense(128,activation = 'relu'))\n",
    "NN_5000E_Adam_128.add(Dense(1))\n",
    "NN_5000E_Adam_128.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 187us/step - loss: 38690821717.5527 - val_loss: 39492075295.5616\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 38067167105.6452 - val_loss: 38787898438.1370\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 37190807258.9272 - val_loss: 37712415982.4658\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 35843968595.7978 - val_loss: 36016919930.7397\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 33685374161.7138 - val_loss: 33240283023.7808\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 30315134535.5133 - val_loss: 29067547633.9726\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 25726978767.5201 - val_loss: 23658073130.0822\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 20687751560.2262 - val_loss: 17790178893.1507\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 16332400330.2554 - val_loss: 13507428772.8219\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 13912655913.2408 - val_loss: 10726015200.4384\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 12373337261.7378 - val_loss: 9575326229.0411\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 11631635721.4327 - val_loss: 8843370446.9041\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 10945010183.0197 - val_loss: 7988676488.7671\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 10465418717.9983 - val_loss: 7637861512.7671\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 9926815894.7044 - val_loss: 7271128025.4247\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 9455436974.6153 - val_loss: 6929785249.3151\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 9050618438.8552 - val_loss: 6756696190.2466\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 8458948733.6967 - val_loss: 6462015224.9863\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 8060035825.3025 - val_loss: 6213013760.0000\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 7721604976.3153 - val_loss: 6058501624.9863\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 7373885489.5767 - val_loss: 5837274918.5753\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 7046762151.7052 - val_loss: 5713586190.0274\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 6714713529.8029 - val_loss: 5560544222.6849\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 6362998518.3479 - val_loss: 5411602556.4932\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 6114681818.9272 - val_loss: 5261357199.7808\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 5900298889.7618 - val_loss: 5140693509.2603\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 5516017395.4961 - val_loss: 5024565849.4247\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 5262466807.8835 - val_loss: 4911693916.9315\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 5033685816.8158 - val_loss: 4801774953.2055\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 4785897392.1508 - val_loss: 4711544777.6438\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 4565737980.9289 - val_loss: 4624816354.1918\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 4373846530.4130 - val_loss: 4538253781.9178\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 4170533894.2519 - val_loss: 4471793390.4658\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 4027080239.1637 - val_loss: 4381940913.0959\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 3817368406.1011 - val_loss: 4327780955.1781\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 3671174179.9760 - val_loss: 4255320768.8767\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 3509813862.4439 - val_loss: 4198651961.8630\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 3364312493.4087 - val_loss: 4145323935.5616\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 3233605219.0437 - val_loss: 4083228373.9178\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 3113194758.7455 - val_loss: 4038278405.2603\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 3000024144.7266 - val_loss: 3987423161.8630\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2897044877.8201 - val_loss: 3940047659.8356\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 2794538529.1243 - val_loss: 3898660660.6027\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 2693925538.3308 - val_loss: 3870363409.5342\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 2615841646.3410 - val_loss: 3825830321.0959\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 2525107162.5981 - val_loss: 3806016874.9589\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2449423670.2931 - val_loss: 3767683345.5342\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 2376894208.9323 - val_loss: 3727610050.6301\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 2313798758.7729 - val_loss: 3701340980.6027\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2247536923.4207 - val_loss: 3680367987.7260\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2197614061.7926 - val_loss: 3645921213.3699\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2149296620.4764 - val_loss: 3624097872.6575\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 2101034082.4953 - val_loss: 3586967536.2192\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 2060009782.6221 - val_loss: 3564261879.2329\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 2017589945.1448 - val_loss: 3563128867.0685\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1986802820.9906 - val_loss: 3533260000.4384\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1953185707.7635 - val_loss: 3501930897.5342\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1921736022.6495 - val_loss: 3493290848.4384\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1891507887.7395 - val_loss: 3492703905.3151\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1872829801.2408 - val_loss: 3479570442.5205\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1850119845.6213 - val_loss: 3461361009.9726\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1833115860.6752 - val_loss: 3449006435.9452\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1815890317.2716 - val_loss: 3434561678.0274\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1798590653.0934 - val_loss: 3440421863.4521\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1790863390.4919 - val_loss: 3453983845.6986\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1780702719.2871 - val_loss: 3410398958.4658\n",
      "Epoch 67/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 77us/step - loss: 1765073418.6941 - val_loss: 3411879232.8767\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1758582463.3967 - val_loss: 3396047347.7260\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1744228315.3111 - val_loss: 3391115974.1370\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1740766425.8303 - val_loss: 3383109579.3973\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1731204590.8895 - val_loss: 3370837176.1096\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1725354745.0900 - val_loss: 3366179091.2877\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1718979936.6307 - val_loss: 3377862207.1233\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1712693046.8415 - val_loss: 3371860983.2329\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1708490303.5613 - val_loss: 3378068564.1644\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1704787961.7481 - val_loss: 3339882573.1507\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1703973682.1251 - val_loss: 3357356445.8082\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1695298964.4010 - val_loss: 3339705628.0548\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1689236585.5150 - val_loss: 3365038067.7260\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1690931133.9709 - val_loss: 3380680388.3836\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1688811651.5099 - val_loss: 3322751175.8904\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1685057484.7781 - val_loss: 3350533686.3562\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1679914373.8680 - val_loss: 3327800735.5616\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1679541471.7532 - val_loss: 3319176711.0137\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1669090301.0386 - val_loss: 3360757882.7397\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1668658837.2785 - val_loss: 3353300683.3973\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1664280498.7832 - val_loss: 3345946013.8082\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1661486543.7395 - val_loss: 3336027839.1233\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1659604884.7301 - val_loss: 3359719420.4932\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1656503828.5107 - val_loss: 3338508133.6986\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1660222528.0000 - val_loss: 3330688746.9589\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1655388307.1945 - val_loss: 3313850185.6438\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1648768505.3093 - val_loss: 3330929718.3562\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1644764600.8158 - val_loss: 3338596937.6438\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 88us/step - loss: 1646409940.0720 - val_loss: 3339107941.6986\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1643141319.1294 - val_loss: 3353732111.7808\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1636680942.8895 - val_loss: 3323005729.3151\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1635330841.0077 - val_loss: 3323178573.1507\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1634361155.8389 - val_loss: 3359353712.2192\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1631797359.3282 - val_loss: 3318075213.1507\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1629113475.6744 - val_loss: 3319366305.3151\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1627805146.2691 - val_loss: 3334105887.5616\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1629693944.9803 - val_loss: 3318363120.2192\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1626486403.6195 - val_loss: 3317063436.2740\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1618069643.4893 - val_loss: 3336611662.9041\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1619008606.3273 - val_loss: 3334277624.9863\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1617880009.8166 - val_loss: 3302084320.4384\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1610036048.9460 - val_loss: 3332106103.2329\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1617190670.5878 - val_loss: 3321647861.4795\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1608856921.8303 - val_loss: 3330656031.5616\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1607572440.8432 - val_loss: 3327140579.9452\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1607331026.5638 - val_loss: 3349558008.9863\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1610473889.6727 - val_loss: 3352170232.9863\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1606755967.5064 - val_loss: 3293851223.6712\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1602693199.9589 - val_loss: 3327297080.1096\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1597508780.4764 - val_loss: 3291412015.3425\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1595430501.3470 - val_loss: 3300611426.1918\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1596271285.1962 - val_loss: 3320798600.7671\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1604448752.0411 - val_loss: 3320680824.9863\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1590719620.1680 - val_loss: 3327045374.2466\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1587118316.5861 - val_loss: 3304098256.6575\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1587905175.7464 - val_loss: 3322614454.3562\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1590611202.5775 - val_loss: 3312616069.2603\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1599297664.5484 - val_loss: 3298919811.5068\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1585494978.3582 - val_loss: 3293800064.0000\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1579680696.3222 - val_loss: 3337476565.9178\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1579820378.9820 - val_loss: 3313943704.5479\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1575912695.0608 - val_loss: 3332444913.9726\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1571208151.8560 - val_loss: 3292374017.7534\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1570281117.3950 - val_loss: 3293225699.9452\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1567917123.9486 - val_loss: 3292520125.3699\n",
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1566880886.2382 - val_loss: 3258921528.1096\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1570674846.0531 - val_loss: 3255703068.0548\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1563468971.9829 - val_loss: 3270459430.5753\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1562383891.7978 - val_loss: 3266234839.6712\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1558576079.4653 - val_loss: 3267359046.1370\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1557170502.5261 - val_loss: 3324256352.4384\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1558175326.0531 - val_loss: 3227761960.3288\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1556239595.2151 - val_loss: 3343159089.0959\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1555928486.4987 - val_loss: 3265898780.0548\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1549365186.9066 - val_loss: 3272611441.9726\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1550740153.1448 - val_loss: 3262053619.7260\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1545835599.7395 - val_loss: 3290108261.6986\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1555280816.2605 - val_loss: 3227780208.2192\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1546191682.9066 - val_loss: 3286265459.7260\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1543843507.4413 - val_loss: 3246309409.3151\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1546611775.6161 - val_loss: 3236711758.9041\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1543095150.5604 - val_loss: 3265072373.4795\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1544141717.0591 - val_loss: 3252649612.2740\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1549295673.8578 - val_loss: 3208628355.5068\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1537496262.7455 - val_loss: 3269559927.2329\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1538829363.6058 - val_loss: 3275735843.0685\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1534752111.5476 - val_loss: 3240437475.9452\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1528171347.5784 - val_loss: 3299476245.0411\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1527832808.3085 - val_loss: 3271722033.0959\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1526442370.9614 - val_loss: 3302241050.3014\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1521467822.2862 - val_loss: 3237932151.2329\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1525087722.7763 - val_loss: 3292718020.3836\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1521724569.7207 - val_loss: 3232061354.0822\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1524208277.4430 - val_loss: 3227814670.0274\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1521896209.0831 - val_loss: 3244831744.0000\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1515204024.4867 - val_loss: 3241420286.2466\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1514870550.0463 - val_loss: 3225552178.8493\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1518505196.0377 - val_loss: 3297948612.3836\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1519946437.4841 - val_loss: 3275245373.3699\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1511770031.0540 - val_loss: 3264484257.3151\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1508949058.9066 - val_loss: 3265124969.2055\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1508174864.7815 - val_loss: 3237168403.2877\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1506134690.1114 - val_loss: 3192860545.7534\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1509281838.2862 - val_loss: 3215589863.4521\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1502696444.3256 - val_loss: 3241339698.8493\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1518082064.6718 - val_loss: 3235630469.2603\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1505001920.1645 - val_loss: 3217396911.3425\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1502174259.7704 - val_loss: 3263088120.9863\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1498949032.4730 - val_loss: 3163575196.0548\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1500360093.6144 - val_loss: 3210644658.8493\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1499736509.9709 - val_loss: 3279094901.4795\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1489219477.8269 - val_loss: 3204941343.5616\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1492210820.3873 - val_loss: 3206655747.5068\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1494217260.6958 - val_loss: 3257974954.0822\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1487787094.4302 - val_loss: 3234007706.3014\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1485465137.1380 - val_loss: 3218581318.1370\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1483763452.8192 - val_loss: 3198049711.3425\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1481178088.3085 - val_loss: 3204465613.1507\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1482555226.7078 - val_loss: 3261255616.8767\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1482571524.7712 - val_loss: 3194029340.0548\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1478707517.3676 - val_loss: 3224623731.7260\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1480306651.1465 - val_loss: 3186029978.3014\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1478709230.3410 - val_loss: 3159731394.6301\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1479275049.2408 - val_loss: 3250761664.8767\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1482342482.8106 - val_loss: 3267631095.2329\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1471410374.4439 - val_loss: 3162824761.8630\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1471130591.6710 - val_loss: 3203604450.1918\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1470992428.7506 - val_loss: 3156579708.4932\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1465524189.7789 - val_loss: 3270903902.6849\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1479179382.5673 - val_loss: 3171777918.2466\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1465199777.5630 - val_loss: 3167468505.4247\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1468787796.8946 - val_loss: 3259688109.5890\n",
      "Epoch 199/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 76us/step - loss: 1459983632.4524 - val_loss: 3164350236.0548\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1459066925.9023 - val_loss: 3197279603.7260\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1459778316.4490 - val_loss: 3218539085.1507\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1455076517.3470 - val_loss: 3173961978.7397\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1469992295.7601 - val_loss: 3283399466.0822\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1460079372.9426 - val_loss: 3190675250.8493\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1453765550.6427 - val_loss: 3171887473.9726\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1452141877.0865 - val_loss: 3207173442.6301\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 73us/step - loss: 1450308726.2382 - val_loss: 3140537363.2877\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1455467347.7429 - val_loss: 3158695727.3425\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1457097786.8997 - val_loss: 3236488868.8219\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1445272320.2194 - val_loss: 3151867181.5890\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1448726323.8800 - val_loss: 3129861691.6164\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1444694799.0814 - val_loss: 3184161118.6849\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1441820524.3668 - val_loss: 3201284160.8767\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1444807998.5741 - val_loss: 3143746782.6849\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1441310125.6829 - val_loss: 3185047536.2192\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1436894381.1894 - val_loss: 3125299704.9863\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1440056680.4730 - val_loss: 3172982231.6712\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1438581463.6367 - val_loss: 3212501116.4932\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1431788792.8158 - val_loss: 3129181317.2603\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1438056743.3762 - val_loss: 3123997243.6164\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1428388586.1731 - val_loss: 3216173487.3425\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1429461352.5278 - val_loss: 3115027247.3425\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1438753438.6015 - val_loss: 3166862371.0685\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1427598430.6564 - val_loss: 3152806924.2740\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1429185400.9803 - val_loss: 3148019068.4932\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1431519465.1859 - val_loss: 3119732390.5753\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1441836262.3342 - val_loss: 3086973923.9452\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1420599451.0643 - val_loss: 3214283218.4110\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1422123318.1834 - val_loss: 3119011876.8219\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1421013158.7181 - val_loss: 3123354688.8767\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1425210077.2305 - val_loss: 3094355014.1370\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1430701472.5758 - val_loss: 3144101454.9041\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1420990268.7644 - val_loss: 3125365507.5068\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1415673226.4747 - val_loss: 3117257035.3973\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1413499840.6033 - val_loss: 3131148209.0959\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1414287335.1568 - val_loss: 3151103903.5616\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1410445226.8312 - val_loss: 3162833341.3699\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1411216462.7524 - val_loss: 3115704568.9863\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1408967594.5570 - val_loss: 3227565624.1096\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1417664557.2991 - val_loss: 3155346859.8356\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1412325827.9486 - val_loss: 3121459501.5890\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1414032368.9734 - val_loss: 3254407434.5205\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1408156286.3548 - val_loss: 3089405517.1507\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1407466827.2425 - val_loss: 3110096759.2329\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1423163069.5321 - val_loss: 3061301426.8493\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1398782519.2802 - val_loss: 3228339350.7945\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1407763370.0086 - val_loss: 3123664224.4384\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1404778553.9126 - val_loss: 3122049402.7397\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1402358232.0754 - val_loss: 3097738089.2055\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1395852786.7284 - val_loss: 3121076334.4658\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1392871868.3256 - val_loss: 3106225716.6027\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1396617313.1791 - val_loss: 3104958130.8493\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1402070312.3633 - val_loss: 3217251138.6301\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1396243119.6024 - val_loss: 3119514148.8219\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1392088711.2391 - val_loss: 3108879517.8082\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1393486394.5707 - val_loss: 3202783826.4110\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1407678321.7961 - val_loss: 3086451079.0137\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1394161979.2288 - val_loss: 3044552004.3836\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1383245954.0840 - val_loss: 3163632622.4658\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1394473553.8783 - val_loss: 3175485191.0137\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1392112947.6607 - val_loss: 3116972316.0548\n",
      "Epoch 262/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1381544879.6572 - val_loss: 3075198071.2329\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1387125735.4310 - val_loss: 3122764186.3014\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1388404438.6495 - val_loss: 3135612026.7397\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1380272853.8817 - val_loss: 3126829838.0274\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1385263274.1183 - val_loss: 3088221492.6027\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1379664126.6290 - val_loss: 3028957592.5479\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1385406710.7318 - val_loss: 3115687194.3014\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1374478207.6161 - val_loss: 3038248479.5616\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1381103564.4490 - val_loss: 3060688012.2740\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1376695011.8663 - val_loss: 3045779603.2877\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1371959017.6247 - val_loss: 3077756873.6438\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1382485112.2125 - val_loss: 3040809156.3836\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1375180540.3805 - val_loss: 3061841941.0411\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1381865727.2322 - val_loss: 3099671299.5068\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1375198702.1765 - val_loss: 3029189533.8082\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1371808962.6872 - val_loss: 3086926825.2055\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1365922099.9075 - val_loss: 3010399744.0000\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1371698134.3205 - val_loss: 3078232810.9589\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1361941808.5895 - val_loss: 3045185141.4795\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1363065444.6341 - val_loss: 3083774732.2740\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1363375714.9340 - val_loss: 3082717962.5205\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1362878025.3779 - val_loss: 3060259324.4932\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1361047722.5021 - val_loss: 3080823986.8493\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1365214802.8106 - val_loss: 3106728784.6575\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 74us/step - loss: 1362335999.2871 - val_loss: 3081102300.9315\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1357606438.4987 - val_loss: 3061972636.0548\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1359639439.8492 - val_loss: 3090892526.4658\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1353645182.7935 - val_loss: 3084135718.5753\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1354111368.7746 - val_loss: 3073343224.9863\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1354369310.4919 - val_loss: 3113130583.6712\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1354708760.7883 - val_loss: 3015799327.5616\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1362132809.2682 - val_loss: 3126767104.0000\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1357877152.2468 - val_loss: 3057918451.7260\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1352642612.2091 - val_loss: 3166298865.9726\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1360120600.2399 - val_loss: 3050691349.0411\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1346302341.7035 - val_loss: 3017302001.9726\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1341032707.5167 - val_loss: 3101190776.9863\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1343770026.8312 - val_loss: 3016246910.2466\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1345521843.2219 - val_loss: 3070906932.6027\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1345454852.4422 - val_loss: 3087084437.0411\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1347558890.1731 - val_loss: 3026717827.5068\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1336329178.9272 - val_loss: 3026385308.0548\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1333413089.1243 - val_loss: 3061588067.9452\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1334000292.6889 - val_loss: 3001451218.4110\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1343190219.7361 - val_loss: 3088089372.0548\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1337866588.3530 - val_loss: 3053700671.1233\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1338517866.2828 - val_loss: 3043050360.9863\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1340747394.4953 - val_loss: 2988757214.6849\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1330004558.4233 - val_loss: 3081226380.2740\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1332904437.0865 - val_loss: 3025844413.3699\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1328083447.8835 - val_loss: 3135792119.2329\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1337228534.7318 - val_loss: 3028174865.5342\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1330163055.6572 - val_loss: 2984515168.4384\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1349279007.9177 - val_loss: 3102535371.3973\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1341520307.9897 - val_loss: 3098223803.6164\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1325636692.8946 - val_loss: 3060485095.4521\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1325307057.6864 - val_loss: 3037452056.5479\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1324753657.4739 - val_loss: 2951958235.1781\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1316794146.3582 - val_loss: 3109231149.5890\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1324125157.4567 - val_loss: 2990095093.4795\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1332797153.5630 - val_loss: 2964578309.2603\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1320622854.6358 - val_loss: 3006426827.3973\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1328548622.5330 - val_loss: 3057665690.3014\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1322097556.8946 - val_loss: 2994948404.6027\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1316253060.9357 - val_loss: 3069805233.0959\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1321836734.7386 - val_loss: 3024580665.8630\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1310083821.0249 - val_loss: 2984159621.2603\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1312768183.1979 - val_loss: 2956205994.0822\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1316037687.3350 - val_loss: 3075665685.0411\n",
      "Epoch 331/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 75us/step - loss: 1318095657.1585 - val_loss: 3072681147.6164\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1310741673.5698 - val_loss: 2991538752.8767\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1304317506.9066 - val_loss: 3077951677.3699\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1308258751.0677 - val_loss: 3022073233.5342\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1305852619.5716 - val_loss: 2977221335.6712\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1301489061.0180 - val_loss: 3007367178.5205\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1307269222.9923 - val_loss: 2974396563.2877\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 83us/step - loss: 1311471675.5578 - val_loss: 2942342524.4932\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1313529684.9494 - val_loss: 2958171923.2877\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1316290481.0283 - val_loss: 3017989852.9315\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1302944480.9597 - val_loss: 2998342955.8356\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1306564665.9949 - val_loss: 2908773049.8630\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1290980283.5578 - val_loss: 3123779135.1233\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1308742390.5124 - val_loss: 2935492285.3699\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1297373274.5433 - val_loss: 3005975734.3562\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1292808827.4482 - val_loss: 2978460682.5205\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1293763687.2117 - val_loss: 2950951525.6986\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1295543918.9991 - val_loss: 2993230232.5479\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1291041840.2605 - val_loss: 2999319848.3288\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1296490945.7001 - val_loss: 3029567340.7123\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1291783669.4704 - val_loss: 3000805719.6712\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1297742858.6392 - val_loss: 2913414792.7671\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1291499661.4910 - val_loss: 2970454889.2055\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1284413575.5681 - val_loss: 3019385172.1644\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1282621840.0137 - val_loss: 2978971733.9178\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1286484404.8946 - val_loss: 2917488555.8356\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1288201781.7446 - val_loss: 3014019899.6164\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1280858378.0908 - val_loss: 2941398133.4795\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1283020353.5904 - val_loss: 2960164555.3973\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1281955410.4267 - val_loss: 2927155918.9041\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1285965594.1594 - val_loss: 2958851633.0959\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1278120962.8518 - val_loss: 2974425696.4384\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1278688157.1757 - val_loss: 3002447468.7123\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1282968973.2716 - val_loss: 2908233389.5890\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1270675301.3470 - val_loss: 3042337748.1644\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1275843168.0823 - val_loss: 2916194828.2740\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1275324840.4319 - val_loss: 2984403573.4795\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1277021424.4250 - val_loss: 2966258325.0411\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1272731812.1954 - val_loss: 2982504847.7808\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1278813451.0780 - val_loss: 3013950954.9589\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1274804810.5844 - val_loss: 2907470065.9726\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1291251532.9974 - val_loss: 2910986667.8356\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1267836301.7104 - val_loss: 2943999021.5890\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1265071365.7584 - val_loss: 2990992748.7123\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1265105959.4310 - val_loss: 2928455160.9863\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1266203766.3479 - val_loss: 3033253621.4795\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1265334329.6110 - val_loss: 2946811788.2740\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1264504616.7472 - val_loss: 2947762929.9726\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1263734859.0231 - val_loss: 3080744739.0685\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1276775422.0531 - val_loss: 2946494548.1644\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1258152897.3710 - val_loss: 2991523173.6986\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1259881912.5690 - val_loss: 2951391042.6301\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1259461874.3445 - val_loss: 2968980723.7260\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1256595616.4662 - val_loss: 2917681232.6575\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1258324576.4662 - val_loss: 2987782436.8219\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1260789038.5604 - val_loss: 3012164927.1233\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1258888362.7763 - val_loss: 2899332646.5753\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1247820202.4473 - val_loss: 2984323282.4110\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1254597403.7498 - val_loss: 2971140271.3425\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1266001405.7515 - val_loss: 2992339782.1370\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1253539477.9914 - val_loss: 3001430494.6849\n",
      "Epoch 392/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1253713981.4225 - val_loss: 2976922604.7123\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1247123562.8312 - val_loss: 2948899569.9726\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1252074047.9452 - val_loss: 2891783206.5753\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1256808549.8406 - val_loss: 2901894648.9863\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1251228532.2639 - val_loss: 2918974471.0137\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1265526109.1757 - val_loss: 2994700487.8904\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1249477696.9323 - val_loss: 2923473774.4658\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1263228350.1902 - val_loss: 2841196572.0548\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1253857367.0060 - val_loss: 3006037844.1644\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1257952915.9623 - val_loss: 2984407609.8630\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1238504395.4619 - val_loss: 2906402198.7945\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1241806175.0951 - val_loss: 2927156844.7123\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1243983053.4087 - val_loss: 2927357115.6164\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1237128931.4824 - val_loss: 2926511282.8493\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1233665872.1234 - val_loss: 2997228852.6027\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1238511880.0891 - val_loss: 2904200455.0137\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1235477104.9734 - val_loss: 2920015459.9452\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1234618984.7746 - val_loss: 2934325523.2877\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1240989484.4216 - val_loss: 3020602385.5342\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1236627842.3033 - val_loss: 2869569036.2740\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1226983362.6324 - val_loss: 3001148559.7808\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1224360613.1551 - val_loss: 2843035321.8630\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1249337101.1620 - val_loss: 2879421618.8493\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1232299887.9314 - val_loss: 2968840810.9589\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1232177238.3205 - val_loss: 3011360599.6712\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1232062966.0189 - val_loss: 2900132120.5479\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1226646518.8963 - val_loss: 2925581280.4384\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1227932505.0626 - val_loss: 2856132620.2740\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1231789232.5895 - val_loss: 2927484873.6438\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1219139068.9563 - val_loss: 2877281948.0548\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1224683678.2725 - val_loss: 2996278755.9452\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1229758011.5578 - val_loss: 2897336609.3151\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1214978020.7986 - val_loss: 2977064635.6164\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1223068926.4507 - val_loss: 2916446541.1507\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1221659319.0608 - val_loss: 2912431905.3151\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1217538875.2562 - val_loss: 2869864540.9315\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1217142348.9152 - val_loss: 2930624545.3151\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1222312499.9897 - val_loss: 2867450592.4384\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1209137791.8903 - val_loss: 2937896367.3425\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1215404177.8235 - val_loss: 2858437553.0959\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1218568101.6761 - val_loss: 2907695489.7534\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1216943899.8595 - val_loss: 2931543352.1096\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1219653819.9966 - val_loss: 2864201936.6575\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1224183054.5330 - val_loss: 2963660305.5342\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1209737774.9169 - val_loss: 2867739323.6164\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1219377380.8809 - val_loss: 2924813738.0822\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1221337511.5407 - val_loss: 2903357704.7671\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1204105478.4713 - val_loss: 2853729185.3151\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1218430043.1465 - val_loss: 2856052581.6986\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1211704926.3273 - val_loss: 2870568556.7123\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 82us/step - loss: 1209801040.0411 - val_loss: 2869554891.3973\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1203635385.9126 - val_loss: 2876667469.1507\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1199343002.5433 - val_loss: 2954341232.2192\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1216482433.9743 - val_loss: 2825529194.9589\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1199646071.3350 - val_loss: 2913399094.3562\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1210088481.2888 - val_loss: 2925798184.3288\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1199240831.1774 - val_loss: 2845141917.8082\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1199894547.7155 - val_loss: 2878760011.3973\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1195495279.7121 - val_loss: 2953202502.1370\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1200559536.6992 - val_loss: 2889595114.9589\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1197830189.2442 - val_loss: 2844562000.6575\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1189756686.9169 - val_loss: 2963656702.2466\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1199854526.4096 - val_loss: 2889451176.3288\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1191452364.0103 - val_loss: 2902736387.5068\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1190887908.1954 - val_loss: 2897406898.8493\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1196029999.8766 - val_loss: 2829632066.6301\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1197924567.9657 - val_loss: 2974215927.2329\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1198655793.3573 - val_loss: 2865318622.6849\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1185361183.9314 - val_loss: 2905817503.5616\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1193657399.2528 - val_loss: 2888334469.2603\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1184990142.3548 - val_loss: 2863090505.6438\n",
      "Epoch 463/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 78us/step - loss: 1188486239.4790 - val_loss: 2842957252.3836\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1188595814.2245 - val_loss: 2892383512.5479\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1190590372.5244 - val_loss: 2827251631.3425\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1198473161.7069 - val_loss: 2877694178.1918\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1182912518.0326 - val_loss: 2909647379.2877\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1181008338.1525 - val_loss: 2894570378.5205\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1180401603.0163 - val_loss: 2903558436.8219\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1187769878.9237 - val_loss: 2938208680.3288\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1170512352.6855 - val_loss: 2805979788.2740\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1187661252.0583 - val_loss: 2866597125.2603\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1173947475.3042 - val_loss: 2886037191.8904\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1167690929.0283 - val_loss: 2787052300.2740\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1190646753.1243 - val_loss: 2852057607.0137\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1175442687.8081 - val_loss: 2914026687.1233\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1173763821.4636 - val_loss: 2841390635.8356\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1172965924.4970 - val_loss: 2829201697.3151\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1171807578.5159 - val_loss: 2849873688.5479\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1168474988.3668 - val_loss: 2839483623.4521\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1177092422.4987 - val_loss: 2769246840.9863\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1206495235.0163 - val_loss: 2831547923.2877\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1170958137.0900 - val_loss: 2852846818.1918\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1170009483.6264 - val_loss: 2905960951.2329\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1168014905.3368 - val_loss: 2824092203.8356\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1160493215.6984 - val_loss: 2959308070.5753\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1175736961.6452 - val_loss: 2825483765.4795\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1178672316.4901 - val_loss: 2936679169.7534\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1169963932.7095 - val_loss: 2838038087.8904\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 76us/step - loss: 1166746205.2853 - val_loss: 2896109929.2055\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1162822181.2922 - val_loss: 2814668561.5342\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 79us/step - loss: 1156284524.8877 - val_loss: 2876832206.9041\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 80us/step - loss: 1179239352.5416 - val_loss: 2936843397.2603\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 75us/step - loss: 1160361078.4850 - val_loss: 2809424945.0959\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 81us/step - loss: 1153918058.1183 - val_loss: 2836176119.2329\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1157276527.2734 - val_loss: 2823545093.2603\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1159490228.2091 - val_loss: 2866422954.0822\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 78us/step - loss: 1159725948.1611 - val_loss: 2816017116.9315\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1158251411.9623 - val_loss: 2893251059.7260\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 77us/step - loss: 1163715978.4199 - val_loss: 2830939967.1233\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_5000E_Adam_128.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_5000E_Adam_128.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_37 (Dense)             (None, 512)               169472    \n",
      "_________________________________________________________________\n",
      "dense_38 (Dense)             (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "dense_39 (Dense)             (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 432,641\n",
      "Trainable params: 432,641\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_500E_1H = Sequential()\n",
    "NN_500E_1H.add(Dense(512,input_dim = 330,activation = 'relu'))\n",
    "NN_500E_1H.add(Dense(512,activation = 'relu'))\n",
    "NN_500E_1H.add(Dense(1))\n",
    "NN_500E_1H.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1167 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1167/1167 [==============================] - 0s 274us/step - loss: 37315028674.3582 - val_loss: 37191868359.8904\n",
      "Epoch 2/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 34670875241.7344 - val_loss: 34077835656.7671\n",
      "Epoch 3/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 30991707826.5638 - val_loss: 29510343750.1370\n",
      "Epoch 4/500\n",
      "1167/1167 [==============================] - 0s 181us/step - loss: 26036271552.3839 - val_loss: 23620228306.4110\n",
      "Epoch 5/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 20506454550.3753 - val_loss: 17563180817.5342\n",
      "Epoch 6/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 16197170727.9246 - val_loss: 13097517168.2192\n",
      "Epoch 7/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 13490555313.4670 - val_loss: 10542575125.0411\n",
      "Epoch 8/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 11950503666.6187 - val_loss: 9014504882.8493\n",
      "Epoch 9/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 10914384783.6847 - val_loss: 7763919468.7123\n",
      "Epoch 10/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 10067454718.4644 - val_loss: 7220150450.8493\n",
      "Epoch 11/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 9136653946.1320 - val_loss: 6749834225.9726\n",
      "Epoch 12/500\n",
      "1167/1167 [==============================] - 0s 182us/step - loss: 8366443936.5758 - val_loss: 6327585886.6849\n",
      "Epoch 13/500\n",
      "1167/1167 [==============================] - 0s 180us/step - loss: 7717556697.8303 - val_loss: 6000714986.9589\n",
      "Epoch 14/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 7186532844.9152 - val_loss: 5706583201.3151\n",
      "Epoch 15/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 6515859842.7421 - val_loss: 5414860233.6438\n",
      "Epoch 16/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 5978292068.9083 - val_loss: 5178900264.3288\n",
      "Epoch 17/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 5542993590.0737 - val_loss: 4983877091.9452\n",
      "Epoch 18/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 5032659067.4207 - val_loss: 4799289342.2466\n",
      "Epoch 19/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 4665423896.8980 - val_loss: 4645329190.5753\n",
      "Epoch 20/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 4353698071.9109 - val_loss: 4495737047.6712\n",
      "Epoch 21/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 4054433321.0214 - val_loss: 4392187577.8630\n",
      "Epoch 22/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 3749640709.5938 - val_loss: 4272741889.7534\n",
      "Epoch 23/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 3536665930.6941 - val_loss: 4174349447.0137\n",
      "Epoch 24/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 3297821516.3393 - val_loss: 4096969671.8904\n",
      "Epoch 25/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 3110048505.9674 - val_loss: 4052673525.4795\n",
      "Epoch 26/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 2935378196.4010 - val_loss: 3961064462.0274\n",
      "Epoch 27/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 2779042569.0488 - val_loss: 3905559702.7945\n",
      "Epoch 28/500\n",
      "1167/1167 [==============================] - 0s 180us/step - loss: 2652014658.9066 - val_loss: 3842299432.3288\n",
      "Epoch 29/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 2531300596.7027 - val_loss: 3798827141.2603\n",
      "Epoch 30/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 2430988893.7241 - val_loss: 3745671564.2740\n",
      "Epoch 31/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 2338622697.1859 - val_loss: 3720047421.3699\n",
      "Epoch 32/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 2260526976.9871 - val_loss: 3665621954.6301\n",
      "Epoch 33/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 2187125384.8843 - val_loss: 3631143564.2740\n",
      "Epoch 34/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 2132022882.8243 - val_loss: 3617098813.3699\n",
      "Epoch 35/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 2076032572.4353 - val_loss: 3576360938.9589\n",
      "Epoch 36/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 2024760738.1114 - val_loss: 3555087389.8082\n",
      "Epoch 37/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1986936308.3736 - val_loss: 3552991342.4658\n",
      "Epoch 38/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1949047595.7635 - val_loss: 3503962915.0685\n",
      "Epoch 39/500\n",
      "1167/1167 [==============================] - 0s 182us/step - loss: 1917269984.5210 - val_loss: 3509827701.4795\n",
      "Epoch 40/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1887271394.7147 - val_loss: 3503179740.9315\n",
      "Epoch 41/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1862797180.8192 - val_loss: 3470618148.8219\n",
      "Epoch 42/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1838407799.1157 - val_loss: 3456655587.9452\n",
      "Epoch 43/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1821753861.9229 - val_loss: 3451772394.9589\n",
      "Epoch 44/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1806544170.7763 - val_loss: 3431800449.7534\n",
      "Epoch 45/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1787762029.2442 - val_loss: 3431017857.7534\n",
      "Epoch 46/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1772413779.3042 - val_loss: 3415600562.8493\n",
      "Epoch 47/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1758605655.5270 - val_loss: 3421956329.2055\n",
      "Epoch 48/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1754524641.2888 - val_loss: 3403857846.3562\n",
      "Epoch 49/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1743829941.3059 - val_loss: 3417888590.9041\n",
      "Epoch 50/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1733097519.4927 - val_loss: 3393846142.2466\n",
      "Epoch 51/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1722630336.8226 - val_loss: 3380560606.6849\n",
      "Epoch 52/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1714408404.2365 - val_loss: 3376155770.7397\n",
      "Epoch 53/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1709978285.9023 - val_loss: 3375490689.7534\n",
      "Epoch 54/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1704343698.6461 - val_loss: 3366487829.0411\n",
      "Epoch 55/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1702087523.7018 - val_loss: 3356079570.4110\n",
      "Epoch 56/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1691812845.7378 - val_loss: 3360359315.2877\n",
      "Epoch 57/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1685336782.4781 - val_loss: 3375123894.3562\n",
      "Epoch 58/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1686588384.9049 - val_loss: 3359355602.4110\n",
      "Epoch 59/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1681345133.0249 - val_loss: 3333076779.8356\n",
      "Epoch 60/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1675188586.1731 - val_loss: 3370475821.5890\n",
      "Epoch 61/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1667915564.4764 - val_loss: 3356171025.5342\n",
      "Epoch 62/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1667027711.8903 - val_loss: 3388621645.1507\n",
      "Epoch 63/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1657937805.7104 - val_loss: 3342111945.6438\n",
      "Epoch 64/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1663353738.2005 - val_loss: 3348182754.1918\n",
      "Epoch 65/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1653331115.1054 - val_loss: 3366345747.2877\n",
      "Epoch 66/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1655902098.5364 - val_loss: 3379900068.8219\n",
      "Epoch 67/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1648449816.3496 - val_loss: 3355527729.0959\n",
      "Epoch 68/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1643874144.0823 - val_loss: 3337687820.2740\n",
      "Epoch 69/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1640827852.7781 - val_loss: 3353097636.8219\n",
      "Epoch 70/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1639270160.3976 - val_loss: 3337490688.0000\n",
      "Epoch 71/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1634624033.5630 - val_loss: 3343139952.2192\n",
      "Epoch 72/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1631566120.5827 - val_loss: 3341120948.6027\n",
      "Epoch 73/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1635436093.4225 - val_loss: 3305758039.6712\n",
      "Epoch 74/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1622641392.2057 - val_loss: 3348590930.4110\n",
      "Epoch 75/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1625013358.5604 - val_loss: 3349232969.6438\n",
      "Epoch 76/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1622145831.3762 - val_loss: 3320904789.9178\n",
      "Epoch 77/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1617142902.3479 - val_loss: 3345644933.2603\n",
      "Epoch 78/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1614403354.1045 - val_loss: 3372705511.4521\n",
      "Epoch 79/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1612069741.7378 - val_loss: 3293882510.0274\n",
      "Epoch 80/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1608827022.4781 - val_loss: 3325956581.6986\n",
      "Epoch 81/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1607301779.5236 - val_loss: 3352520817.9726\n",
      "Epoch 82/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1604683141.8132 - val_loss: 3338757952.8767\n",
      "Epoch 83/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1604178378.1183 - val_loss: 3336159556.3836\n",
      "Epoch 84/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1599913849.7481 - val_loss: 3297213664.4384\n",
      "Epoch 85/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1594738597.1825 - val_loss: 3366801141.4795\n",
      "Epoch 86/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1591738433.1517 - val_loss: 3329352926.6849\n",
      "Epoch 87/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1591458210.8792 - val_loss: 3318909029.6986\n",
      "Epoch 88/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1590982046.3822 - val_loss: 3308497811.2877\n",
      "Epoch 89/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1588919537.9057 - val_loss: 3323234640.6575\n",
      "Epoch 90/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1589014372.7301 - val_loss: 3307856873.2055\n",
      "Epoch 91/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1581768065.7549 - val_loss: 3309634752.8767\n",
      "Epoch 92/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1584813957.7035 - val_loss: 3298508785.9726\n",
      "Epoch 93/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1579001397.8543 - val_loss: 3272298894.0274\n",
      "Epoch 94/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1572653457.8783 - val_loss: 3333281609.6438\n",
      "Epoch 95/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1574142408.7198 - val_loss: 3285592177.9726\n",
      "Epoch 96/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1576762191.9589 - val_loss: 3314581454.9041\n",
      "Epoch 97/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1572271445.0591 - val_loss: 3342638886.5753\n",
      "Epoch 98/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1564947625.2956 - val_loss: 3285431837.8082\n",
      "Epoch 99/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1565575662.2314 - val_loss: 3279120340.1644\n",
      "Epoch 100/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1560303332.5793 - val_loss: 3278397857.3151\n",
      "Epoch 101/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1567668225.2065 - val_loss: 3365976786.4110\n",
      "Epoch 102/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1557275402.5296 - val_loss: 3265063264.4384\n",
      "Epoch 103/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1558877992.6924 - val_loss: 3354253952.0000\n",
      "Epoch 104/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1552700832.3565 - val_loss: 3266758706.8493\n",
      "Epoch 105/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1553089813.0591 - val_loss: 3320114624.8767\n",
      "Epoch 106/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1546484455.9794 - val_loss: 3285040417.3151\n",
      "Epoch 107/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1546580004.8535 - val_loss: 3326570366.2466\n",
      "Epoch 108/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1544759771.4207 - val_loss: 3273808839.8904\n",
      "Epoch 109/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1549289305.0626 - val_loss: 3277493314.6301\n",
      "Epoch 110/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1539036359.8423 - val_loss: 3260876477.3699\n",
      "Epoch 111/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1542860901.2374 - val_loss: 3282846078.2466\n",
      "Epoch 112/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1534709017.4464 - val_loss: 3269854865.5342\n",
      "Epoch 113/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1534751264.8500 - val_loss: 3296201457.9726\n",
      "Epoch 114/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1528887551.3419 - val_loss: 3261128533.9178\n",
      "Epoch 115/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1528385593.4739 - val_loss: 3274377991.0137\n",
      "Epoch 116/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1533527148.8055 - val_loss: 3271168042.0822\n",
      "Epoch 117/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1522830240.1919 - val_loss: 3245230220.2740\n",
      "Epoch 118/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1523892837.2374 - val_loss: 3237864956.4932\n",
      "Epoch 119/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1522261966.0394 - val_loss: 3284612741.2603\n",
      "Epoch 120/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1522785384.0891 - val_loss: 3262917102.4658\n",
      "Epoch 121/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1519165144.1165 - val_loss: 3298911586.1918\n",
      "Epoch 122/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1529299856.2879 - val_loss: 3278141496.1096\n",
      "Epoch 123/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1522754574.2588 - val_loss: 3263848749.5890\n",
      "Epoch 124/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1513285360.0960 - val_loss: 3278229623.2329\n",
      "Epoch 125/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1514650125.2716 - val_loss: 3201067363.9452\n",
      "Epoch 126/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1511519014.7181 - val_loss: 3299334919.0137\n",
      "Epoch 127/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1509639659.4619 - val_loss: 3265399362.6301\n",
      "Epoch 128/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1504056791.2528 - val_loss: 3230316549.2603\n",
      "Epoch 129/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1500614732.2296 - val_loss: 3269410938.7397\n",
      "Epoch 130/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1498183694.9169 - val_loss: 3235858393.4247\n",
      "Epoch 131/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1500985989.1003 - val_loss: 3227157684.6027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 132/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1499194769.6590 - val_loss: 3228848405.0411\n",
      "Epoch 133/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1494834233.9126 - val_loss: 3203703075.0685\n",
      "Epoch 134/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1493585565.2853 - val_loss: 3217558794.5205\n",
      "Epoch 135/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1496104212.6204 - val_loss: 3188617789.3699\n",
      "Epoch 136/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1494191274.8312 - val_loss: 3298419843.5068\n",
      "Epoch 137/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1487912581.0454 - val_loss: 3203506900.1644\n",
      "Epoch 138/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1495450916.8535 - val_loss: 3238017388.7123\n",
      "Epoch 139/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1487215100.0514 - val_loss: 3229990629.6986\n",
      "Epoch 140/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1484912450.1937 - val_loss: 3185915483.1781\n",
      "Epoch 141/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1481396838.9374 - val_loss: 3199070316.7123\n",
      "Epoch 142/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1480322579.9349 - val_loss: 3209328999.4521\n",
      "Epoch 143/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1477870024.2811 - val_loss: 3218827420.0548\n",
      "Epoch 144/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1476194067.0848 - val_loss: 3214861838.0274\n",
      "Epoch 145/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1476328917.3333 - val_loss: 3222851275.3973\n",
      "Epoch 146/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1473758148.3325 - val_loss: 3204257483.3973\n",
      "Epoch 147/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1473763765.9092 - val_loss: 3167591762.4110\n",
      "Epoch 148/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1468890786.3719 - val_loss: 3249749333.9178\n",
      "Epoch 149/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1466296010.4747 - val_loss: 3184588740.3836\n",
      "Epoch 150/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1479339953.0831 - val_loss: 3257148735.1233\n",
      "Epoch 151/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1459738126.9717 - val_loss: 3155807203.9452\n",
      "Epoch 152/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1468020457.8989 - val_loss: 3230487455.5616\n",
      "Epoch 153/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1467576966.5810 - val_loss: 3208534713.8630\n",
      "Epoch 154/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1459418274.2211 - val_loss: 3203149312.0000\n",
      "Epoch 155/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1467769116.6272 - val_loss: 3221557642.5205\n",
      "Epoch 156/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1458138971.2836 - val_loss: 3126444642.1918\n",
      "Epoch 157/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1457023278.3136 - val_loss: 3189015031.2329\n",
      "Epoch 158/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1452216593.3299 - val_loss: 3221237842.4110\n",
      "Epoch 159/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1463663629.9297 - val_loss: 3234237727.5616\n",
      "Epoch 160/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1456292125.1757 - val_loss: 3201688453.2603\n",
      "Epoch 161/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1448378137.9126 - val_loss: 3173139971.5068\n",
      "Epoch 162/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1450147078.8003 - val_loss: 3207035353.4247\n",
      "Epoch 163/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1449656834.3582 - val_loss: 3195080924.9315\n",
      "Epoch 164/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1446053675.9829 - val_loss: 3176446840.9863\n",
      "Epoch 165/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1448684265.8440 - val_loss: 3207829398.7945\n",
      "Epoch 166/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1441257053.0111 - val_loss: 3134001546.5205\n",
      "Epoch 167/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1442477649.6041 - val_loss: 3180028721.0959\n",
      "Epoch 168/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1438727716.0857 - val_loss: 3181341624.1096\n",
      "Epoch 169/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1437855169.5904 - val_loss: 3183206371.9452\n",
      "Epoch 170/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1436665262.8346 - val_loss: 3190770396.9315\n",
      "Epoch 171/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1438779932.7918 - val_loss: 3175425834.0822\n",
      "Epoch 172/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1437544903.6230 - val_loss: 3147711533.5890\n",
      "Epoch 173/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1431401924.8535 - val_loss: 3160866731.8356\n",
      "Epoch 174/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1430647362.7421 - val_loss: 3162103627.3973\n",
      "Epoch 175/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1438471647.8629 - val_loss: 3080320263.0137\n",
      "Epoch 176/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1427229249.3162 - val_loss: 3151083239.4521\n",
      "Epoch 177/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1429403859.0848 - val_loss: 3099396525.5890\n",
      "Epoch 178/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1434836707.4824 - val_loss: 3152029385.6438\n",
      "Epoch 179/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1427304010.8038 - val_loss: 3138980679.8904\n",
      "Epoch 180/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1430047138.6598 - val_loss: 3183665506.1918\n",
      "Epoch 181/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1424476048.1234 - val_loss: 3104112087.6712\n",
      "Epoch 182/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1418332255.8629 - val_loss: 3247054963.7260\n",
      "Epoch 183/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1420282364.0514 - val_loss: 3087104087.6712\n",
      "Epoch 184/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1421974227.2494 - val_loss: 3136672613.6986\n",
      "Epoch 185/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1415499252.9220 - val_loss: 3144995750.5753\n",
      "Epoch 186/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1416346929.0831 - val_loss: 3119313169.5342\n",
      "Epoch 187/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1413025078.6221 - val_loss: 3118817448.3288\n",
      "Epoch 188/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1409323811.7566 - val_loss: 3143995865.4247\n",
      "Epoch 189/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1408828644.5793 - val_loss: 3125622117.6986\n",
      "Epoch 190/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1416309831.0746 - val_loss: 3177310183.4521\n",
      "Epoch 191/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1409511477.0865 - val_loss: 3128015984.2192\n",
      "Epoch 192/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1405851288.4045 - val_loss: 3170470042.3014\n",
      "Epoch 193/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1404473893.7309 - val_loss: 3084152369.0959\n",
      "Epoch 194/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1406922148.5518 - val_loss: 3083450050.6301\n",
      "Epoch 195/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1403853702.1971 - val_loss: 3111937008.2192\n",
      "Epoch 196/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1400905794.6324 - val_loss: 3103091245.5890\n",
      "Epoch 197/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1396826188.5587 - val_loss: 3098127440.6575\n",
      "Epoch 198/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1397424325.6487 - val_loss: 3104948420.3836\n",
      "Epoch 199/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1394368216.9529 - val_loss: 3130412942.0274\n",
      "Epoch 200/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1399225521.6864 - val_loss: 3037778556.4932\n",
      "Epoch 201/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1401163106.7969 - val_loss: 3152657278.2466\n",
      "Epoch 202/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1395044963.8115 - val_loss: 3071270368.4384\n",
      "Epoch 203/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1391320107.8732 - val_loss: 3112499575.2329\n",
      "Epoch 204/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1388457493.6898 - val_loss: 3100581118.2466\n",
      "Epoch 205/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1389107352.6787 - val_loss: 3101615866.7397\n",
      "Epoch 206/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1387113397.1962 - val_loss: 3123269554.8493\n",
      "Epoch 207/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1388549245.8063 - val_loss: 3093736062.2466\n",
      "Epoch 208/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1389706529.5630 - val_loss: 3162695736.1096\n",
      "Epoch 209/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1386238145.4259 - val_loss: 3087381155.0685\n",
      "Epoch 210/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1382074242.3033 - val_loss: 3093986693.2603\n",
      "Epoch 211/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1381908823.4173 - val_loss: 3075597987.0685\n",
      "Epoch 212/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1387165900.0925 - val_loss: 3031338225.9726\n",
      "Epoch 213/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1395195432.2536 - val_loss: 3107067234.1918\n",
      "Epoch 214/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1380676580.8535 - val_loss: 3108179841.7534\n",
      "Epoch 215/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1391910216.2262 - val_loss: 3155716341.4795\n",
      "Epoch 216/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1375506934.5398 - val_loss: 3053554724.8219\n",
      "Epoch 217/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1386063965.3402 - val_loss: 3121331087.7808\n",
      "Epoch 218/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1374614086.5261 - val_loss: 3055813235.7260\n",
      "Epoch 219/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1378252734.2999 - val_loss: 3108765056.0000\n",
      "Epoch 220/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1375893148.4627 - val_loss: 3070547697.9726\n",
      "Epoch 221/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1380307815.1020 - val_loss: 3127550990.0274\n",
      "Epoch 222/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1367932035.1808 - val_loss: 3066910870.7945\n",
      "Epoch 223/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1365885398.6495 - val_loss: 3068394497.7534\n",
      "Epoch 224/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1378019126.0463 - val_loss: 3034665966.4658\n",
      "Epoch 225/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1373355156.4010 - val_loss: 3073423796.6027\n",
      "Epoch 226/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1364225923.1808 - val_loss: 3095787670.7945\n",
      "Epoch 227/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1367264075.2973 - val_loss: 3144637317.2603\n",
      "Epoch 228/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1364112170.0086 - val_loss: 3010497653.4795\n",
      "Epoch 229/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1369643733.2237 - val_loss: 3005151482.7397\n",
      "Epoch 230/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1359913242.5433 - val_loss: 3032340928.8767\n",
      "Epoch 231/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1364319435.5716 - val_loss: 3063132154.7397\n",
      "Epoch 232/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1361290383.4653 - val_loss: 3044027251.7260\n",
      "Epoch 233/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1355220255.2048 - val_loss: 3037863781.6986\n",
      "Epoch 234/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1351072918.0463 - val_loss: 3065345930.5205\n",
      "Epoch 235/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1363206585.8303 - val_loss: 3063245441.7534\n",
      "Epoch 236/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1351144082.3171 - val_loss: 3052489652.6027\n",
      "Epoch 237/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1368458748.2708 - val_loss: 3102289928.7671\n",
      "Epoch 238/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1362870192.8089 - val_loss: 3016290407.4521\n",
      "Epoch 239/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1347241756.6821 - val_loss: 3036717748.6027\n",
      "Epoch 240/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1345427874.3856 - val_loss: 2997531214.9041\n",
      "Epoch 241/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1342691961.4739 - val_loss: 3122720469.9178\n",
      "Epoch 242/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1359942157.8201 - val_loss: 3077138090.0822\n",
      "Epoch 243/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1344189502.6290 - val_loss: 3064088216.5479\n",
      "Epoch 244/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1339096164.8260 - val_loss: 2997879210.0822\n",
      "Epoch 245/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1346550806.9512 - val_loss: 2995009921.7534\n",
      "Epoch 246/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1345517716.8946 - val_loss: 3138842161.0959\n",
      "Epoch 247/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1347399486.0257 - val_loss: 3054075013.2603\n",
      "Epoch 248/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1339446634.5021 - val_loss: 2960078034.4110\n",
      "Epoch 249/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1344987320.2125 - val_loss: 3038697237.0411\n",
      "Epoch 250/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1339969448.3085 - val_loss: 3088044126.6849\n",
      "Epoch 251/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1336182689.5630 - val_loss: 2975944120.1096\n",
      "Epoch 252/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1341513526.5124 - val_loss: 3030476545.7534\n",
      "Epoch 253/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1341409879.4722 - val_loss: 3064799081.2055\n",
      "Epoch 254/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1333578372.9357 - val_loss: 3016648251.6164\n",
      "Epoch 255/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1335303471.1088 - val_loss: 3038118813.8082\n",
      "Epoch 256/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1325362177.8646 - val_loss: 2981165999.3425\n",
      "Epoch 257/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1330154972.5724 - val_loss: 3070005761.7534\n",
      "Epoch 258/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1332356265.3505 - val_loss: 3014303875.5068\n",
      "Epoch 259/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1323182665.6795 - val_loss: 2995376853.9178\n",
      "Epoch 260/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1325399349.3608 - val_loss: 3039482706.4110\n",
      "Epoch 261/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1323853158.4439 - val_loss: 2972400126.2466\n",
      "Epoch 262/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 175us/step - loss: 1331742575.9040 - val_loss: 2983347953.9726\n",
      "Epoch 263/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1322969068.1474 - val_loss: 2999233437.8082\n",
      "Epoch 264/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1325709812.3736 - val_loss: 2948871111.8904\n",
      "Epoch 265/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1319454295.1705 - val_loss: 3027924883.2877\n",
      "Epoch 266/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1319151775.4790 - val_loss: 2978417109.9178\n",
      "Epoch 267/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1316628719.1637 - val_loss: 2983793849.8630\n",
      "Epoch 268/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1324831273.9537 - val_loss: 3020292837.6986\n",
      "Epoch 269/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1319023766.7592 - val_loss: 2923802243.5068\n",
      "Epoch 270/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1314177399.4996 - val_loss: 3019780404.6027\n",
      "Epoch 271/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1317192751.3830 - val_loss: 2940756032.8767\n",
      "Epoch 272/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1317408587.3522 - val_loss: 2965475229.8082\n",
      "Epoch 273/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1311329999.6572 - val_loss: 3033924343.2329\n",
      "Epoch 274/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1313409653.5801 - val_loss: 3009352467.2877\n",
      "Epoch 275/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1312649425.7138 - val_loss: 3073956322.1918\n",
      "Epoch 276/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1320635826.8380 - val_loss: 2988136314.7397\n",
      "Epoch 277/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1304978835.4687 - val_loss: 2914882991.3425\n",
      "Epoch 278/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1310697930.4747 - val_loss: 3021648403.2877\n",
      "Epoch 279/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1306578379.7909 - val_loss: 2992599409.9726\n",
      "Epoch 280/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1304749285.2374 - val_loss: 2997729367.6712\n",
      "Epoch 281/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1305360851.1397 - val_loss: 3054258596.8219\n",
      "Epoch 282/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1302241478.8826 - val_loss: 2973233916.4932\n",
      "Epoch 283/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1301173095.1568 - val_loss: 3034198931.2877\n",
      "Epoch 284/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1296778896.0686 - val_loss: 2893460650.0822\n",
      "Epoch 285/500\n",
      "1167/1167 [==============================] - 0s 182us/step - loss: 1304081349.7584 - val_loss: 3044056584.7671\n",
      "Epoch 286/500\n",
      "1167/1167 [==============================] - 0s 190us/step - loss: 1307546485.8543 - val_loss: 2904137501.8082\n",
      "Epoch 287/500\n",
      "1167/1167 [==============================] - 0s 191us/step - loss: 1301135578.3787 - val_loss: 2934102077.3699\n",
      "Epoch 288/500\n",
      "1167/1167 [==============================] - 0s 186us/step - loss: 1302715198.9854 - val_loss: 2940728409.4247\n",
      "Epoch 289/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1292220500.2365 - val_loss: 2975911737.8630\n",
      "Epoch 290/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1294294835.8800 - val_loss: 3026599834.3014\n",
      "Epoch 291/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1295697770.1183 - val_loss: 2921296426.0822\n",
      "Epoch 292/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1288709267.6332 - val_loss: 2981139771.6164\n",
      "Epoch 293/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1292714257.2202 - val_loss: 2983060436.1644\n",
      "Epoch 294/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1292512649.4327 - val_loss: 2971155313.9726\n",
      "Epoch 295/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1298626690.9614 - val_loss: 2937800774.1370\n",
      "Epoch 296/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1289446185.3505 - val_loss: 2925798617.4247\n",
      "Epoch 297/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1283740500.5656 - val_loss: 2992090240.0000\n",
      "Epoch 298/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1286734162.8106 - val_loss: 2966588396.7123\n",
      "Epoch 299/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1287027812.9083 - val_loss: 2953481577.2055\n",
      "Epoch 300/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1292886082.2485 - val_loss: 2926363100.9315\n",
      "Epoch 301/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1280676051.1397 - val_loss: 2985889136.2192\n",
      "Epoch 302/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1305777941.4979 - val_loss: 3059140937.6438\n",
      "Epoch 303/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1293633796.1680 - val_loss: 2981515888.2192\n",
      "Epoch 304/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1279606451.7704 - val_loss: 2876981006.0274\n",
      "Epoch 305/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1273897293.4636 - val_loss: 3064936339.2877\n",
      "Epoch 306/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1282833342.1354 - val_loss: 2915763689.2055\n",
      "Epoch 307/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1275759232.9871 - val_loss: 2917104559.3425\n",
      "Epoch 308/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1271161642.8312 - val_loss: 2913697653.4795\n",
      "Epoch 309/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1280673440.5758 - val_loss: 2882323215.7808\n",
      "Epoch 310/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1267589126.1971 - val_loss: 2960895763.2877\n",
      "Epoch 311/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1271020445.3950 - val_loss: 2900114602.0822\n",
      "Epoch 312/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1270690873.6932 - val_loss: 2924173790.6849\n",
      "Epoch 313/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1269123571.9349 - val_loss: 2898760111.3425\n",
      "Epoch 314/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1274095231.1225 - val_loss: 2882125105.0959\n",
      "Epoch 315/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1272002406.4439 - val_loss: 2878777275.6164\n",
      "Epoch 316/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1268666860.3668 - val_loss: 2903444699.1781\n",
      "Epoch 317/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1267449707.3796 - val_loss: 2874975317.9178\n",
      "Epoch 318/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1266572173.9846 - val_loss: 2880182734.9041\n",
      "Epoch 319/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1265637773.1620 - val_loss: 2948092503.6712\n",
      "Epoch 320/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1263553088.0548 - val_loss: 2879617700.8219\n",
      "Epoch 321/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1261248178.7832 - val_loss: 2963113410.6301\n",
      "Epoch 322/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1255773775.4105 - val_loss: 2865518763.8356\n",
      "Epoch 323/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1267417911.9109 - val_loss: 2931513121.3151\n",
      "Epoch 324/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1258263824.0686 - val_loss: 2938248986.3014\n",
      "Epoch 325/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1254945253.8406 - val_loss: 2879494678.7945\n",
      "Epoch 326/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1260733119.3419 - val_loss: 2949298361.8630\n",
      "Epoch 327/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1253375998.2451 - val_loss: 2905074081.3151\n",
      "Epoch 328/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1263154684.3805 - val_loss: 2955825062.5753\n",
      "Epoch 329/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1254281836.3668 - val_loss: 2912716538.7397\n",
      "Epoch 330/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1250946054.2519 - val_loss: 2922847789.5890\n",
      "Epoch 331/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1254447664.4799 - val_loss: 2961930769.5342\n",
      "Epoch 332/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1253670352.5073 - val_loss: 2863370872.9863\n",
      "Epoch 333/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1248950893.0249 - val_loss: 2853797747.7260\n",
      "Epoch 334/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1254028187.8046 - val_loss: 3003580763.1781\n",
      "Epoch 335/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1256037022.2725 - val_loss: 2908640815.3425\n",
      "Epoch 336/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1259318683.4756 - val_loss: 2842734609.5342\n",
      "Epoch 337/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1264734909.4773 - val_loss: 2839478536.7671\n",
      "Epoch 338/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1242764779.2699 - val_loss: 2888942840.9863\n",
      "Epoch 339/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1242306391.0883 - val_loss: 2834431638.7945\n",
      "Epoch 340/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1242636591.6024 - val_loss: 2915873255.4521\n",
      "Epoch 341/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1243641610.7489 - val_loss: 2855332581.6986\n",
      "Epoch 342/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1240285475.7566 - val_loss: 2848123176.3288\n",
      "Epoch 343/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1242308943.1911 - val_loss: 2898200674.1918\n",
      "Epoch 344/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1238495101.0386 - val_loss: 2900130461.8082\n",
      "Epoch 345/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1241199578.6530 - val_loss: 2832159284.6027\n",
      "Epoch 346/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1235823598.3136 - val_loss: 2844795453.3699\n",
      "Epoch 347/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1245936585.2682 - val_loss: 2846639542.3562\n",
      "Epoch 348/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1238163026.3719 - val_loss: 2824456528.6575\n",
      "Epoch 349/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1239989174.0737 - val_loss: 2857422416.6575\n",
      "Epoch 350/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1235677960.9940 - val_loss: 2928574348.2740\n",
      "Epoch 351/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1232735735.9931 - val_loss: 2869492679.8904\n",
      "Epoch 352/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1240868395.9829 - val_loss: 2827326483.2877\n",
      "Epoch 353/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1228010216.3085 - val_loss: 2863481084.4932\n",
      "Epoch 354/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1227188903.5955 - val_loss: 2865763142.1370\n",
      "Epoch 355/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1225744499.7155 - val_loss: 2821229688.9863\n",
      "Epoch 356/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1231328016.5347 - val_loss: 2918150119.4521\n",
      "Epoch 357/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1244080364.2571 - val_loss: 2813549192.7671\n",
      "Epoch 358/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1235131676.9563 - val_loss: 2853226131.2877\n",
      "Epoch 359/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1222747815.5955 - val_loss: 2837830068.6027\n",
      "Epoch 360/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1218564204.7506 - val_loss: 2907235177.2055\n",
      "Epoch 361/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1230142663.8423 - val_loss: 2840624029.8082\n",
      "Epoch 362/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1216783149.7378 - val_loss: 2835447369.6438\n",
      "Epoch 363/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1220425965.0249 - val_loss: 2807666619.6164\n",
      "Epoch 364/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1220695535.3282 - val_loss: 2838438296.5479\n",
      "Epoch 365/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1220780544.6033 - val_loss: 2878642477.5890\n",
      "Epoch 366/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1217090689.3710 - val_loss: 2855982250.0822\n",
      "Epoch 367/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1213924091.1740 - val_loss: 2843535907.0685\n",
      "Epoch 368/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1219327932.9837 - val_loss: 2774986350.4658\n",
      "Epoch 369/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1218864004.4970 - val_loss: 2793300429.1507\n",
      "Epoch 370/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1212229949.6967 - val_loss: 2831930737.9726\n",
      "Epoch 371/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1211667747.0985 - val_loss: 2852982601.6438\n",
      "Epoch 372/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1213747994.9272 - val_loss: 2805749505.7534\n",
      "Epoch 373/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1207991295.5613 - val_loss: 2794501393.5342\n",
      "Epoch 374/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1205371672.1302 - val_loss: 2821225438.6849\n",
      "Epoch 375/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1204352147.0300 - val_loss: 2906593716.6027\n",
      "Epoch 376/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1208957601.1243 - val_loss: 2819471949.1507\n",
      "Epoch 377/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1203188959.4790 - val_loss: 2779140890.3014\n",
      "Epoch 378/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1219976639.7258 - val_loss: 2748292750.0274\n",
      "Epoch 379/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1216865482.5844 - val_loss: 2767301097.2055\n",
      "Epoch 380/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1203462797.0523 - val_loss: 2845217637.6986\n",
      "Epoch 381/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1198210961.2202 - val_loss: 2815441457.0959\n",
      "Epoch 382/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1198454603.4619 - val_loss: 2870674684.4932\n",
      "Epoch 383/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1198387118.6153 - val_loss: 2794913521.9726\n",
      "Epoch 384/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1199062104.4319 - val_loss: 2783874996.6027\n",
      "Epoch 385/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1199923650.4679 - val_loss: 2854769905.9726\n",
      "Epoch 386/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1192066503.7875 - val_loss: 2778701944.9863\n",
      "Epoch 387/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1194436834.6050 - val_loss: 2785146143.5616\n",
      "Epoch 388/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1210357915.2014 - val_loss: 2770981518.0274\n",
      "Epoch 389/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1196198321.7412 - val_loss: 2809237987.9452\n",
      "Epoch 390/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1190510725.4841 - val_loss: 2769142254.4658\n",
      "Epoch 391/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1195550978.4679 - val_loss: 2754038622.6849\n",
      "Epoch 392/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1167/1167 [==============================] - 0s 167us/step - loss: 1194996301.6555 - val_loss: 2860333389.1507\n",
      "Epoch 393/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1187779177.9537 - val_loss: 2812116793.8630\n",
      "Epoch 394/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1186398046.2725 - val_loss: 2826688878.4658\n",
      "Epoch 395/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1189064424.7198 - val_loss: 2785884032.0000\n",
      "Epoch 396/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1188443536.5621 - val_loss: 2836600046.4658\n",
      "Epoch 397/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1185510655.2322 - val_loss: 2776230901.4795\n",
      "Epoch 398/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1187375549.3128 - val_loss: 2809821203.2877\n",
      "Epoch 399/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1180257150.9580 - val_loss: 2774626161.9726\n",
      "Epoch 400/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1181016472.2399 - val_loss: 2853340526.4658\n",
      "Epoch 401/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1180045217.2888 - val_loss: 2741655937.7534\n",
      "Epoch 402/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1181592951.2802 - val_loss: 2819270608.6575\n",
      "Epoch 403/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1175301321.3231 - val_loss: 2775820109.1507\n",
      "Epoch 404/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1180646243.5921 - val_loss: 2746420702.6849\n",
      "Epoch 405/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1177251809.6178 - val_loss: 2782180085.4795\n",
      "Epoch 406/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1171888328.3907 - val_loss: 2781179200.8767\n",
      "Epoch 407/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1174505434.5433 - val_loss: 2800674368.8767\n",
      "Epoch 408/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1177882332.7918 - val_loss: 2814586997.4795\n",
      "Epoch 409/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1173492407.3762 - val_loss: 2766070936.5479\n",
      "Epoch 410/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1179875054.5604 - val_loss: 2744561909.4795\n",
      "Epoch 411/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1174554510.1491 - val_loss: 2736749667.9452\n",
      "Epoch 412/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1171589981.0111 - val_loss: 2745053953.7534\n",
      "Epoch 413/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1166020395.8458 - val_loss: 2809257733.2603\n",
      "Epoch 414/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1166949792.9049 - val_loss: 2779700383.5616\n",
      "Epoch 415/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1174294388.8123 - val_loss: 2768061720.5479\n",
      "Epoch 416/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1167368243.9349 - val_loss: 2892913295.7808\n",
      "Epoch 417/500\n",
      "1167/1167 [==============================] - 0s 164us/step - loss: 1172740965.9503 - val_loss: 2753840739.9452\n",
      "Epoch 418/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1165217020.1611 - val_loss: 2703913491.2877\n",
      "Epoch 419/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1173349306.7901 - val_loss: 2793832118.3562\n",
      "Epoch 420/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1162344795.3659 - val_loss: 2772192119.2329\n",
      "Epoch 421/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1163219127.9383 - val_loss: 2833508399.3425\n",
      "Epoch 422/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1167402177.4533 - val_loss: 2785251086.0274\n",
      "Epoch 423/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1157276278.1285 - val_loss: 2774923884.7123\n",
      "Epoch 424/500\n",
      "1167/1167 [==============================] - 0s 179us/step - loss: 1160454854.9649 - val_loss: 2750381678.4658\n",
      "Epoch 425/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1153314921.6247 - val_loss: 2761940039.8904\n",
      "Epoch 426/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1162512417.1791 - val_loss: 2757364306.4110\n",
      "Epoch 427/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1154348778.0360 - val_loss: 2735159820.2740\n",
      "Epoch 428/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1156092964.2502 - val_loss: 2756811258.7397\n",
      "Epoch 429/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1160378365.3676 - val_loss: 2696211238.5753\n",
      "Epoch 430/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1164571595.9829 - val_loss: 2830658128.6575\n",
      "Epoch 431/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1158279900.3805 - val_loss: 2781469382.1370\n",
      "Epoch 432/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1148499736.2399 - val_loss: 2717235117.5890\n",
      "Epoch 433/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1150633840.3702 - val_loss: 2773992328.7671\n",
      "Epoch 434/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1146513381.3745 - val_loss: 2717910899.7260\n",
      "Epoch 435/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1146350182.5536 - val_loss: 2709895185.5342\n",
      "Epoch 436/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1152961658.0223 - val_loss: 2772352322.6301\n",
      "Epoch 437/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1145670806.8963 - val_loss: 2758604992.8767\n",
      "Epoch 438/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1142615517.6692 - val_loss: 2734153098.5205\n",
      "Epoch 439/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1157493938.3376 - val_loss: 2800605462.7945\n",
      "Epoch 440/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1138191524.1954 - val_loss: 2688267379.7260\n",
      "Epoch 441/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1146616604.4079 - val_loss: 2715024245.4795\n",
      "Epoch 442/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1150861615.5201 - val_loss: 2837676724.6027\n",
      "Epoch 443/500\n",
      "1167/1167 [==============================] - 0s 165us/step - loss: 1145849471.0129 - val_loss: 2718175209.2055\n",
      "Epoch 444/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1145488372.3736 - val_loss: 2652436608.0000\n",
      "Epoch 445/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1164997228.4216 - val_loss: 2775186291.7260\n",
      "Epoch 446/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1135284835.9760 - val_loss: 2751465933.1507\n",
      "Epoch 447/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1137593442.4953 - val_loss: 2735837638.1370\n",
      "Epoch 448/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1132006251.7361 - val_loss: 2708203386.7397\n",
      "Epoch 449/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1138115343.6847 - val_loss: 2683439391.5616\n",
      "Epoch 450/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1139765243.2836 - val_loss: 2728276522.0822\n",
      "Epoch 451/500\n",
      "1167/1167 [==============================] - 0s 166us/step - loss: 1132071740.3256 - val_loss: 2718741155.0685\n",
      "Epoch 452/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1130127707.8046 - val_loss: 2712132436.1644\n",
      "Epoch 453/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1145174902.0463 - val_loss: 2658424153.4247\n",
      "Epoch 454/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1134574207.0129 - val_loss: 2705778723.0685\n",
      "Epoch 455/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1130904981.0043 - val_loss: 2710782251.8356\n",
      "Epoch 456/500\n",
      "1167/1167 [==============================] - 0s 168us/step - loss: 1139274133.6624 - val_loss: 2794782509.5890\n",
      "Epoch 457/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1151135686.8003 - val_loss: 2754844763.1781\n",
      "Epoch 458/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1131579318.6221 - val_loss: 2659550974.2466\n",
      "Epoch 459/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1116771861.9914 - val_loss: 2782558399.1233\n",
      "Epoch 460/500\n",
      "1167/1167 [==============================] - 0s 178us/step - loss: 1130127745.1517 - val_loss: 2713946674.8493\n",
      "Epoch 461/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1120211946.1731 - val_loss: 2700679653.6986\n",
      "Epoch 462/500\n",
      "1167/1167 [==============================] - 0s 167us/step - loss: 1128732453.6761 - val_loss: 2766594328.5479\n",
      "Epoch 463/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1123788964.7986 - val_loss: 2709428238.0274\n",
      "Epoch 464/500\n",
      "1167/1167 [==============================] - 0s 171us/step - loss: 1119661451.6812 - val_loss: 2770372464.2192\n",
      "Epoch 465/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1126740613.4841 - val_loss: 2699937313.3151\n",
      "Epoch 466/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1122872063.1225 - val_loss: 2657749944.1096\n",
      "Epoch 467/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1124363109.5664 - val_loss: 2656557343.5616\n",
      "Epoch 468/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1122707196.8740 - val_loss: 2668460640.4384\n",
      "Epoch 469/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1118526203.0643 - val_loss: 2660293842.4110\n",
      "Epoch 470/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1115871453.8338 - val_loss: 2689540097.7534\n",
      "Epoch 471/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1115483877.8955 - val_loss: 2736692207.3425\n",
      "Epoch 472/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1111898191.9589 - val_loss: 2651729849.8630\n",
      "Epoch 473/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1101995607.3076 - val_loss: 2825248943.3425\n",
      "Epoch 474/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1116079575.1979 - val_loss: 2649945166.9041\n",
      "Epoch 475/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1120136125.4773 - val_loss: 2676112843.3973\n",
      "Epoch 476/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1104416788.1817 - val_loss: 2765490617.8630\n",
      "Epoch 477/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1119598747.3111 - val_loss: 2676602434.6301\n",
      "Epoch 478/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1115953851.3933 - val_loss: 2684436091.6164\n",
      "Epoch 479/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1111801332.9220 - val_loss: 2747725692.4932\n",
      "Epoch 480/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1108185286.5261 - val_loss: 2711726090.5205\n",
      "Epoch 481/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1104744101.6213 - val_loss: 2700173410.1918\n",
      "Epoch 482/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1103761732.6615 - val_loss: 2675744471.6712\n",
      "Epoch 483/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1100812274.1799 - val_loss: 2721587943.4521\n",
      "Epoch 484/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1101844595.9349 - val_loss: 2725707306.0822\n",
      "Epoch 485/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1104896594.8655 - val_loss: 2631405182.2466\n",
      "Epoch 486/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1102339366.6084 - val_loss: 2681277388.2740\n",
      "Epoch 487/500\n",
      "1167/1167 [==============================] - 0s 175us/step - loss: 1102172249.3642 - val_loss: 2615350475.3973\n",
      "Epoch 488/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1092507479.1842 - val_loss: 2779674129.5342\n",
      "Epoch 489/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1111013321.9263 - val_loss: 2667322356.6027\n",
      "Epoch 490/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1097216608.3016 - val_loss: 2713862548.1644\n",
      "Epoch 491/500\n",
      "1167/1167 [==============================] - 0s 169us/step - loss: 1102458905.1174 - val_loss: 2683880463.7808\n",
      "Epoch 492/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1099997836.5587 - val_loss: 2662023864.1096\n",
      "Epoch 493/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1097724972.0925 - val_loss: 2626973273.4247\n",
      "Epoch 494/500\n",
      "1167/1167 [==============================] - 0s 174us/step - loss: 1087307654.2519 - val_loss: 2726045308.4932\n",
      "Epoch 495/500\n",
      "1167/1167 [==============================] - 0s 173us/step - loss: 1097397482.9409 - val_loss: 2700623652.8219\n",
      "Epoch 496/500\n",
      "1167/1167 [==============================] - 0s 170us/step - loss: 1094706161.6315 - val_loss: 2691211351.6712\n",
      "Epoch 497/500\n",
      "1167/1167 [==============================] - 0s 172us/step - loss: 1094648169.7344 - val_loss: 2704708844.7123\n",
      "Epoch 498/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1089752727.3625 - val_loss: 2664464984.5479\n",
      "Epoch 499/500\n",
      "1167/1167 [==============================] - 0s 177us/step - loss: 1093873680.2879 - val_loss: 2682409014.3562\n",
      "Epoch 500/500\n",
      "1167/1167 [==============================] - 0s 176us/step - loss: 1085849150.7386 - val_loss: 2679263491.5068\n"
     ]
    }
   ],
   "source": [
    "newAdam = Adam(learning_rate=0.0001)\n",
    "NN_500E_1H.compile(loss='mean_squared_error', optimizer=newAdam)\n",
    "history = NN_500E_1H.fit(x=X,y=y,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 512)               207872    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 512)               262656    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 733,697\n",
      "Trainable params: 733,697\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "NN_500E_Adam_LReLU = Sequential()\n",
    "NN_500E_Adam_LReLU.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam_LReLU.add(LeakyReLU(alpha=0.1))\n",
    "NN_500E_Adam_LReLU.add(Dense(512))\n",
    "NN_500E_Adam_LReLU.add(LeakyReLU(alpha=0.1))\n",
    "NN_500E_Adam_LReLU.add(Dense(512))\n",
    "NN_500E_Adam_LReLU.add(LeakyReLU(alpha=0.1))\n",
    "NN_500E_Adam_LReLU.add(Dense(1))\n",
    "NN_500E_Adam_LReLU.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/500\n",
      "1164/1164 [==============================] - 0s 128us/step - loss: 1132822656.0000 - val_loss: 3111554304.0000\n",
      "Epoch 2/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 3090528768.0000 - val_loss: 1536600320.0000\n",
      "Epoch 3/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1127461120.0000 - val_loss: 3096504320.0000\n",
      "Epoch 4/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 2590830592.0000 - val_loss: 2544436224.0000\n",
      "Epoch 5/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 2019893120.0000 - val_loss: 1605175168.0000\n",
      "Epoch 6/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1158199296.0000 - val_loss: 1671871744.0000\n",
      "Epoch 7/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1403308160.0000 - val_loss: 2038270336.0000\n",
      "Epoch 8/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1870407808.0000 - val_loss: 1968149376.0000\n",
      "Epoch 9/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1788353664.0000 - val_loss: 1636038784.0000\n",
      "Epoch 10/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1362334080.0000 - val_loss: 1531789440.0000\n",
      "Epoch 11/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1133008384.0000 - val_loss: 1816308608.0000\n",
      "Epoch 12/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1320723584.0000 - val_loss: 2090470016.0000\n",
      "Epoch 13/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1562720256.0000 - val_loss: 1985211392.0000\n",
      "Epoch 14/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1468499072.0000 - val_loss: 1680083840.0000\n",
      "Epoch 15/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1216626432.0000 - val_loss: 1518363392.0000\n",
      "Epoch 16/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1138666112.0000 - val_loss: 1565416064.0000\n",
      "Epoch 17/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1261355904.0000 - val_loss: 1645171456.0000\n",
      "Epoch 18/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1378655872.0000 - val_loss: 1622349824.0000\n",
      "Epoch 19/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1345455488.0000 - val_loss: 1538355712.0000\n",
      "Epoch 20/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1212951296.0000 - val_loss: 1523071616.0000\n",
      "Epoch 21/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1133982464.0000 - val_loss: 1625926784.0000\n",
      "Epoch 22/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1182931584.0000 - val_loss: 1739264256.0000\n",
      "Epoch 23/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1269882240.0000 - val_loss: 1731633536.0000\n",
      "Epoch 24/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1264624640.0000 - val_loss: 1621146624.0000\n",
      "Epoch 25/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1181017216.0000 - val_loss: 1529152768.0000\n",
      "Epoch 26/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1132773632.0000 - val_loss: 1516662272.0000\n",
      "Epoch 27/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1163510400.0000 - val_loss: 1540798976.0000\n",
      "Epoch 28/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1213611136.0000 - val_loss: 1540446080.0000\n",
      "Epoch 29/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1212793600.0000 - val_loss: 1517107072.0000\n",
      "Epoch 30/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1165128832.0000 - val_loss: 1519777280.0000\n",
      "Epoch 31/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1131532288.0000 - val_loss: 1569218176.0000\n",
      "Epoch 32/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1147538176.0000 - val_loss: 1620703104.0000\n",
      "Epoch 33/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1180012800.0000 - val_loss: 1618556672.0000\n",
      "Epoch 34/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1178039936.0000 - val_loss: 1568859520.0000\n",
      "Epoch 35/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1145990656.0000 - val_loss: 1524080256.0000\n",
      "Epoch 36/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1128749056.0000 - val_loss: 1512502144.0000\n",
      "Epoch 37/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1143108096.0000 - val_loss: 1517009664.0000\n",
      "Epoch 38/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1160838272.0000 - val_loss: 1515428352.0000\n",
      "Epoch 39/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1154499456.0000 - val_loss: 1514515328.0000\n",
      "Epoch 40/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1133987584.0000 - val_loss: 1532985984.0000\n",
      "Epoch 41/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1127401600.0000 - val_loss: 1564914560.0000\n",
      "Epoch 42/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1139438336.0000 - val_loss: 1580550912.0000\n",
      "Epoch 43/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1147435776.0000 - val_loss: 1565080960.0000\n",
      "Epoch 44/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1138190336.0000 - val_loss: 1536996864.0000\n",
      "Epoch 45/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1126262400.0000 - val_loss: 1520329344.0000\n",
      "Epoch 46/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1127840512.0000 - val_loss: 1517382016.0000\n",
      "Epoch 47/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1136389248.0000 - val_loss: 1517879424.0000\n",
      "Epoch 48/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1136390784.0000 - val_loss: 1520925056.0000\n",
      "Epoch 49/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1127763584.0000 - val_loss: 1533425280.0000\n",
      "Epoch 50/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1123539200.0000 - val_loss: 1551949952.0000\n",
      "Epoch 51/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1128119040.0000 - val_loss: 1560860160.0000\n",
      "Epoch 52/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1131545216.0000 - val_loss: 1552194176.0000\n",
      "Epoch 53/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1127311104.0000 - val_loss: 1535915776.0000\n",
      "Epoch 54/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1122307328.0000 - val_loss: 1525173376.0000\n",
      "Epoch 55/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1123517696.0000 - val_loss: 1521962112.0000\n",
      "Epoch 56/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1126794752.0000 - val_loss: 1522684800.0000\n",
      "Epoch 57/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1125450368.0000 - val_loss: 1527799936.0000\n",
      "Epoch 58/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1121450624.0000 - val_loss: 1538381056.0000\n",
      "Epoch 59/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1120833792.0000 - val_loss: 1548325376.0000\n",
      "Epoch 60/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1123102208.0000 - val_loss: 1549098496.0000\n",
      "Epoch 61/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1122983168.0000 - val_loss: 1540775040.0000\n",
      "Epoch 62/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1120187648.0000 - val_loss: 1531241344.0000\n",
      "Epoch 63/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1119091840.0000 - val_loss: 1526115072.0000\n",
      "Epoch 64/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1120418560.0000 - val_loss: 1525462784.0000\n",
      "Epoch 65/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1120613888.0000 - val_loss: 1528600320.0000\n",
      "Epoch 66/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1118714112.0000 - val_loss: 1535432960.0000\n",
      "Epoch 67/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1117602944.0000 - val_loss: 1542702336.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 68/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1118319232.0000 - val_loss: 1544869504.0000\n",
      "Epoch 69/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1118459904.0000 - val_loss: 1540610304.0000\n",
      "Epoch 70/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1117089536.0000 - val_loss: 1534191744.0000\n",
      "Epoch 71/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1116189568.0000 - val_loss: 1530053760.0000\n",
      "Epoch 72/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1116561280.0000 - val_loss: 1529394048.0000\n",
      "Epoch 73/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1116533760.0000 - val_loss: 1531975168.0000\n",
      "Epoch 74/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1115476480.0000 - val_loss: 1537012480.0000\n",
      "Epoch 75/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1114795392.0000 - val_loss: 1541759744.0000\n",
      "Epoch 76/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1114959744.0000 - val_loss: 1542776704.0000\n",
      "Epoch 77/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1114764928.0000 - val_loss: 1539645568.0000\n",
      "Epoch 78/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1113904000.0000 - val_loss: 1535327744.0000\n",
      "Epoch 79/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1113412224.0000 - val_loss: 1532701184.0000\n",
      "Epoch 80/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1113435392.0000 - val_loss: 1532769280.0000\n",
      "Epoch 81/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1113092992.0000 - val_loss: 1535291520.0000\n",
      "Epoch 82/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1112375296.0000 - val_loss: 1539018752.0000\n",
      "Epoch 83/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1112018816.0000 - val_loss: 1541520000.0000\n",
      "Epoch 84/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1111917184.0000 - val_loss: 1541006080.0000\n",
      "Epoch 85/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1111477376.0000 - val_loss: 1538234880.0000\n",
      "Epoch 86/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1110890880.0000 - val_loss: 1535522560.0000\n",
      "Epoch 87/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1110618496.0000 - val_loss: 1534543616.0000\n",
      "Epoch 88/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1110400640.0000 - val_loss: 1535648768.0000\n",
      "Epoch 89/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1109912576.0000 - val_loss: 1538178560.0000\n",
      "Epoch 90/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1109451008.0000 - val_loss: 1540507776.0000\n",
      "Epoch 91/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1109204096.0000 - val_loss: 1540981504.0000\n",
      "Epoch 92/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1108873216.0000 - val_loss: 1539484032.0000\n",
      "Epoch 93/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1108390016.0000 - val_loss: 1537462016.0000\n",
      "Epoch 94/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1108026496.0000 - val_loss: 1536433024.0000\n",
      "Epoch 95/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1107753344.0000 - val_loss: 1536977152.0000\n",
      "Epoch 96/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1107354752.0000 - val_loss: 1538734336.0000\n",
      "Epoch 97/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1106921984.0000 - val_loss: 1540556288.0000\n",
      "Epoch 98/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1106601728.0000 - val_loss: 1541175040.0000\n",
      "Epoch 99/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1106270720.0000 - val_loss: 1540268544.0000\n",
      "Epoch 100/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1105848960.0000 - val_loss: 1538798592.0000\n",
      "Epoch 101/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1105471104.0000 - val_loss: 1537972352.0000\n",
      "Epoch 102/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1105150208.0000 - val_loss: 1538361216.0000\n",
      "Epoch 103/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1104765440.0000 - val_loss: 1539679488.0000\n",
      "Epoch 104/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1104361216.0000 - val_loss: 1540995328.0000\n",
      "Epoch 105/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1104018048.0000 - val_loss: 1541365248.0000\n",
      "Epoch 106/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1103659520.0000 - val_loss: 1540671744.0000\n",
      "Epoch 107/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1103259008.0000 - val_loss: 1539668864.0000\n",
      "Epoch 108/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1102891776.0000 - val_loss: 1539258880.0000\n",
      "Epoch 109/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1102542848.0000 - val_loss: 1539773312.0000\n",
      "Epoch 110/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1102161792.0000 - val_loss: 1540832640.0000\n",
      "Epoch 111/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1101789056.0000 - val_loss: 1541684736.0000\n",
      "Epoch 112/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1101442432.0000 - val_loss: 1541737216.0000\n",
      "Epoch 113/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1101074944.0000 - val_loss: 1541153920.0000\n",
      "Epoch 114/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1100694144.0000 - val_loss: 1540568832.0000\n",
      "Epoch 115/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1100328192.0000 - val_loss: 1540500224.0000\n",
      "Epoch 116/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1099961856.0000 - val_loss: 1541016832.0000\n",
      "Epoch 117/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1099587072.0000 - val_loss: 1541738624.0000\n",
      "Epoch 118/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1099219200.0000 - val_loss: 1542161792.0000\n",
      "Epoch 119/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1098854400.0000 - val_loss: 1542069632.0000\n",
      "Epoch 120/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1098478208.0000 - val_loss: 1541718400.0000\n",
      "Epoch 121/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1098104832.0000 - val_loss: 1541590656.0000\n",
      "Epoch 122/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1097735040.0000 - val_loss: 1541924480.0000\n",
      "Epoch 123/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1097359616.0000 - val_loss: 1542532224.0000\n",
      "Epoch 124/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1096985728.0000 - val_loss: 1542982784.0000\n",
      "Epoch 125/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1096615296.0000 - val_loss: 1543036160.0000\n",
      "Epoch 126/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1096239744.0000 - val_loss: 1542839808.0000\n",
      "Epoch 127/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1095864064.0000 - val_loss: 1542706432.0000\n",
      "Epoch 128/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1095490048.0000 - val_loss: 1542855808.0000\n",
      "Epoch 129/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1095111680.0000 - val_loss: 1543249280.0000\n",
      "Epoch 130/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1094733312.0000 - val_loss: 1543668480.0000\n",
      "Epoch 131/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1094354304.0000 - val_loss: 1543873920.0000\n",
      "Epoch 132/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1093975424.0000 - val_loss: 1543866624.0000\n",
      "Epoch 133/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1093594240.0000 - val_loss: 1543828864.0000\n",
      "Epoch 134/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1093214592.0000 - val_loss: 1543916928.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 135/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1092834176.0000 - val_loss: 1544153984.0000\n",
      "Epoch 136/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1092452096.0000 - val_loss: 1544417024.0000\n",
      "Epoch 137/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1092067968.0000 - val_loss: 1544593024.0000\n",
      "Epoch 138/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1091683584.0000 - val_loss: 1544639360.0000\n",
      "Epoch 139/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1091300352.0000 - val_loss: 1544584704.0000\n",
      "Epoch 140/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1090917248.0000 - val_loss: 1544582400.0000\n",
      "Epoch 141/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1090533504.0000 - val_loss: 1544726784.0000\n",
      "Epoch 142/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1090147456.0000 - val_loss: 1544897024.0000\n",
      "Epoch 143/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1089762816.0000 - val_loss: 1544985472.0000\n",
      "Epoch 144/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1089375872.0000 - val_loss: 1544980096.0000\n",
      "Epoch 145/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1088992000.0000 - val_loss: 1544973952.0000\n",
      "Epoch 146/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1088608640.0000 - val_loss: 1545092096.0000\n",
      "Epoch 147/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1088221184.0000 - val_loss: 1545325952.0000\n",
      "Epoch 148/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1087837440.0000 - val_loss: 1545464576.0000\n",
      "Epoch 149/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1087454720.0000 - val_loss: 1545465472.0000\n",
      "Epoch 150/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1087061632.0000 - val_loss: 1545502336.0000\n",
      "Epoch 151/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1086673024.0000 - val_loss: 1545706112.0000\n",
      "Epoch 152/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1086286848.0000 - val_loss: 1545942784.0000\n",
      "Epoch 153/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1085898624.0000 - val_loss: 1546096640.0000\n",
      "Epoch 154/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1085505792.0000 - val_loss: 1546110208.0000\n",
      "Epoch 155/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1085115008.0000 - val_loss: 1546101376.0000\n",
      "Epoch 156/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1084719744.0000 - val_loss: 1546139392.0000\n",
      "Epoch 157/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1084330112.0000 - val_loss: 1546340096.0000\n",
      "Epoch 158/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1083935872.0000 - val_loss: 1546668928.0000\n",
      "Epoch 159/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1083541376.0000 - val_loss: 1546893952.0000\n",
      "Epoch 160/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1083145088.0000 - val_loss: 1546878848.0000\n",
      "Epoch 161/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1082749056.0000 - val_loss: 1546844800.0000\n",
      "Epoch 162/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1082354560.0000 - val_loss: 1546964352.0000\n",
      "Epoch 163/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1081954816.0000 - val_loss: 1547153152.0000\n",
      "Epoch 164/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1081554560.0000 - val_loss: 1547352448.0000\n",
      "Epoch 165/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1081158144.0000 - val_loss: 1547574912.0000\n",
      "Epoch 166/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1080755840.0000 - val_loss: 1547602176.0000\n",
      "Epoch 167/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1080356608.0000 - val_loss: 1547493248.0000\n",
      "Epoch 168/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1079958912.0000 - val_loss: 1547511168.0000\n",
      "Epoch 169/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1079557504.0000 - val_loss: 1547863424.0000\n",
      "Epoch 170/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1079153024.0000 - val_loss: 1548114688.0000\n",
      "Epoch 171/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1078750592.0000 - val_loss: 1548080640.0000\n",
      "Epoch 172/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1078348032.0000 - val_loss: 1547987712.0000\n",
      "Epoch 173/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1077944320.0000 - val_loss: 1548098560.0000\n",
      "Epoch 174/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1077544832.0000 - val_loss: 1548385280.0000\n",
      "Epoch 175/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1077135360.0000 - val_loss: 1548547840.0000\n",
      "Epoch 176/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1076731776.0000 - val_loss: 1548480256.0000\n",
      "Epoch 177/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1076327040.0000 - val_loss: 1548373376.0000\n",
      "Epoch 178/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1075916672.0000 - val_loss: 1548497664.0000\n",
      "Epoch 179/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1075504896.0000 - val_loss: 1548784384.0000\n",
      "Epoch 180/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1075099264.0000 - val_loss: 1549007488.0000\n",
      "Epoch 181/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1074684288.0000 - val_loss: 1548957952.0000\n",
      "Epoch 182/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1074273920.0000 - val_loss: 1548805888.0000\n",
      "Epoch 183/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1073864320.0000 - val_loss: 1548789760.0000\n",
      "Epoch 184/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1073448512.0000 - val_loss: 1549099392.0000\n",
      "Epoch 185/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1073036928.0000 - val_loss: 1549316224.0000\n",
      "Epoch 186/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1072616768.0000 - val_loss: 1549341952.0000\n",
      "Epoch 187/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1072206912.0000 - val_loss: 1549194624.0000\n",
      "Epoch 188/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1071787008.0000 - val_loss: 1549187712.0000\n",
      "Epoch 189/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1071361792.0000 - val_loss: 1549429888.0000\n",
      "Epoch 190/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1070946624.0000 - val_loss: 1549814144.0000\n",
      "Epoch 191/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1070527296.0000 - val_loss: 1549852416.0000\n",
      "Epoch 192/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1070111104.0000 - val_loss: 1549678592.0000\n",
      "Epoch 193/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1069685120.0000 - val_loss: 1549433856.0000\n",
      "Epoch 194/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1069269376.0000 - val_loss: 1549524608.0000\n",
      "Epoch 195/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1068843520.0000 - val_loss: 1549852800.0000\n",
      "Epoch 196/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1068420480.0000 - val_loss: 1549939200.0000\n",
      "Epoch 197/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1067996608.0000 - val_loss: 1549796736.0000\n",
      "Epoch 198/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1067581888.0000 - val_loss: 1549706752.0000\n",
      "Epoch 199/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1067148928.0000 - val_loss: 1549896832.0000\n",
      "Epoch 200/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1066734528.0000 - val_loss: 1550089216.0000\n",
      "Epoch 201/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 23us/step - loss: 1066299008.0000 - val_loss: 1550261504.0000\n",
      "Epoch 202/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1065885952.0000 - val_loss: 1550321536.0000\n",
      "Epoch 203/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1065440256.0000 - val_loss: 1550252800.0000\n",
      "Epoch 204/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1065030144.0000 - val_loss: 1550147840.0000\n",
      "Epoch 205/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1064588544.0000 - val_loss: 1550418688.0000\n",
      "Epoch 206/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1064143360.0000 - val_loss: 1550566144.0000\n",
      "Epoch 207/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1063709952.0000 - val_loss: 1550337920.0000\n",
      "Epoch 208/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1063270912.0000 - val_loss: 1549831168.0000\n",
      "Epoch 209/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1062834944.0000 - val_loss: 1549871360.0000\n",
      "Epoch 210/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1062388800.0000 - val_loss: 1550340224.0000\n",
      "Epoch 211/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1061950848.0000 - val_loss: 1550394368.0000\n",
      "Epoch 212/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1061498752.0000 - val_loss: 1549834368.0000\n",
      "Epoch 213/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1061065024.0000 - val_loss: 1549678592.0000\n",
      "Epoch 214/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1060600704.0000 - val_loss: 1549958656.0000\n",
      "Epoch 215/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1060177024.0000 - val_loss: 1549650688.0000\n",
      "Epoch 216/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1059712384.0000 - val_loss: 1549311872.0000\n",
      "Epoch 217/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1059269824.0000 - val_loss: 1549609216.0000\n",
      "Epoch 218/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1058815360.0000 - val_loss: 1549931136.0000\n",
      "Epoch 219/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1058379584.0000 - val_loss: 1549513856.0000\n",
      "Epoch 220/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1057918400.0000 - val_loss: 1549462272.0000\n",
      "Epoch 221/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1057474240.0000 - val_loss: 1549823104.0000\n",
      "Epoch 222/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1057020928.0000 - val_loss: 1549471872.0000\n",
      "Epoch 223/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1056554496.0000 - val_loss: 1549304448.0000\n",
      "Epoch 224/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1056118848.0000 - val_loss: 1550024320.0000\n",
      "Epoch 225/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1055639616.0000 - val_loss: 1550038656.0000\n",
      "Epoch 226/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1055188480.0000 - val_loss: 1549401344.0000\n",
      "Epoch 227/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1054722752.0000 - val_loss: 1549286016.0000\n",
      "Epoch 228/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1054282112.0000 - val_loss: 1550017920.0000\n",
      "Epoch 229/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1053818176.0000 - val_loss: 1550407168.0000\n",
      "Epoch 230/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1053352384.0000 - val_loss: 1549699072.0000\n",
      "Epoch 231/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1052881664.0000 - val_loss: 1549538176.0000\n",
      "Epoch 232/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1052428416.0000 - val_loss: 1550221056.0000\n",
      "Epoch 233/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1051957184.0000 - val_loss: 1550623360.0000\n",
      "Epoch 234/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1051485504.0000 - val_loss: 1550425344.0000\n",
      "Epoch 235/500\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 1051017152.0000 - val_loss: 1550171648.0000\n",
      "Epoch 236/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1050542656.0000 - val_loss: 1550053376.0000\n",
      "Epoch 237/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1050064768.0000 - val_loss: 1550525184.0000\n",
      "Epoch 238/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1049607104.0000 - val_loss: 1550788096.0000\n",
      "Epoch 239/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1049119552.0000 - val_loss: 1550608512.0000\n",
      "Epoch 240/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1048658880.0000 - val_loss: 1549946112.0000\n",
      "Epoch 241/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1048161280.0000 - val_loss: 1550501248.0000\n",
      "Epoch 242/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1047709056.0000 - val_loss: 1551430144.0000\n",
      "Epoch 243/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1047198528.0000 - val_loss: 1551222144.0000\n",
      "Epoch 244/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1046737600.0000 - val_loss: 1549852672.0000\n",
      "Epoch 245/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1046244096.0000 - val_loss: 1550451840.0000\n",
      "Epoch 246/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1045724928.0000 - val_loss: 1551900416.0000\n",
      "Epoch 247/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1045259456.0000 - val_loss: 1550931584.0000\n",
      "Epoch 248/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1044731776.0000 - val_loss: 1549985152.0000\n",
      "Epoch 249/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1044273792.0000 - val_loss: 1550942976.0000\n",
      "Epoch 250/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1043760704.0000 - val_loss: 1551659904.0000\n",
      "Epoch 251/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1043251712.0000 - val_loss: 1551117568.0000\n",
      "Epoch 252/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1042743296.0000 - val_loss: 1550298624.0000\n",
      "Epoch 253/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1042248832.0000 - val_loss: 1551287424.0000\n",
      "Epoch 254/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1041744128.0000 - val_loss: 1551973376.0000\n",
      "Epoch 255/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1041238656.0000 - val_loss: 1550661888.0000\n",
      "Epoch 256/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1040703104.0000 - val_loss: 1550227584.0000\n",
      "Epoch 257/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1040202112.0000 - val_loss: 1551755776.0000\n",
      "Epoch 258/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1039658944.0000 - val_loss: 1552154624.0000\n",
      "Epoch 259/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1039158656.0000 - val_loss: 1551213440.0000\n",
      "Epoch 260/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1038604864.0000 - val_loss: 1550432640.0000\n",
      "Epoch 261/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1038099456.0000 - val_loss: 1551033856.0000\n",
      "Epoch 262/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1037552768.0000 - val_loss: 1551750528.0000\n",
      "Epoch 263/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1037017792.0000 - val_loss: 1551575168.0000\n",
      "Epoch 264/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1036498240.0000 - val_loss: 1551593472.0000\n",
      "Epoch 265/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1035948480.0000 - val_loss: 1551006208.0000\n",
      "Epoch 266/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1035409472.0000 - val_loss: 1551031680.0000\n",
      "Epoch 267/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1034871104.0000 - val_loss: 1552285568.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 268/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1034338816.0000 - val_loss: 1551852544.0000\n",
      "Epoch 269/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1033789760.0000 - val_loss: 1551555072.0000\n",
      "Epoch 270/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1033247232.0000 - val_loss: 1551652736.0000\n",
      "Epoch 271/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1032713152.0000 - val_loss: 1552035840.0000\n",
      "Epoch 272/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1032150464.0000 - val_loss: 1551839744.0000\n",
      "Epoch 273/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1031638976.0000 - val_loss: 1551774592.0000\n",
      "Epoch 274/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1031075072.0000 - val_loss: 1552515456.0000\n",
      "Epoch 275/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1030481088.0000 - val_loss: 1552660224.0000\n",
      "Epoch 276/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1029945664.0000 - val_loss: 1551287680.0000\n",
      "Epoch 277/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1029357952.0000 - val_loss: 1552052352.0000\n",
      "Epoch 278/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1028801472.0000 - val_loss: 1553126912.0000\n",
      "Epoch 279/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1028243712.0000 - val_loss: 1551557376.0000\n",
      "Epoch 280/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1027664064.0000 - val_loss: 1551912064.0000\n",
      "Epoch 281/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1027069632.0000 - val_loss: 1553101952.0000\n",
      "Epoch 282/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1026517184.0000 - val_loss: 1552501376.0000\n",
      "Epoch 283/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1025926336.0000 - val_loss: 1552283648.0000\n",
      "Epoch 284/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1025359360.0000 - val_loss: 1552039680.0000\n",
      "Epoch 285/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 1024773056.0000 - val_loss: 1552379648.0000\n",
      "Epoch 286/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1024177664.0000 - val_loss: 1554115200.0000\n",
      "Epoch 287/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1023593600.0000 - val_loss: 1553287808.0000\n",
      "Epoch 288/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1023017280.0000 - val_loss: 1551031808.0000\n",
      "Epoch 289/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1022447040.0000 - val_loss: 1553470336.0000\n",
      "Epoch 290/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1021799808.0000 - val_loss: 1553550464.0000\n",
      "Epoch 291/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1021173632.0000 - val_loss: 1551272192.0000\n",
      "Epoch 292/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1020616128.0000 - val_loss: 1552791296.0000\n",
      "Epoch 293/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1019958976.0000 - val_loss: 1553024768.0000\n",
      "Epoch 294/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1019319360.0000 - val_loss: 1552217216.0000\n",
      "Epoch 295/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1018717568.0000 - val_loss: 1552578432.0000\n",
      "Epoch 296/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1018103424.0000 - val_loss: 1551786880.0000\n",
      "Epoch 297/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1017472064.0000 - val_loss: 1553390720.0000\n",
      "Epoch 298/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1016875840.0000 - val_loss: 1551772160.0000\n",
      "Epoch 299/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1016199616.0000 - val_loss: 1550721664.0000\n",
      "Epoch 300/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1015605440.0000 - val_loss: 1552259712.0000\n",
      "Epoch 301/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1015004800.0000 - val_loss: 1551334912.0000\n",
      "Epoch 302/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1014322496.0000 - val_loss: 1551237632.0000\n",
      "Epoch 303/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1013721536.0000 - val_loss: 1552465792.0000\n",
      "Epoch 304/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1013080576.0000 - val_loss: 1550281984.0000\n",
      "Epoch 305/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1012453184.0000 - val_loss: 1550836864.0000\n",
      "Epoch 306/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1011788032.0000 - val_loss: 1552313984.0000\n",
      "Epoch 307/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1011154368.0000 - val_loss: 1550958080.0000\n",
      "Epoch 308/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 1010471744.0000 - val_loss: 1550809088.0000\n",
      "Epoch 309/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1009826304.0000 - val_loss: 1550825216.0000\n",
      "Epoch 310/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1009165888.0000 - val_loss: 1550620928.0000\n",
      "Epoch 311/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1008500736.0000 - val_loss: 1551626496.0000\n",
      "Epoch 312/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1007831616.0000 - val_loss: 1551369600.0000\n",
      "Epoch 313/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1007098240.0000 - val_loss: 1549974144.0000\n",
      "Epoch 314/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1006454592.0000 - val_loss: 1549961984.0000\n",
      "Epoch 315/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1005734144.0000 - val_loss: 1550527616.0000\n",
      "Epoch 316/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1005025664.0000 - val_loss: 1549382784.0000\n",
      "Epoch 317/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1004370944.0000 - val_loss: 1550929920.0000\n",
      "Epoch 318/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1003633280.0000 - val_loss: 1550134400.0000\n",
      "Epoch 319/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1002917248.0000 - val_loss: 1549490048.0000\n",
      "Epoch 320/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1002239488.0000 - val_loss: 1550019072.0000\n",
      "Epoch 321/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 1001530368.0000 - val_loss: 1549691520.0000\n",
      "Epoch 322/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1000804672.0000 - val_loss: 1547894272.0000\n",
      "Epoch 323/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 1000137216.0000 - val_loss: 1548992640.0000\n",
      "Epoch 324/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 999410048.0000 - val_loss: 1548114816.0000\n",
      "Epoch 325/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 998680384.0000 - val_loss: 1550184960.0000\n",
      "Epoch 326/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 997922304.0000 - val_loss: 1548892544.0000\n",
      "Epoch 327/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 997177728.0000 - val_loss: 1547616768.0000\n",
      "Epoch 328/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 996470720.0000 - val_loss: 1548757632.0000\n",
      "Epoch 329/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 995697216.0000 - val_loss: 1548401664.0000\n",
      "Epoch 330/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 995012352.0000 - val_loss: 1549260800.0000\n",
      "Epoch 331/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 994237888.0000 - val_loss: 1547998080.0000\n",
      "Epoch 332/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 993482304.0000 - val_loss: 1546984064.0000\n",
      "Epoch 333/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 992709696.0000 - val_loss: 1548721536.0000\n",
      "Epoch 334/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 992024256.0000 - val_loss: 1546772096.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 335/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 991357120.0000 - val_loss: 1551486208.0000\n",
      "Epoch 336/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 990576960.0000 - val_loss: 1546175744.0000\n",
      "Epoch 337/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 989744832.0000 - val_loss: 1547384576.0000\n",
      "Epoch 338/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 988879488.0000 - val_loss: 1548253824.0000\n",
      "Epoch 339/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 988051840.0000 - val_loss: 1547079936.0000\n",
      "Epoch 340/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 987360192.0000 - val_loss: 1551044352.0000\n",
      "Epoch 341/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 986618944.0000 - val_loss: 1545955072.0000\n",
      "Epoch 342/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 985743744.0000 - val_loss: 1547206016.0000\n",
      "Epoch 343/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 984921600.0000 - val_loss: 1546622464.0000\n",
      "Epoch 344/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 984016704.0000 - val_loss: 1547812864.0000\n",
      "Epoch 345/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 983218816.0000 - val_loss: 1548179712.0000\n",
      "Epoch 346/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 982391744.0000 - val_loss: 1546229376.0000\n",
      "Epoch 347/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 981567104.0000 - val_loss: 1547014144.0000\n",
      "Epoch 348/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 980760320.0000 - val_loss: 1546382464.0000\n",
      "Epoch 349/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 979896960.0000 - val_loss: 1549311360.0000\n",
      "Epoch 350/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 979140352.0000 - val_loss: 1545619712.0000\n",
      "Epoch 351/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 978235840.0000 - val_loss: 1547398784.0000\n",
      "Epoch 352/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 977445696.0000 - val_loss: 1544465664.0000\n",
      "Epoch 353/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 976611712.0000 - val_loss: 1550501632.0000\n",
      "Epoch 354/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 975802432.0000 - val_loss: 1544405120.0000\n",
      "Epoch 355/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 975246720.0000 - val_loss: 1553577216.0000\n",
      "Epoch 356/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 974798208.0000 - val_loss: 1540557440.0000\n",
      "Epoch 357/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 974411072.0000 - val_loss: 1553781120.0000\n",
      "Epoch 358/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 973363648.0000 - val_loss: 1542795008.0000\n",
      "Epoch 359/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 971805504.0000 - val_loss: 1548035584.0000\n",
      "Epoch 360/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 970407872.0000 - val_loss: 1549788544.0000\n",
      "Epoch 361/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 969701504.0000 - val_loss: 1541829632.0000\n",
      "Epoch 362/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 969339392.0000 - val_loss: 1552105088.0000\n",
      "Epoch 363/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 968637056.0000 - val_loss: 1541005440.0000\n",
      "Epoch 364/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 967528320.0000 - val_loss: 1548590080.0000\n",
      "Epoch 365/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 966141376.0000 - val_loss: 1545437440.0000\n",
      "Epoch 366/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 964977088.0000 - val_loss: 1542074752.0000\n",
      "Epoch 367/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 964331072.0000 - val_loss: 1550492928.0000\n",
      "Epoch 368/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 963944576.0000 - val_loss: 1538586496.0000\n",
      "Epoch 369/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 963399040.0000 - val_loss: 1551294464.0000\n",
      "Epoch 370/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 962558208.0000 - val_loss: 1538958336.0000\n",
      "Epoch 371/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 961140608.0000 - val_loss: 1546636544.0000\n",
      "Epoch 372/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 959487232.0000 - val_loss: 1544007168.0000\n",
      "Epoch 373/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 958350144.0000 - val_loss: 1540133376.0000\n",
      "Epoch 374/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 957745344.0000 - val_loss: 1549101696.0000\n",
      "Epoch 375/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 957241792.0000 - val_loss: 1537864832.0000\n",
      "Epoch 376/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 956845312.0000 - val_loss: 1552748800.0000\n",
      "Epoch 377/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 956097280.0000 - val_loss: 1537163648.0000\n",
      "Epoch 378/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 954558656.0000 - val_loss: 1545884800.0000\n",
      "Epoch 379/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 952712832.0000 - val_loss: 1541431168.0000\n",
      "Epoch 380/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 951286784.0000 - val_loss: 1538980352.0000\n",
      "Epoch 381/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 950572096.0000 - val_loss: 1549212672.0000\n",
      "Epoch 382/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 950291008.0000 - val_loss: 1533155072.0000\n",
      "Epoch 383/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 950169408.0000 - val_loss: 1551415040.0000\n",
      "Epoch 384/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 949815808.0000 - val_loss: 1531273856.0000\n",
      "Epoch 385/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 948741824.0000 - val_loss: 1548206720.0000\n",
      "Epoch 386/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 946474752.0000 - val_loss: 1535082880.0000\n",
      "Epoch 387/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 944147904.0000 - val_loss: 1539100288.0000\n",
      "Epoch 388/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 942518464.0000 - val_loss: 1541831680.0000\n",
      "Epoch 389/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 941741824.0000 - val_loss: 1532054528.0000\n",
      "Epoch 390/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 941794624.0000 - val_loss: 1550772992.0000\n",
      "Epoch 391/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 942425216.0000 - val_loss: 1529056384.0000\n",
      "Epoch 392/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 943398336.0000 - val_loss: 1557400704.0000\n",
      "Epoch 393/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 942873920.0000 - val_loss: 1527718016.0000\n",
      "Epoch 394/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 940919040.0000 - val_loss: 1547299712.0000\n",
      "Epoch 395/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 937144000.0000 - val_loss: 1531478528.0000\n",
      "Epoch 396/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 934038976.0000 - val_loss: 1532690432.0000\n",
      "Epoch 397/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 932539456.0000 - val_loss: 1541620608.0000\n",
      "Epoch 398/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 932641088.0000 - val_loss: 1526125568.0000\n",
      "Epoch 399/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 933581824.0000 - val_loss: 1551958912.0000\n",
      "Epoch 400/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 934125504.0000 - val_loss: 1524068352.0000\n",
      "Epoch 401/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 934420928.0000 - val_loss: 1551735680.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 402/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 933100416.0000 - val_loss: 1521411584.0000\n",
      "Epoch 403/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 930659392.0000 - val_loss: 1541016064.0000\n",
      "Epoch 404/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 927284032.0000 - val_loss: 1523885184.0000\n",
      "Epoch 405/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 924252800.0000 - val_loss: 1529503616.0000\n",
      "Epoch 406/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 922196288.0000 - val_loss: 1532018688.0000\n",
      "Epoch 407/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 921324672.0000 - val_loss: 1522319104.0000\n",
      "Epoch 408/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 921262272.0000 - val_loss: 1539541376.0000\n",
      "Epoch 409/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 921379904.0000 - val_loss: 1519012096.0000\n",
      "Epoch 410/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 921787008.0000 - val_loss: 1544723968.0000\n",
      "Epoch 411/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 921897536.0000 - val_loss: 1517151872.0000\n",
      "Epoch 412/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 922666304.0000 - val_loss: 1549603072.0000\n",
      "Epoch 413/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 922256704.0000 - val_loss: 1516641024.0000\n",
      "Epoch 414/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 921987840.0000 - val_loss: 1549262336.0000\n",
      "Epoch 415/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 919776832.0000 - val_loss: 1517362688.0000\n",
      "Epoch 416/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 917035328.0000 - val_loss: 1540020864.0000\n",
      "Epoch 417/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 913195200.0000 - val_loss: 1520357376.0000\n",
      "Epoch 418/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 910001536.0000 - val_loss: 1528333696.0000\n",
      "Epoch 419/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 907644416.0000 - val_loss: 1526151552.0000\n",
      "Epoch 420/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 906314560.0000 - val_loss: 1519414016.0000\n",
      "Epoch 421/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 905984320.0000 - val_loss: 1535106688.0000\n",
      "Epoch 422/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 906649664.0000 - val_loss: 1515318912.0000\n",
      "Epoch 423/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 908716096.0000 - val_loss: 1552789632.0000\n",
      "Epoch 424/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 912774848.0000 - val_loss: 1517667840.0000\n",
      "Epoch 425/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 921302912.0000 - val_loss: 1583701248.0000\n",
      "Epoch 426/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 929262272.0000 - val_loss: 1527865856.0000\n",
      "Epoch 427/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 941611136.0000 - val_loss: 1590770688.0000\n",
      "Epoch 428/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 932726848.0000 - val_loss: 1516275200.0000\n",
      "Epoch 429/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 917954304.0000 - val_loss: 1534761984.0000\n",
      "Epoch 430/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 899182848.0000 - val_loss: 1528146432.0000\n",
      "Epoch 431/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 896097984.0000 - val_loss: 1512067968.0000\n",
      "Epoch 432/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 906367040.0000 - val_loss: 1566540672.0000\n",
      "Epoch 433/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 913899712.0000 - val_loss: 1514221568.0000\n",
      "Epoch 434/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 913011712.0000 - val_loss: 1545047168.0000\n",
      "Epoch 435/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 899546560.0000 - val_loss: 1513790208.0000\n",
      "Epoch 436/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 889954752.0000 - val_loss: 1512878976.0000\n",
      "Epoch 437/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 889150464.0000 - val_loss: 1539884544.0000\n",
      "Epoch 438/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 894704576.0000 - val_loss: 1509941888.0000\n",
      "Epoch 439/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 901157632.0000 - val_loss: 1552551936.0000\n",
      "Epoch 440/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 900630144.0000 - val_loss: 1508157312.0000\n",
      "Epoch 441/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 896442368.0000 - val_loss: 1532480768.0000\n",
      "Epoch 442/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 888125248.0000 - val_loss: 1509535488.0000\n",
      "Epoch 443/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 882343104.0000 - val_loss: 1509994496.0000\n",
      "Epoch 444/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 880833088.0000 - val_loss: 1525056256.0000\n",
      "Epoch 445/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 882668224.0000 - val_loss: 1504306688.0000\n",
      "Epoch 446/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 885358400.0000 - val_loss: 1535768448.0000\n",
      "Epoch 447/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 885949504.0000 - val_loss: 1504480256.0000\n",
      "Epoch 448/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 884972864.0000 - val_loss: 1531883520.0000\n",
      "Epoch 449/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 881155072.0000 - val_loss: 1506443904.0000\n",
      "Epoch 450/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 877497920.0000 - val_loss: 1521084544.0000\n",
      "Epoch 451/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 874079360.0000 - val_loss: 1511823488.0000\n",
      "Epoch 452/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 871820992.0000 - val_loss: 1511398912.0000\n",
      "Epoch 453/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 870670464.0000 - val_loss: 1519317120.0000\n",
      "Epoch 454/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 870440448.0000 - val_loss: 1506504192.0000\n",
      "Epoch 455/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 871062720.0000 - val_loss: 1529813376.0000\n",
      "Epoch 456/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 872521728.0000 - val_loss: 1505667328.0000\n",
      "Epoch 457/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 876304320.0000 - val_loss: 1549431296.0000\n",
      "Epoch 458/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 881709632.0000 - val_loss: 1511793408.0000\n",
      "Epoch 459/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 892622464.0000 - val_loss: 1581593472.0000\n",
      "Epoch 460/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 902284928.0000 - val_loss: 1528394368.0000\n",
      "Epoch 461/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 919665024.0000 - val_loss: 1602731520.0000\n",
      "Epoch 462/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 915903104.0000 - val_loss: 1522651520.0000\n",
      "Epoch 463/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 908378496.0000 - val_loss: 1555567104.0000\n",
      "Epoch 464/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 879577728.0000 - val_loss: 1506546688.0000\n",
      "Epoch 465/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 860371840.0000 - val_loss: 1506085760.0000\n",
      "Epoch 466/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 860193280.0000 - val_loss: 1551247232.0000\n",
      "Epoch 467/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 873429952.0000 - val_loss: 1513606784.0000\n",
      "Epoch 468/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 886505216.0000 - val_loss: 1566424704.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 469/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 881507392.0000 - val_loss: 1506685056.0000\n",
      "Epoch 470/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 870114112.0000 - val_loss: 1524763264.0000\n",
      "Epoch 471/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 855606656.0000 - val_loss: 1512831872.0000\n",
      "Epoch 472/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 850968256.0000 - val_loss: 1502479744.0000\n",
      "Epoch 473/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 856532672.0000 - val_loss: 1544334080.0000\n",
      "Epoch 474/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 864668672.0000 - val_loss: 1506667648.0000\n",
      "Epoch 475/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 871491968.0000 - val_loss: 1551837952.0000\n",
      "Epoch 476/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 868209152.0000 - val_loss: 1503614720.0000\n",
      "Epoch 477/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 862043456.0000 - val_loss: 1527938048.0000\n",
      "Epoch 478/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 851618240.0000 - val_loss: 1503846912.0000\n",
      "Epoch 479/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 844742272.0000 - val_loss: 1506025344.0000\n",
      "Epoch 480/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 842773504.0000 - val_loss: 1521278848.0000\n",
      "Epoch 481/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 844956928.0000 - val_loss: 1502629376.0000\n",
      "Epoch 482/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 849269312.0000 - val_loss: 1539633280.0000\n",
      "Epoch 483/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 852193856.0000 - val_loss: 1505371776.0000\n",
      "Epoch 484/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 855169344.0000 - val_loss: 1545320320.0000\n",
      "Epoch 485/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 853637888.0000 - val_loss: 1504820992.0000\n",
      "Epoch 486/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 852026944.0000 - val_loss: 1536222976.0000\n",
      "Epoch 487/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 846699072.0000 - val_loss: 1501942784.0000\n",
      "Epoch 488/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 842007168.0000 - val_loss: 1520745984.0000\n",
      "Epoch 489/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 836992576.0000 - val_loss: 1503668480.0000\n",
      "Epoch 490/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 833373824.0000 - val_loss: 1509071488.0000\n",
      "Epoch 491/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 831215872.0000 - val_loss: 1512458112.0000\n",
      "Epoch 492/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 830492352.0000 - val_loss: 1504802816.0000\n",
      "Epoch 493/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 830906496.0000 - val_loss: 1524316544.0000\n",
      "Epoch 494/500\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 832404416.0000 - val_loss: 1503518336.0000\n",
      "Epoch 495/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 836208128.0000 - val_loss: 1543123584.0000\n",
      "Epoch 496/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 842242688.0000 - val_loss: 1511363456.0000\n",
      "Epoch 497/500\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 856685440.0000 - val_loss: 1589721728.0000\n",
      "Epoch 498/500\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 876201920.0000 - val_loss: 1553139456.0000\n",
      "Epoch 499/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 918812224.0000 - val_loss: 1663653888.0000\n",
      "Epoch 500/500\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 935757312.0000 - val_loss: 1580802432.0000\n"
     ]
    }
   ],
   "source": [
    "NN_500E_Adam_LReLU.compile(loss='mean_squared_error', optimizer='adam')\n",
    "history = NN_500E_Adam_LReLU.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=500,validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/1000\n",
      "1164/1164 [==============================] - 0s 131us/step - loss: 195837.5156 - val_loss: 190412.3750\n",
      "Epoch 2/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 190527.8750 - val_loss: 184706.3750\n",
      "Epoch 3/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 184732.1250 - val_loss: 177407.1562\n",
      "Epoch 4/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 177498.1562 - val_loss: 167916.0156\n",
      "Epoch 5/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 168439.4844 - val_loss: 155818.9531\n",
      "Epoch 6/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 157591.8281 - val_loss: 140952.8438\n",
      "Epoch 7/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 145666.6094 - val_loss: 124073.9453\n",
      "Epoch 8/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 134824.0000 - val_loss: 106170.7656\n",
      "Epoch 9/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 128423.9453 - val_loss: 92611.5234\n",
      "Epoch 10/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 130806.8281 - val_loss: 87430.5781\n",
      "Epoch 11/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 137014.7969 - val_loss: 85986.1875\n",
      "Epoch 12/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 138097.3125 - val_loss: 85117.2734\n",
      "Epoch 13/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 133693.3438 - val_loss: 85596.0781\n",
      "Epoch 14/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 127033.9375 - val_loss: 88464.7734\n",
      "Epoch 15/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 121230.0234 - val_loss: 93362.5000\n",
      "Epoch 16/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 117958.8047 - val_loss: 98759.1953\n",
      "Epoch 17/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 117054.9844 - val_loss: 103037.9453\n",
      "Epoch 18/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 117208.1953 - val_loss: 105162.0547\n",
      "Epoch 19/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 117052.7422 - val_loss: 104748.2422\n",
      "Epoch 20/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 115775.9453 - val_loss: 101846.5625\n",
      "Epoch 21/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 113158.7578 - val_loss: 96788.2500\n",
      "Epoch 22/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 109441.0625 - val_loss: 90177.4766\n",
      "Epoch 23/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 105234.7891 - val_loss: 82864.9297\n",
      "Epoch 24/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 101335.0078 - val_loss: 76212.9766\n",
      "Epoch 25/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 98471.0234 - val_loss: 71150.7422\n",
      "Epoch 26/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 96536.0156 - val_loss: 67809.4922\n",
      "Epoch 27/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 94519.9219 - val_loss: 65621.6406\n",
      "Epoch 28/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 91312.2578 - val_loss: 64037.8438\n",
      "Epoch 29/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 86607.3359 - val_loss: 63191.2617\n",
      "Epoch 30/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 81187.7109 - val_loss: 63264.2734\n",
      "Epoch 31/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 76233.0312 - val_loss: 63644.9805\n",
      "Epoch 32/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 72250.6875 - val_loss: 62878.3906\n",
      "Epoch 33/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 68465.6875 - val_loss: 59784.2188\n",
      "Epoch 34/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 63618.3555 - val_loss: 54840.3672\n",
      "Epoch 35/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 57680.3789 - val_loss: 50937.7852\n",
      "Epoch 36/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 52844.4648 - val_loss: 50889.1328\n",
      "Epoch 37/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 51397.8867 - val_loss: 52113.2227\n",
      "Epoch 38/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 51382.3672 - val_loss: 51735.1289\n",
      "Epoch 39/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 51739.4648 - val_loss: 51559.3672\n",
      "Epoch 40/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 54624.1016 - val_loss: 52086.3711\n",
      "Epoch 41/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 56960.0039 - val_loss: 52702.7773\n",
      "Epoch 42/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 56720.4023 - val_loss: 53845.6172\n",
      "Epoch 43/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 55830.5039 - val_loss: 54046.4062\n",
      "Epoch 44/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 54640.8711 - val_loss: 52072.9023\n",
      "Epoch 45/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 52325.6367 - val_loss: 49619.8789\n",
      "Epoch 46/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50056.3867 - val_loss: 48481.7383\n",
      "Epoch 47/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48994.8867 - val_loss: 48093.2227\n",
      "Epoch 48/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 48357.5117 - val_loss: 47670.0000\n",
      "Epoch 49/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47684.5117 - val_loss: 47465.2109\n",
      "Epoch 50/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47619.1523 - val_loss: 47500.8633\n",
      "Epoch 51/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48125.3828 - val_loss: 47357.2812\n",
      "Epoch 52/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 48385.5117 - val_loss: 47146.9961\n",
      "Epoch 53/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48304.8594 - val_loss: 47186.3867\n",
      "Epoch 54/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48290.1211 - val_loss: 47212.4062\n",
      "Epoch 55/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48221.1133 - val_loss: 46848.2617\n",
      "Epoch 56/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47763.1133 - val_loss: 46296.2461\n",
      "Epoch 57/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47088.6445 - val_loss: 45967.4609\n",
      "Epoch 58/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46553.1523 - val_loss: 45781.0391\n",
      "Epoch 59/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46057.8711 - val_loss: 45472.8789\n",
      "Epoch 60/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45453.5117 - val_loss: 45159.7812\n",
      "Epoch 61/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45012.8711 - val_loss: 44978.2109\n",
      "Epoch 62/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44859.5352 - val_loss: 44841.0430\n",
      "Epoch 63/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 44723.5430 - val_loss: 44830.2109\n",
      "Epoch 64/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44589.0117 - val_loss: 44995.0039\n",
      "Epoch 65/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44602.1641 - val_loss: 44984.9961\n",
      "Epoch 66/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44569.0117 - val_loss: 44637.2461\n",
      "Epoch 67/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44368.8359 - val_loss: 44248.0391\n",
      "Epoch 68/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44170.3672 - val_loss: 43985.2188\n",
      "Epoch 69/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43943.0117 - val_loss: 43806.3789\n",
      "Epoch 70/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43600.3633 - val_loss: 43717.0859\n",
      "Epoch 71/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43298.9570 - val_loss: 43589.9141\n",
      "Epoch 72/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43078.1641 - val_loss: 43304.4570\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42844.3711 - val_loss: 43018.0312\n",
      "Epoch 74/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42673.4570 - val_loss: 42846.4180\n",
      "Epoch 75/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42584.4648 - val_loss: 42721.2188\n",
      "Epoch 76/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42471.1289 - val_loss: 42630.7617\n",
      "Epoch 77/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42357.5469 - val_loss: 42552.6211\n",
      "Epoch 78/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42262.8711 - val_loss: 42404.9180\n",
      "Epoch 79/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42111.8633 - val_loss: 42227.5898\n",
      "Epoch 80/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41938.8633 - val_loss: 42083.0859\n",
      "Epoch 81/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41781.1680 - val_loss: 41966.4180\n",
      "Epoch 82/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41599.0117 - val_loss: 41905.0352\n",
      "Epoch 83/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41430.5352 - val_loss: 41864.6133\n",
      "Epoch 84/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 41301.9961 - val_loss: 41754.8633\n",
      "Epoch 85/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41173.6445 - val_loss: 41614.9961\n",
      "Epoch 86/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41070.4883 - val_loss: 41518.9102\n",
      "Epoch 87/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40985.9648 - val_loss: 41480.7539\n",
      "Epoch 88/1000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 40883.9883 - val_loss: 41474.0039\n",
      "Epoch 89/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40788.4570 - val_loss: 41404.1211\n",
      "Epoch 90/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40684.1445 - val_loss: 41256.6680\n",
      "Epoch 91/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40565.0430 - val_loss: 41125.6211\n",
      "Epoch 92/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40458.6758 - val_loss: 41050.3711\n",
      "Epoch 93/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40350.3242 - val_loss: 41014.8398\n",
      "Epoch 94/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40252.9961 - val_loss: 40961.6211\n",
      "Epoch 95/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40173.5469 - val_loss: 40861.8789\n",
      "Epoch 96/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40094.9570 - val_loss: 40771.6289\n",
      "Epoch 97/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40028.9570 - val_loss: 40721.7383\n",
      "Epoch 98/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39960.9648 - val_loss: 40704.4180\n",
      "Epoch 99/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39890.3203 - val_loss: 40682.5117\n",
      "Epoch 100/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39822.3594 - val_loss: 40626.6289\n",
      "Epoch 101/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39748.9570 - val_loss: 40574.3867\n",
      "Epoch 102/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39683.1445 - val_loss: 40558.7031\n",
      "Epoch 103/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39620.0117 - val_loss: 40574.5039\n",
      "Epoch 104/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39563.9883 - val_loss: 40575.1602\n",
      "Epoch 105/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39515.8164 - val_loss: 40538.3789\n",
      "Epoch 106/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39468.9961 - val_loss: 40508.6133\n",
      "Epoch 107/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39427.8203 - val_loss: 40511.2070\n",
      "Epoch 108/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39384.4492 - val_loss: 40522.0039\n",
      "Epoch 109/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39343.6797 - val_loss: 40498.1289\n",
      "Epoch 110/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39302.8555 - val_loss: 40454.8867\n",
      "Epoch 111/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39264.6719 - val_loss: 40434.7070\n",
      "Epoch 112/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39230.4492 - val_loss: 40439.4102\n",
      "Epoch 113/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39198.6719 - val_loss: 40435.2461\n",
      "Epoch 114/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39170.9961 - val_loss: 40407.9180\n",
      "Epoch 115/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39143.8164 - val_loss: 40390.4570\n",
      "Epoch 116/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39119.1406 - val_loss: 40399.3320\n",
      "Epoch 117/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39093.1406 - val_loss: 40413.5039\n",
      "Epoch 118/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39068.9609 - val_loss: 40407.4180\n",
      "Epoch 119/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39044.3516 - val_loss: 40398.5430\n",
      "Epoch 120/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39022.1836 - val_loss: 40410.4570\n",
      "Epoch 121/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39001.3516 - val_loss: 40429.3711\n",
      "Epoch 122/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38982.3555 - val_loss: 40426.8320\n",
      "Epoch 123/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38963.9570 - val_loss: 40413.2461\n",
      "Epoch 124/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38945.8594 - val_loss: 40414.6289\n",
      "Epoch 125/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38927.4961 - val_loss: 40423.7070\n",
      "Epoch 126/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38908.9883 - val_loss: 40412.5820\n",
      "Epoch 127/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38890.0117 - val_loss: 40392.6367\n",
      "Epoch 128/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38871.9609 - val_loss: 40388.0117\n",
      "Epoch 129/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38854.0391 - val_loss: 40391.7383\n",
      "Epoch 130/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38836.8516 - val_loss: 40380.5469\n",
      "Epoch 131/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38819.4844 - val_loss: 40363.7969\n",
      "Epoch 132/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38803.0391 - val_loss: 40365.2539\n",
      "Epoch 133/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38787.3555 - val_loss: 40373.7070\n",
      "Epoch 134/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38773.3516 - val_loss: 40364.9648\n",
      "Epoch 135/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38760.3242 - val_loss: 40353.2969\n",
      "Epoch 136/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38747.8242 - val_loss: 40357.7930\n",
      "Epoch 137/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38735.0391 - val_loss: 40361.2383\n",
      "Epoch 138/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38722.6445 - val_loss: 40347.7070\n",
      "Epoch 139/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38709.5508 - val_loss: 40335.1133\n",
      "Epoch 140/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38696.3555 - val_loss: 40334.1211\n",
      "Epoch 141/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38682.5039 - val_loss: 40326.9180\n",
      "Epoch 142/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38668.6875 - val_loss: 40306.7383\n",
      "Epoch 143/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38655.1406 - val_loss: 40295.4961\n",
      "Epoch 144/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38641.6758 - val_loss: 40292.7383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 145/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38628.1875 - val_loss: 40280.3711\n",
      "Epoch 146/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38614.6523 - val_loss: 40264.7148\n",
      "Epoch 147/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38601.0430 - val_loss: 40261.2461\n",
      "Epoch 148/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38587.5156 - val_loss: 40259.4961\n",
      "Epoch 149/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38574.1523 - val_loss: 40248.4102\n",
      "Epoch 150/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38561.1875 - val_loss: 40240.9570\n",
      "Epoch 151/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38548.8242 - val_loss: 40241.2969\n",
      "Epoch 152/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38536.5156 - val_loss: 40233.2383\n",
      "Epoch 153/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38524.3164 - val_loss: 40223.2109\n",
      "Epoch 154/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38512.1875 - val_loss: 40221.6211\n",
      "Epoch 155/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38500.1523 - val_loss: 40218.0000\n",
      "Epoch 156/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38488.3164 - val_loss: 40207.6719\n",
      "Epoch 157/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38476.3242 - val_loss: 40204.7617\n",
      "Epoch 158/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38464.1875 - val_loss: 40202.2461\n",
      "Epoch 159/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38452.1406 - val_loss: 40195.1211\n",
      "Epoch 160/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38440.0391 - val_loss: 40192.0039\n",
      "Epoch 161/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38428.3203 - val_loss: 40192.6289\n",
      "Epoch 162/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38416.6875 - val_loss: 40188.2930\n",
      "Epoch 163/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38405.1875 - val_loss: 40184.4570\n",
      "Epoch 164/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38394.0039 - val_loss: 40184.7070\n",
      "Epoch 165/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38382.8594 - val_loss: 40182.0859\n",
      "Epoch 166/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38371.8125 - val_loss: 40178.8594\n",
      "Epoch 167/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38360.6758 - val_loss: 40175.9883\n",
      "Epoch 168/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38349.5508 - val_loss: 40174.9648\n",
      "Epoch 169/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38338.5391 - val_loss: 40170.1406\n",
      "Epoch 170/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38327.6523 - val_loss: 40167.9219\n",
      "Epoch 171/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38316.8125 - val_loss: 40166.6680\n",
      "Epoch 172/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38305.9531 - val_loss: 40161.7383\n",
      "Epoch 173/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38295.0391 - val_loss: 40156.8281\n",
      "Epoch 174/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38284.1406 - val_loss: 40156.9219\n",
      "Epoch 175/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38273.1758 - val_loss: 40150.7500\n",
      "Epoch 176/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38262.3125 - val_loss: 40146.7344\n",
      "Epoch 177/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38251.4609 - val_loss: 40145.9570\n",
      "Epoch 178/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38240.6875 - val_loss: 40138.6680\n",
      "Epoch 179/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38230.0469 - val_loss: 40138.5156\n",
      "Epoch 180/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38219.4961 - val_loss: 40131.7500\n",
      "Epoch 181/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38208.9570 - val_loss: 40127.1719\n",
      "Epoch 182/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38198.3477 - val_loss: 40125.2891\n",
      "Epoch 183/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38187.6758 - val_loss: 40118.1367\n",
      "Epoch 184/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38176.9961 - val_loss: 40118.7461\n",
      "Epoch 185/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38166.3125 - val_loss: 40109.4141\n",
      "Epoch 186/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38155.4609 - val_loss: 40112.8320\n",
      "Epoch 187/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38144.4961 - val_loss: 40107.7617\n",
      "Epoch 188/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38133.4961 - val_loss: 40102.8906\n",
      "Epoch 189/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38122.5000 - val_loss: 40100.8906\n",
      "Epoch 190/1000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 38111.4844 - val_loss: 40097.5898\n",
      "Epoch 191/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38100.5156 - val_loss: 40092.2070\n",
      "Epoch 192/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38089.6562 - val_loss: 40094.4961\n",
      "Epoch 193/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38078.8477 - val_loss: 40084.6602\n",
      "Epoch 194/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38068.1797 - val_loss: 40089.6289\n",
      "Epoch 195/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38057.5391 - val_loss: 40080.9570\n",
      "Epoch 196/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38046.9531 - val_loss: 40082.9648\n",
      "Epoch 197/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38035.9961 - val_loss: 40080.1211\n",
      "Epoch 198/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38025.4844 - val_loss: 40081.2461\n",
      "Epoch 199/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38015.0391 - val_loss: 40079.8789\n",
      "Epoch 200/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38004.9570 - val_loss: 40078.0898\n",
      "Epoch 201/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37994.9609 - val_loss: 40078.6289\n",
      "Epoch 202/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37984.9961 - val_loss: 40073.2383\n",
      "Epoch 203/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37974.9844 - val_loss: 40075.9648\n",
      "Epoch 204/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37964.9961 - val_loss: 40065.9648\n",
      "Epoch 205/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37955.0117 - val_loss: 40075.2539\n",
      "Epoch 206/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37945.0039 - val_loss: 40061.6211\n",
      "Epoch 207/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37934.9609 - val_loss: 40067.9961\n",
      "Epoch 208/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37924.6875 - val_loss: 40066.3633\n",
      "Epoch 209/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37914.4844 - val_loss: 40054.5039\n",
      "Epoch 210/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37904.1875 - val_loss: 40067.2148\n",
      "Epoch 211/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37894.0039 - val_loss: 40047.3711\n",
      "Epoch 212/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37883.6797 - val_loss: 40061.7539\n",
      "Epoch 213/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37873.1875 - val_loss: 40047.1406\n",
      "Epoch 214/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37862.9531 - val_loss: 40051.9219\n",
      "Epoch 215/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37852.4609 - val_loss: 40046.6719\n",
      "Epoch 216/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37841.9609 - val_loss: 40042.4844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 217/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37831.5508 - val_loss: 40044.5000\n",
      "Epoch 218/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37821.1875 - val_loss: 40035.1211\n",
      "Epoch 219/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37810.9844 - val_loss: 40042.3867\n",
      "Epoch 220/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37800.5039 - val_loss: 40027.9883\n",
      "Epoch 221/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37790.0469 - val_loss: 40037.2539\n",
      "Epoch 222/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37779.6562 - val_loss: 40021.1367\n",
      "Epoch 223/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37768.9961 - val_loss: 40035.3711\n",
      "Epoch 224/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37758.1875 - val_loss: 40016.1680\n",
      "Epoch 225/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37747.3203 - val_loss: 40029.3789\n",
      "Epoch 226/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37736.3086 - val_loss: 40008.4961\n",
      "Epoch 227/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37725.3438 - val_loss: 40025.9961\n",
      "Epoch 228/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37714.4844 - val_loss: 39998.2070\n",
      "Epoch 229/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37704.0469 - val_loss: 40026.9961\n",
      "Epoch 230/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37693.6562 - val_loss: 39986.7070\n",
      "Epoch 231/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37683.0469 - val_loss: 40027.9180\n",
      "Epoch 232/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37671.9531 - val_loss: 39980.2148\n",
      "Epoch 233/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37660.0391 - val_loss: 40017.0391\n",
      "Epoch 234/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37647.9844 - val_loss: 39977.1211\n",
      "Epoch 235/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37635.5469 - val_loss: 40001.2617\n",
      "Epoch 236/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37623.0430 - val_loss: 39972.2344\n",
      "Epoch 237/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37610.8555 - val_loss: 39986.8281\n",
      "Epoch 238/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37598.6914 - val_loss: 39965.1211\n",
      "Epoch 239/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37586.6797 - val_loss: 39985.9102\n",
      "Epoch 240/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37574.6914 - val_loss: 39952.2031\n",
      "Epoch 241/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37562.8516 - val_loss: 39992.2617\n",
      "Epoch 242/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37551.1875 - val_loss: 39929.6289\n",
      "Epoch 243/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37540.5039 - val_loss: 40017.9883\n",
      "Epoch 244/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37530.8125 - val_loss: 39892.5820\n",
      "Epoch 245/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37525.3555 - val_loss: 40087.1133\n",
      "Epoch 246/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37523.9883 - val_loss: 39841.3633\n",
      "Epoch 247/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37535.3516 - val_loss: 40211.3398\n",
      "Epoch 248/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37549.6562 - val_loss: 39802.7617\n",
      "Epoch 249/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37573.6484 - val_loss: 40294.2539\n",
      "Epoch 250/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37569.0039 - val_loss: 39801.7852\n",
      "Epoch 251/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37546.6562 - val_loss: 40168.5430\n",
      "Epoch 252/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37488.9609 - val_loss: 39879.8398\n",
      "Epoch 253/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37441.1445 - val_loss: 39945.8359\n",
      "Epoch 254/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37417.1875 - val_loss: 40054.6250\n",
      "Epoch 255/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37421.5156 - val_loss: 39826.6289\n",
      "Epoch 256/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37435.9961 - val_loss: 40156.4648\n",
      "Epoch 257/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37438.1562 - val_loss: 39810.3867\n",
      "Epoch 258/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37425.4609 - val_loss: 40097.7344\n",
      "Epoch 259/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37395.0391 - val_loss: 39868.3281\n",
      "Epoch 260/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37362.9609 - val_loss: 39948.0039\n",
      "Epoch 261/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37341.5117 - val_loss: 39988.0039\n",
      "Epoch 262/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37334.6562 - val_loss: 39844.7461\n",
      "Epoch 263/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37337.1562 - val_loss: 40090.7383\n",
      "Epoch 264/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37341.0039 - val_loss: 39801.0430\n",
      "Epoch 265/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37340.8516 - val_loss: 40116.5820\n",
      "Epoch 266/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37332.0000 - val_loss: 39796.8789\n",
      "Epoch 267/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37315.8086 - val_loss: 40066.5430\n",
      "Epoch 268/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37294.5117 - val_loss: 39822.0391\n",
      "Epoch 269/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37271.6914 - val_loss: 39983.0820\n",
      "Epoch 270/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37250.8555 - val_loss: 39870.7383\n",
      "Epoch 271/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37233.6562 - val_loss: 39908.8906\n",
      "Epoch 272/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37220.3164 - val_loss: 39928.2461\n",
      "Epoch 273/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37210.0430 - val_loss: 39854.0859\n",
      "Epoch 274/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37202.4844 - val_loss: 39986.8789\n",
      "Epoch 275/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37197.4492 - val_loss: 39805.0781\n",
      "Epoch 276/1000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 37195.5039 - val_loss: 40062.5039\n",
      "Epoch 277/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37198.9570 - val_loss: 39753.2148\n",
      "Epoch 278/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37211.5156 - val_loss: 40194.7461\n",
      "Epoch 279/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37237.1836 - val_loss: 39704.2148\n",
      "Epoch 280/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37303.3438 - val_loss: 40455.3867\n",
      "Epoch 281/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37371.0000 - val_loss: 39702.4219\n",
      "Epoch 282/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37458.3555 - val_loss: 40551.6133\n",
      "Epoch 283/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37411.6836 - val_loss: 39688.7461\n",
      "Epoch 284/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37302.0430 - val_loss: 40110.2930\n",
      "Epoch 285/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37136.3086 - val_loss: 39887.7070\n",
      "Epoch 286/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37068.0391 - val_loss: 39720.5859\n",
      "Epoch 287/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37119.6836 - val_loss: 40281.7148\n",
      "Epoch 288/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37203.6953 - val_loss: 39665.9961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 289/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37248.1875 - val_loss: 40233.4883\n",
      "Epoch 290/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37160.8125 - val_loss: 39725.1211\n",
      "Epoch 291/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37050.1914 - val_loss: 39822.4883\n",
      "Epoch 292/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37003.5039 - val_loss: 40035.2383\n",
      "Epoch 293/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37036.4961 - val_loss: 39668.7930\n",
      "Epoch 294/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37087.9570 - val_loss: 40182.9883\n",
      "Epoch 295/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37082.9531 - val_loss: 39687.0781\n",
      "Epoch 296/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37026.6836 - val_loss: 39934.0352\n",
      "Epoch 297/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36962.3516 - val_loss: 39860.3633\n",
      "Epoch 298/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36938.8477 - val_loss: 39721.0039\n",
      "Epoch 299/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36955.4531 - val_loss: 40070.8789\n",
      "Epoch 300/1000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36979.9531 - val_loss: 39671.7617\n",
      "Epoch 301/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36983.3516 - val_loss: 40054.2617\n",
      "Epoch 302/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36951.5156 - val_loss: 39719.2070\n",
      "Epoch 303/1000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36908.4492 - val_loss: 39874.1250\n",
      "Epoch 304/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36874.5469 - val_loss: 39850.7383\n",
      "Epoch 305/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36860.4961 - val_loss: 39740.2070\n",
      "Epoch 306/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36862.3164 - val_loss: 39989.8789\n",
      "Epoch 307/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36870.4961 - val_loss: 39685.9141\n",
      "Epoch 308/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36875.6484 - val_loss: 40044.7070\n",
      "Epoch 309/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36869.9883 - val_loss: 39679.8711\n",
      "Epoch 310/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36855.1836 - val_loss: 40001.6680\n",
      "Epoch 311/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36831.6523 - val_loss: 39705.8711\n",
      "Epoch 312/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36805.9961 - val_loss: 39913.7461\n",
      "Epoch 313/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36781.5508 - val_loss: 39755.4883\n",
      "Epoch 314/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36760.9844 - val_loss: 39832.2500\n",
      "Epoch 315/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36744.5156 - val_loss: 39813.5117\n",
      "Epoch 316/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36731.8438 - val_loss: 39772.2891\n",
      "Epoch 317/1000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36721.9570 - val_loss: 39871.4961\n",
      "Epoch 318/1000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36714.6484 - val_loss: 39719.5781\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00318: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN_500E_Adam_LReLU05 = Sequential()\n",
    "NN_500E_Adam_LReLU05.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam_LReLU05.add(Dense(1))\n",
    "\n",
    "es = EarlyStopping(monitor='val_loss', patience = 30, mode='min', restore_best_weights=True, verbose=1)\n",
    "NN_500E_Adam_LReLU05.compile(loss=root_mean_squared_error, optimizer='adam')\n",
    "history = NN_500E_Adam_LReLU05.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=1000,validation_split=0.2,callbacks=[es])\n",
    "\n",
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1164 samples, validate on 292 samples\n",
      "Epoch 1/10000\n",
      "1164/1164 [==============================] - 0s 136us/step - loss: 195136.5781 - val_loss: 194323.0469\n",
      "Epoch 2/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 194573.4219 - val_loss: 193776.8906\n",
      "Epoch 3/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 194005.5156 - val_loss: 193277.9531\n",
      "Epoch 4/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 193487.9844 - val_loss: 192726.0156\n",
      "Epoch 5/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 192916.1562 - val_loss: 192168.1719\n",
      "Epoch 6/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 192339.9844 - val_loss: 191601.4531\n",
      "Epoch 7/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 191755.3906 - val_loss: 191022.6094\n",
      "Epoch 8/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 191159.4375 - val_loss: 190431.1562\n",
      "Epoch 9/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 190551.5469 - val_loss: 189824.9531\n",
      "Epoch 10/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 189929.9531 - val_loss: 189201.1719\n",
      "Epoch 11/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 189291.8750 - val_loss: 188558.0469\n",
      "Epoch 12/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 188635.4062 - val_loss: 187892.9844\n",
      "Epoch 13/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 187957.9531 - val_loss: 187204.4844\n",
      "Epoch 14/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 187258.1250 - val_loss: 186491.0000\n",
      "Epoch 15/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 186535.5312 - val_loss: 185750.4844\n",
      "Epoch 16/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 185786.6250 - val_loss: 184978.9844\n",
      "Epoch 17/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 185009.5156 - val_loss: 184174.5156\n",
      "Epoch 18/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 184201.4844 - val_loss: 183335.0469\n",
      "Epoch 19/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 183360.5156 - val_loss: 182458.5156\n",
      "Epoch 20/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 182485.8750 - val_loss: 181534.3750\n",
      "Epoch 21/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 181566.4844 - val_loss: 180580.6250\n",
      "Epoch 22/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 180621.8750 - val_loss: 179586.6562\n",
      "Epoch 23/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 179640.6250 - val_loss: 178550.5156\n",
      "Epoch 24/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 178622.6406 - val_loss: 177469.5469\n",
      "Epoch 25/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 177565.9844 - val_loss: 176342.4531\n",
      "Epoch 26/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 176469.3750 - val_loss: 175167.4844\n",
      "Epoch 27/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 175332.0469 - val_loss: 173942.4531\n",
      "Epoch 28/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 174153.4531 - val_loss: 172649.9844\n",
      "Epoch 29/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 172917.4531 - val_loss: 171319.0156\n",
      "Epoch 30/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 171652.6562 - val_loss: 169932.5469\n",
      "Epoch 31/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 170345.3125 - val_loss: 168472.5469\n",
      "Epoch 32/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 168978.0156 - val_loss: 166972.3281\n",
      "Epoch 33/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 167584.6875 - val_loss: 165414.3281\n",
      "Epoch 34/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 166151.3125 - val_loss: 163797.0156\n",
      "Epoch 35/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 164676.6875 - val_loss: 162119.1406\n",
      "Epoch 36/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 163163.3438 - val_loss: 160380.3438\n",
      "Epoch 37/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 161612.0469 - val_loss: 158580.5469\n",
      "Epoch 38/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 160026.1562 - val_loss: 156720.0469\n",
      "Epoch 39/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 158408.5625 - val_loss: 154799.3125\n",
      "Epoch 40/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 156762.5781 - val_loss: 152819.1875\n",
      "Epoch 41/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 155092.5625 - val_loss: 150780.9844\n",
      "Epoch 42/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 153402.7031 - val_loss: 148685.9844\n",
      "Epoch 43/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 151698.5781 - val_loss: 146536.1562\n",
      "Epoch 44/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 149985.8438 - val_loss: 144332.7031\n",
      "Epoch 45/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 148269.8281 - val_loss: 142078.7031\n",
      "Epoch 46/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 146558.2031 - val_loss: 139779.0156\n",
      "Epoch 47/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 144860.6094 - val_loss: 137438.2031\n",
      "Epoch 48/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 143184.7969 - val_loss: 135061.5156\n",
      "Epoch 49/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 141541.3281 - val_loss: 132656.5000\n",
      "Epoch 50/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 139940.6562 - val_loss: 130232.2891\n",
      "Epoch 51/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 138394.6875 - val_loss: 127798.4375\n",
      "Epoch 52/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 136914.7969 - val_loss: 125365.5234\n",
      "Epoch 53/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 135512.6406 - val_loss: 122945.6875\n",
      "Epoch 54/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 134198.8125 - val_loss: 120551.7422\n",
      "Epoch 55/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 132983.9375 - val_loss: 118197.9844\n",
      "Epoch 56/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 131876.2188 - val_loss: 115899.4531\n",
      "Epoch 57/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 130883.9453 - val_loss: 113671.5156\n",
      "Epoch 58/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 130010.0625 - val_loss: 111529.9844\n",
      "Epoch 59/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 129257.1406 - val_loss: 109489.8125\n",
      "Epoch 60/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 128622.6953 - val_loss: 107565.3125\n",
      "Epoch 61/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 128100.8359 - val_loss: 105769.0625\n",
      "Epoch 62/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 127682.0234 - val_loss: 104111.3047\n",
      "Epoch 63/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 127353.1406 - val_loss: 102599.6953\n",
      "Epoch 64/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 127098.2266 - val_loss: 101238.5234\n",
      "Epoch 65/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 126899.7812 - val_loss: 100029.2422\n",
      "Epoch 66/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 126737.8125 - val_loss: 98969.8203\n",
      "Epoch 67/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 126594.2031 - val_loss: 98056.5625\n",
      "Epoch 68/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 126453.1250 - val_loss: 97283.2422\n",
      "Epoch 69/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 126299.8203 - val_loss: 96641.9297\n",
      "Epoch 70/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 126122.9141 - val_loss: 96123.6953\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 71/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 125915.7812 - val_loss: 95719.1875\n",
      "Epoch 72/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 125674.0234 - val_loss: 95418.5859\n",
      "Epoch 73/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 125397.9375 - val_loss: 95212.3125\n",
      "Epoch 74/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 125089.7578 - val_loss: 95090.5234\n",
      "Epoch 75/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 124753.7578 - val_loss: 95043.5078\n",
      "Epoch 76/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 124395.7812 - val_loss: 95061.3203\n",
      "Epoch 77/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 124022.0625 - val_loss: 95134.0625\n",
      "Epoch 78/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 123639.8203 - val_loss: 95251.6875\n",
      "Epoch 79/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 123254.0625 - val_loss: 95404.0078\n",
      "Epoch 80/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 122870.8828 - val_loss: 95580.7422\n",
      "Epoch 81/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 122493.2812 - val_loss: 95771.8203\n",
      "Epoch 82/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 122124.8828 - val_loss: 95967.6953\n",
      "Epoch 83/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 121766.2266 - val_loss: 96159.0234\n",
      "Epoch 84/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 121418.7734 - val_loss: 96337.4297\n",
      "Epoch 85/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 121081.2266 - val_loss: 96494.7734\n",
      "Epoch 86/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 120752.9453 - val_loss: 96625.0703\n",
      "Epoch 87/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 120430.0625 - val_loss: 96721.5078\n",
      "Epoch 88/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 120112.7266 - val_loss: 96778.8047\n",
      "Epoch 89/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 119799.7656 - val_loss: 96793.6953\n",
      "Epoch 90/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 119487.0938 - val_loss: 96763.2422\n",
      "Epoch 91/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 119173.2656 - val_loss: 96685.9453\n",
      "Epoch 92/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 118856.2344 - val_loss: 96561.1953\n",
      "Epoch 93/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 118534.9297 - val_loss: 96389.4922\n",
      "Epoch 94/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 118207.8047 - val_loss: 96172.0703\n",
      "Epoch 95/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 117874.0625 - val_loss: 95911.0078\n",
      "Epoch 96/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 117533.7891 - val_loss: 95608.6953\n",
      "Epoch 97/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 117186.0781 - val_loss: 95268.3281\n",
      "Epoch 98/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 116831.9453 - val_loss: 94893.6719\n",
      "Epoch 99/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 116470.9453 - val_loss: 94488.5234\n",
      "Epoch 100/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 116103.0625 - val_loss: 94057.4219\n",
      "Epoch 101/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 115729.7969 - val_loss: 93604.8125\n",
      "Epoch 102/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 115350.7656 - val_loss: 93135.3281\n",
      "Epoch 103/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 114966.2344 - val_loss: 92653.3281\n",
      "Epoch 104/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 114577.7500 - val_loss: 92160.8203\n",
      "Epoch 105/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 114182.2031 - val_loss: 91666.4219\n",
      "Epoch 106/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 113784.2500 - val_loss: 91169.3125\n",
      "Epoch 107/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 113380.0625 - val_loss: 90677.4766\n",
      "Epoch 108/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 112972.9922 - val_loss: 90191.4219\n",
      "Epoch 109/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 112560.7422 - val_loss: 89711.4766\n",
      "Epoch 110/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 112140.9453 - val_loss: 89243.7422\n",
      "Epoch 111/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 111717.0547 - val_loss: 88787.5859\n",
      "Epoch 112/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 111287.0234 - val_loss: 88344.0234\n",
      "Epoch 113/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 110850.2031 - val_loss: 87913.5781\n",
      "Epoch 114/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 110406.0078 - val_loss: 87496.4766\n",
      "Epoch 115/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 109953.7969 - val_loss: 87092.3125\n",
      "Epoch 116/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 109493.0234 - val_loss: 86700.2422\n",
      "Epoch 117/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 109023.7500 - val_loss: 86319.2266\n",
      "Epoch 118/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 108545.2031 - val_loss: 85947.1797\n",
      "Epoch 119/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 108057.9922 - val_loss: 85582.2422\n",
      "Epoch 120/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 107561.0156 - val_loss: 85222.9219\n",
      "Epoch 121/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 107054.0703 - val_loss: 84867.4297\n",
      "Epoch 122/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 106536.9922 - val_loss: 84513.7734\n",
      "Epoch 123/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 106009.2578 - val_loss: 84160.2734\n",
      "Epoch 124/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 105471.7422 - val_loss: 83803.0859\n",
      "Epoch 125/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 104921.0078 - val_loss: 83442.7266\n",
      "Epoch 126/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 104361.7891 - val_loss: 83075.0859\n",
      "Epoch 127/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 103791.2734 - val_loss: 82697.2734\n",
      "Epoch 128/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 103207.0078 - val_loss: 82309.3359\n",
      "Epoch 129/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 102612.0078 - val_loss: 81908.4297\n",
      "Epoch 130/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 102003.9297 - val_loss: 81493.9297\n",
      "Epoch 131/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 101381.9453 - val_loss: 81065.1641\n",
      "Epoch 132/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 100746.2812 - val_loss: 80619.7422\n",
      "Epoch 133/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 100094.0703 - val_loss: 80160.4297\n",
      "Epoch 134/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 99429.2812 - val_loss: 79685.6562\n",
      "Epoch 135/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 98750.0234 - val_loss: 79195.2578\n",
      "Epoch 136/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 98055.0547 - val_loss: 78690.2266\n",
      "Epoch 137/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 97343.2891 - val_loss: 78171.4766\n",
      "Epoch 138/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 96614.7109 - val_loss: 77639.2812\n",
      "Epoch 139/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 95868.7578 - val_loss: 77094.3438\n",
      "Epoch 140/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 95105.7734 - val_loss: 76537.7578\n",
      "Epoch 141/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 94325.9219 - val_loss: 75970.0859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 142/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 93528.0781 - val_loss: 75390.5781\n",
      "Epoch 143/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 92712.0781 - val_loss: 74800.5938\n",
      "Epoch 144/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 91877.2969 - val_loss: 74202.2422\n",
      "Epoch 145/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 91023.2969 - val_loss: 73596.2422\n",
      "Epoch 146/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 90150.6797 - val_loss: 72982.6562\n",
      "Epoch 147/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 89258.3125 - val_loss: 72362.2578\n",
      "Epoch 148/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 88346.6875 - val_loss: 71734.6719\n",
      "Epoch 149/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 87411.2578 - val_loss: 71101.9141\n",
      "Epoch 150/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 86455.0859 - val_loss: 70464.0938\n",
      "Epoch 151/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 85479.7266 - val_loss: 69820.7422\n",
      "Epoch 152/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 84482.6719 - val_loss: 69173.0234\n",
      "Epoch 153/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 83460.2812 - val_loss: 68521.7266\n",
      "Epoch 154/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 82414.3438 - val_loss: 67864.9688\n",
      "Epoch 155/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 81344.2734 - val_loss: 67200.9141\n",
      "Epoch 156/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 80249.6406 - val_loss: 66527.6016\n",
      "Epoch 157/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 79130.6562 - val_loss: 65844.4688\n",
      "Epoch 158/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 77990.9219 - val_loss: 65149.7812\n",
      "Epoch 159/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 76830.9219 - val_loss: 64442.3359\n",
      "Epoch 160/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 75650.3594 - val_loss: 63723.6094\n",
      "Epoch 161/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 74449.9141 - val_loss: 62994.2383\n",
      "Epoch 162/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 73232.3906 - val_loss: 62254.2188\n",
      "Epoch 163/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 71997.6250 - val_loss: 61506.4727\n",
      "Epoch 164/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 70748.9688 - val_loss: 60753.2188\n",
      "Epoch 165/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 69491.0078 - val_loss: 59995.8711\n",
      "Epoch 166/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 68227.3984 - val_loss: 59237.8828\n",
      "Epoch 167/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 66962.5938 - val_loss: 58483.7383\n",
      "Epoch 168/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 65699.8906 - val_loss: 57738.3984\n",
      "Epoch 169/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 64443.6562 - val_loss: 57007.0078\n",
      "Epoch 170/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 63200.1094 - val_loss: 56293.7617\n",
      "Epoch 171/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 61975.3555 - val_loss: 55603.5078\n",
      "Epoch 172/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 60775.0312 - val_loss: 54941.2188\n",
      "Epoch 173/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 59606.5234 - val_loss: 54311.0938\n",
      "Epoch 174/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 58477.5312 - val_loss: 53716.4688\n",
      "Epoch 175/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 57396.1016 - val_loss: 53159.9648\n",
      "Epoch 176/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 56368.3945 - val_loss: 52644.2539\n",
      "Epoch 177/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 55402.1250 - val_loss: 52170.3477\n",
      "Epoch 178/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 54505.8750 - val_loss: 51740.7852\n",
      "Epoch 179/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 53685.8906 - val_loss: 51357.7539\n",
      "Epoch 180/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 52948.1094 - val_loss: 51022.3398\n",
      "Epoch 181/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 52296.8672 - val_loss: 50735.5039\n",
      "Epoch 182/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 51733.9961 - val_loss: 50497.8789\n",
      "Epoch 183/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 51259.6094 - val_loss: 50308.7539\n",
      "Epoch 184/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 50871.8906 - val_loss: 50165.1250\n",
      "Epoch 185/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50566.4727 - val_loss: 50062.3867\n",
      "Epoch 186/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 50337.0039 - val_loss: 49995.4648\n",
      "Epoch 187/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50175.5312 - val_loss: 49957.6367\n",
      "Epoch 188/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 50071.9609 - val_loss: 49942.3789\n",
      "Epoch 189/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 50015.1406 - val_loss: 49942.8477\n",
      "Epoch 190/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49994.4883 - val_loss: 49952.8398\n",
      "Epoch 191/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49998.6133 - val_loss: 49966.4883\n",
      "Epoch 192/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50017.4961 - val_loss: 49978.8789\n",
      "Epoch 193/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 50041.5312 - val_loss: 49986.4102\n",
      "Epoch 194/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50063.1094 - val_loss: 49986.3828\n",
      "Epoch 195/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 50076.1406 - val_loss: 49976.7617\n",
      "Epoch 196/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 50076.5273 - val_loss: 49956.4102\n",
      "Epoch 197/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 50061.5352 - val_loss: 49924.7539\n",
      "Epoch 198/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 50030.3555 - val_loss: 49881.7383\n",
      "Epoch 199/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49982.8906 - val_loss: 49827.7578\n",
      "Epoch 200/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 49920.8633 - val_loss: 49764.0273\n",
      "Epoch 201/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49846.0117 - val_loss: 49692.2383\n",
      "Epoch 202/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49761.1367 - val_loss: 49614.2383\n",
      "Epoch 203/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49669.0273 - val_loss: 49531.8867\n",
      "Epoch 204/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49572.3867 - val_loss: 49447.1367\n",
      "Epoch 205/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 49473.6406 - val_loss: 49361.7461\n",
      "Epoch 206/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 49375.3555 - val_loss: 49277.2852\n",
      "Epoch 207/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 49279.1445 - val_loss: 49194.8867\n",
      "Epoch 208/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 49186.9883 - val_loss: 49115.5039\n",
      "Epoch 209/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 49099.4727 - val_loss: 49039.4648\n",
      "Epoch 210/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 49017.1445 - val_loss: 48966.8867\n",
      "Epoch 211/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48940.8555 - val_loss: 48897.8633\n",
      "Epoch 212/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48869.8633 - val_loss: 48832.0977\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 213/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48804.0117 - val_loss: 48769.2461\n",
      "Epoch 214/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48743.1133 - val_loss: 48709.0039\n",
      "Epoch 215/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48686.3789 - val_loss: 48651.2383\n",
      "Epoch 216/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48632.9883 - val_loss: 48595.5938\n",
      "Epoch 217/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48582.3555 - val_loss: 48541.8789\n",
      "Epoch 218/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48533.6133 - val_loss: 48489.8711\n",
      "Epoch 219/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48486.4883 - val_loss: 48439.2773\n",
      "Epoch 220/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48440.1133 - val_loss: 48389.9961\n",
      "Epoch 221/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 48394.1406 - val_loss: 48341.7617\n",
      "Epoch 222/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48348.3555 - val_loss: 48294.3789\n",
      "Epoch 223/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48302.0117 - val_loss: 48247.6602\n",
      "Epoch 224/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48255.3867 - val_loss: 48201.5039\n",
      "Epoch 225/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48208.0039 - val_loss: 48155.7188\n",
      "Epoch 226/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48160.0117 - val_loss: 48110.2461\n",
      "Epoch 227/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48111.4883 - val_loss: 48065.0312\n",
      "Epoch 228/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 48062.1406 - val_loss: 48020.1211\n",
      "Epoch 229/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 48012.4883 - val_loss: 47975.4023\n",
      "Epoch 230/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47962.3789 - val_loss: 47930.8867\n",
      "Epoch 231/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 47911.9961 - val_loss: 47886.6367\n",
      "Epoch 232/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47861.4883 - val_loss: 47842.7383\n",
      "Epoch 233/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47810.9688 - val_loss: 47799.1289\n",
      "Epoch 234/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47760.4961 - val_loss: 47755.9102\n",
      "Epoch 235/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47710.3555 - val_loss: 47713.1133\n",
      "Epoch 236/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47660.4883 - val_loss: 47670.7148\n",
      "Epoch 237/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 47610.9961 - val_loss: 47628.6602\n",
      "Epoch 238/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 47561.9688 - val_loss: 47586.9609\n",
      "Epoch 239/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 47513.3750 - val_loss: 47545.4883\n",
      "Epoch 240/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47465.1172 - val_loss: 47504.1562\n",
      "Epoch 241/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47417.4609 - val_loss: 47462.8711\n",
      "Epoch 242/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47370.0117 - val_loss: 47421.5859\n",
      "Epoch 243/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47323.0312 - val_loss: 47380.2617\n",
      "Epoch 244/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47276.4688 - val_loss: 47338.9883\n",
      "Epoch 245/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47230.0117 - val_loss: 47297.7070\n",
      "Epoch 246/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47183.9609 - val_loss: 47256.3789\n",
      "Epoch 247/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47137.9883 - val_loss: 47215.1133\n",
      "Epoch 248/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47092.3477 - val_loss: 47173.7148\n",
      "Epoch 249/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 47046.5391 - val_loss: 47132.1562\n",
      "Epoch 250/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 47001.0117 - val_loss: 47090.4609\n",
      "Epoch 251/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 46955.5391 - val_loss: 47048.5039\n",
      "Epoch 252/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46910.1445 - val_loss: 47006.5039\n",
      "Epoch 253/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46864.9688 - val_loss: 46964.3633\n",
      "Epoch 254/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46819.6172 - val_loss: 46922.1641\n",
      "Epoch 255/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46774.4609 - val_loss: 46880.0391\n",
      "Epoch 256/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46729.1523 - val_loss: 46837.9609\n",
      "Epoch 257/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46684.0391 - val_loss: 46795.8633\n",
      "Epoch 258/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46639.0312 - val_loss: 46753.7500\n",
      "Epoch 259/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46594.0391 - val_loss: 46711.6367\n",
      "Epoch 260/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46549.1562 - val_loss: 46669.6133\n",
      "Epoch 261/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46504.4961 - val_loss: 46627.6133\n",
      "Epoch 262/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46459.8750 - val_loss: 46585.7109\n",
      "Epoch 263/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46415.3477 - val_loss: 46543.8633\n",
      "Epoch 264/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46370.8789 - val_loss: 46502.0430\n",
      "Epoch 265/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 46326.5117 - val_loss: 46460.4062\n",
      "Epoch 266/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46282.4688 - val_loss: 46418.8438\n",
      "Epoch 267/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46238.3750 - val_loss: 46377.4609\n",
      "Epoch 268/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46194.3828 - val_loss: 46336.2383\n",
      "Epoch 269/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 46150.4961 - val_loss: 46295.2188\n",
      "Epoch 270/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 46106.6523 - val_loss: 46254.4062\n",
      "Epoch 271/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 46063.0039 - val_loss: 46213.7383\n",
      "Epoch 272/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 46019.4688 - val_loss: 46173.1641\n",
      "Epoch 273/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 45975.9609 - val_loss: 46132.7930\n",
      "Epoch 274/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45932.5039 - val_loss: 46092.5000\n",
      "Epoch 275/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45889.1289 - val_loss: 46052.3633\n",
      "Epoch 276/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45845.9609 - val_loss: 46012.3711\n",
      "Epoch 277/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45802.6484 - val_loss: 45972.4609\n",
      "Epoch 278/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45759.5312 - val_loss: 45932.7188\n",
      "Epoch 279/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 45716.6211 - val_loss: 45893.0312\n",
      "Epoch 280/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45673.6484 - val_loss: 45853.5430\n",
      "Epoch 281/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45630.8516 - val_loss: 45814.2500\n",
      "Epoch 282/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 45588.0000 - val_loss: 45775.1289\n",
      "Epoch 283/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45545.3398 - val_loss: 45736.2109\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 284/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45502.5430 - val_loss: 45697.4609\n",
      "Epoch 285/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45460.0352 - val_loss: 45658.8438\n",
      "Epoch 286/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45417.5430 - val_loss: 45620.3438\n",
      "Epoch 287/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45375.3438 - val_loss: 45581.8867\n",
      "Epoch 288/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45333.0039 - val_loss: 45543.5039\n",
      "Epoch 289/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 45290.9570 - val_loss: 45505.1133\n",
      "Epoch 290/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45248.8789 - val_loss: 45466.6641\n",
      "Epoch 291/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45206.9609 - val_loss: 45428.2891\n",
      "Epoch 292/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45165.0352 - val_loss: 45390.0391\n",
      "Epoch 293/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45123.3789 - val_loss: 45351.9141\n",
      "Epoch 294/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 45081.6602 - val_loss: 45314.0039\n",
      "Epoch 295/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 45040.1562 - val_loss: 45276.2930\n",
      "Epoch 296/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44998.8750 - val_loss: 45238.7461\n",
      "Epoch 297/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44957.5117 - val_loss: 45201.2539\n",
      "Epoch 298/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44916.4570 - val_loss: 45163.7812\n",
      "Epoch 299/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44875.3477 - val_loss: 45126.2930\n",
      "Epoch 300/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44834.3477 - val_loss: 45088.8359\n",
      "Epoch 301/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44793.4688 - val_loss: 45051.2891\n",
      "Epoch 302/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44752.6211 - val_loss: 45013.8359\n",
      "Epoch 303/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44711.9688 - val_loss: 44976.4062\n",
      "Epoch 304/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44671.3711 - val_loss: 44939.2109\n",
      "Epoch 305/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44630.8711 - val_loss: 44902.2109\n",
      "Epoch 306/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44590.4688 - val_loss: 44865.4609\n",
      "Epoch 307/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44550.0430 - val_loss: 44828.7891\n",
      "Epoch 308/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44509.9609 - val_loss: 44792.2070\n",
      "Epoch 309/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 44469.8438 - val_loss: 44755.5859\n",
      "Epoch 310/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44429.6641 - val_loss: 44718.9961\n",
      "Epoch 311/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44389.8477 - val_loss: 44682.3867\n",
      "Epoch 312/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 44349.9883 - val_loss: 44645.9609\n",
      "Epoch 313/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44310.1523 - val_loss: 44609.6133\n",
      "Epoch 314/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44270.5039 - val_loss: 44573.4062\n",
      "Epoch 315/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44230.9883 - val_loss: 44537.3711\n",
      "Epoch 316/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44191.4883 - val_loss: 44501.4609\n",
      "Epoch 317/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44152.0312 - val_loss: 44465.6562\n",
      "Epoch 318/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44112.8398 - val_loss: 44430.0859\n",
      "Epoch 319/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 44073.5391 - val_loss: 44394.5938\n",
      "Epoch 320/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 44034.4883 - val_loss: 44359.2188\n",
      "Epoch 321/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43995.3750 - val_loss: 44323.9609\n",
      "Epoch 322/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43956.3711 - val_loss: 44288.7812\n",
      "Epoch 323/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43917.4688 - val_loss: 44253.6133\n",
      "Epoch 324/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43878.5352 - val_loss: 44218.3867\n",
      "Epoch 325/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43839.8438 - val_loss: 44183.3789\n",
      "Epoch 326/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43801.0352 - val_loss: 44148.6211\n",
      "Epoch 327/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43762.4883 - val_loss: 44114.0117\n",
      "Epoch 328/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43723.9570 - val_loss: 44079.6211\n",
      "Epoch 329/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43685.4648 - val_loss: 44045.3359\n",
      "Epoch 330/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43647.0312 - val_loss: 44011.2188\n",
      "Epoch 331/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43608.8711 - val_loss: 43977.2070\n",
      "Epoch 332/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43570.8359 - val_loss: 43943.2891\n",
      "Epoch 333/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43532.8438 - val_loss: 43909.5430\n",
      "Epoch 334/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43494.9961 - val_loss: 43875.9883\n",
      "Epoch 335/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43457.3711 - val_loss: 43842.4883\n",
      "Epoch 336/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43419.8711 - val_loss: 43809.0312\n",
      "Epoch 337/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43382.5039 - val_loss: 43775.7539\n",
      "Epoch 338/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 43345.3438 - val_loss: 43742.5312\n",
      "Epoch 339/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43308.3320 - val_loss: 43709.4062\n",
      "Epoch 340/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43271.3711 - val_loss: 43676.4141\n",
      "Epoch 341/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43234.5352 - val_loss: 43643.5039\n",
      "Epoch 342/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43197.9883 - val_loss: 43610.7539\n",
      "Epoch 343/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43161.4570 - val_loss: 43578.1562\n",
      "Epoch 344/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43125.0117 - val_loss: 43545.6367\n",
      "Epoch 345/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 43088.6680 - val_loss: 43513.2461\n",
      "Epoch 346/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 43052.5352 - val_loss: 43481.0117\n",
      "Epoch 347/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 43016.5117 - val_loss: 43448.9961\n",
      "Epoch 348/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42980.6289 - val_loss: 43417.0430\n",
      "Epoch 349/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 42944.9570 - val_loss: 43385.1562\n",
      "Epoch 350/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42909.1641 - val_loss: 43353.3633\n",
      "Epoch 351/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42873.6602 - val_loss: 43321.6641\n",
      "Epoch 352/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42838.3398 - val_loss: 43290.1289\n",
      "Epoch 353/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42803.0000 - val_loss: 43258.7461\n",
      "Epoch 354/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42767.8633 - val_loss: 43227.5117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 355/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 42732.8320 - val_loss: 43196.5820\n",
      "Epoch 356/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42697.8711 - val_loss: 43165.8438\n",
      "Epoch 357/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42663.0117 - val_loss: 43135.2617\n",
      "Epoch 358/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42628.4570 - val_loss: 43104.8633\n",
      "Epoch 359/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42593.9570 - val_loss: 43074.5430\n",
      "Epoch 360/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42559.5039 - val_loss: 43044.2539\n",
      "Epoch 361/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42525.3398 - val_loss: 43013.9961\n",
      "Epoch 362/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42491.1641 - val_loss: 42983.8867\n",
      "Epoch 363/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 42457.3398 - val_loss: 42953.9961\n",
      "Epoch 364/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42423.5039 - val_loss: 42924.2539\n",
      "Epoch 365/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 42389.9570 - val_loss: 42894.7617\n",
      "Epoch 366/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42356.4570 - val_loss: 42865.4570\n",
      "Epoch 367/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42323.0352 - val_loss: 42836.2617\n",
      "Epoch 368/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42289.9570 - val_loss: 42807.2383\n",
      "Epoch 369/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 42256.8711 - val_loss: 42778.3438\n",
      "Epoch 370/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42223.9961 - val_loss: 42749.6133\n",
      "Epoch 371/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42191.1719 - val_loss: 42720.9961\n",
      "Epoch 372/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42158.6406 - val_loss: 42692.5898\n",
      "Epoch 373/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42126.1680 - val_loss: 42664.3398\n",
      "Epoch 374/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 42093.9648 - val_loss: 42636.2383\n",
      "Epoch 375/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 42061.8398 - val_loss: 42608.2383\n",
      "Epoch 376/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 42029.8711 - val_loss: 42580.3711\n",
      "Epoch 377/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41998.0430 - val_loss: 42552.7617\n",
      "Epoch 378/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 41966.4961 - val_loss: 42525.4180\n",
      "Epoch 379/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41935.0039 - val_loss: 42498.2617\n",
      "Epoch 380/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41903.6719 - val_loss: 42471.3711\n",
      "Epoch 381/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 41872.6328 - val_loss: 42444.5391\n",
      "Epoch 382/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41841.6445 - val_loss: 42417.9062\n",
      "Epoch 383/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41810.9570 - val_loss: 42391.4883\n",
      "Epoch 384/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41780.3281 - val_loss: 42365.2461\n",
      "Epoch 385/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41749.8398 - val_loss: 42339.1250\n",
      "Epoch 386/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41719.5039 - val_loss: 42313.1133\n",
      "Epoch 387/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41689.4648 - val_loss: 42287.1680\n",
      "Epoch 388/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 41659.4961 - val_loss: 42261.4609\n",
      "Epoch 389/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41629.6680 - val_loss: 42235.9570\n",
      "Epoch 390/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41600.0430 - val_loss: 42210.6641\n",
      "Epoch 391/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41570.6719 - val_loss: 42185.6289\n",
      "Epoch 392/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41541.4961 - val_loss: 42160.8359\n",
      "Epoch 393/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41512.4648 - val_loss: 42136.1250\n",
      "Epoch 394/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41483.5117 - val_loss: 42111.6367\n",
      "Epoch 395/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41454.8359 - val_loss: 42087.2930\n",
      "Epoch 396/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 41426.1719 - val_loss: 42063.1211\n",
      "Epoch 397/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41397.8672 - val_loss: 42039.0430\n",
      "Epoch 398/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41369.5469 - val_loss: 42015.1367\n",
      "Epoch 399/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41341.5039 - val_loss: 41991.5312\n",
      "Epoch 400/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41313.6328 - val_loss: 41968.1367\n",
      "Epoch 401/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41285.8711 - val_loss: 41944.9961\n",
      "Epoch 402/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41258.3281 - val_loss: 41921.9102\n",
      "Epoch 403/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41230.8711 - val_loss: 41898.9102\n",
      "Epoch 404/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41203.5430 - val_loss: 41876.1680\n",
      "Epoch 405/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 41176.5039 - val_loss: 41853.6367\n",
      "Epoch 406/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41149.5430 - val_loss: 41831.2461\n",
      "Epoch 407/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41122.8711 - val_loss: 41808.9180\n",
      "Epoch 408/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41096.3281 - val_loss: 41786.7383\n",
      "Epoch 409/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41069.9570 - val_loss: 41764.7383\n",
      "Epoch 410/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41043.5469 - val_loss: 41742.9180\n",
      "Epoch 411/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 41017.5117 - val_loss: 41721.3633\n",
      "Epoch 412/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40991.5469 - val_loss: 41699.9648\n",
      "Epoch 413/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40965.8359 - val_loss: 41678.7617\n",
      "Epoch 414/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40940.1406 - val_loss: 41657.7383\n",
      "Epoch 415/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40914.6719 - val_loss: 41636.6367\n",
      "Epoch 416/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40889.4609 - val_loss: 41615.7852\n",
      "Epoch 417/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40864.3281 - val_loss: 41595.3320\n",
      "Epoch 418/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40839.1758 - val_loss: 41574.9883\n",
      "Epoch 419/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40814.3320 - val_loss: 41554.7969\n",
      "Epoch 420/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40789.4883 - val_loss: 41534.7539\n",
      "Epoch 421/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40764.8242 - val_loss: 41514.7539\n",
      "Epoch 422/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40740.0430 - val_loss: 41494.9648\n",
      "Epoch 423/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40715.6719 - val_loss: 41475.4570\n",
      "Epoch 424/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40691.4883 - val_loss: 41456.2617\n",
      "Epoch 425/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40667.4609 - val_loss: 41437.2539\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 426/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40643.5117 - val_loss: 41418.4961\n",
      "Epoch 427/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40619.8672 - val_loss: 41399.7539\n",
      "Epoch 428/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40596.4883 - val_loss: 41381.0039\n",
      "Epoch 429/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40573.3242 - val_loss: 41362.0352\n",
      "Epoch 430/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40550.3555 - val_loss: 41343.2539\n",
      "Epoch 431/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40527.6719 - val_loss: 41324.7930\n",
      "Epoch 432/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40505.3555 - val_loss: 41306.5898\n",
      "Epoch 433/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40483.1445 - val_loss: 41288.7461\n",
      "Epoch 434/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40461.3555 - val_loss: 41271.3398\n",
      "Epoch 435/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40439.6406 - val_loss: 41254.1133\n",
      "Epoch 436/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40418.3281 - val_loss: 41237.2148\n",
      "Epoch 437/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40397.1445 - val_loss: 41220.4531\n",
      "Epoch 438/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40376.3281 - val_loss: 41203.8711\n",
      "Epoch 439/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40355.6406 - val_loss: 41187.6133\n",
      "Epoch 440/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40335.1680 - val_loss: 41171.6133\n",
      "Epoch 441/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40314.9570 - val_loss: 41156.1680\n",
      "Epoch 442/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40294.8242 - val_loss: 41141.3320\n",
      "Epoch 443/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40274.9570 - val_loss: 41126.7383\n",
      "Epoch 444/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40255.1758 - val_loss: 41112.0820\n",
      "Epoch 445/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40235.8203 - val_loss: 41097.3320\n",
      "Epoch 446/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 40216.5039 - val_loss: 41082.8633\n",
      "Epoch 447/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40197.5039 - val_loss: 41068.7539\n",
      "Epoch 448/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40178.6758 - val_loss: 41055.0469\n",
      "Epoch 449/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40160.1445 - val_loss: 41041.5352\n",
      "Epoch 450/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40141.8320 - val_loss: 41028.1289\n",
      "Epoch 451/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40123.6367 - val_loss: 41015.1133\n",
      "Epoch 452/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40105.6445 - val_loss: 41002.1680\n",
      "Epoch 453/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40087.9570 - val_loss: 40989.4102\n",
      "Epoch 454/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 40070.3281 - val_loss: 40976.8789\n",
      "Epoch 455/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40052.9570 - val_loss: 40964.6133\n",
      "Epoch 456/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40035.6719 - val_loss: 40952.5820\n",
      "Epoch 457/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40018.6445 - val_loss: 40940.7852\n",
      "Epoch 458/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 40001.8320 - val_loss: 40929.0117\n",
      "Epoch 459/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39985.0430 - val_loss: 40917.1680\n",
      "Epoch 460/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39968.5469 - val_loss: 40905.4883\n",
      "Epoch 461/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39952.3203 - val_loss: 40893.9102\n",
      "Epoch 462/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39936.1367 - val_loss: 40882.7930\n",
      "Epoch 463/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39920.1719 - val_loss: 40872.1289\n",
      "Epoch 464/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39904.5156 - val_loss: 40861.5898\n",
      "Epoch 465/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39889.0391 - val_loss: 40850.7617\n",
      "Epoch 466/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39873.8281 - val_loss: 40839.9961\n",
      "Epoch 467/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39858.6484 - val_loss: 40829.6211\n",
      "Epoch 468/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39843.8203 - val_loss: 40819.7500\n",
      "Epoch 469/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39829.0039 - val_loss: 40810.2461\n",
      "Epoch 470/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39814.4883 - val_loss: 40801.0820\n",
      "Epoch 471/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39800.0352 - val_loss: 40791.8398\n",
      "Epoch 472/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39785.8633 - val_loss: 40782.2617\n",
      "Epoch 473/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39771.8320 - val_loss: 40772.7539\n",
      "Epoch 474/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39757.9648 - val_loss: 40763.8398\n",
      "Epoch 475/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39744.1719 - val_loss: 40755.3789\n",
      "Epoch 476/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39730.6484 - val_loss: 40746.9570\n",
      "Epoch 477/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39717.3281 - val_loss: 40738.5898\n",
      "Epoch 478/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39704.0352 - val_loss: 40730.3711\n",
      "Epoch 479/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39691.0039 - val_loss: 40722.1367\n",
      "Epoch 480/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39678.1367 - val_loss: 40714.0820\n",
      "Epoch 481/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39665.4844 - val_loss: 40706.3633\n",
      "Epoch 482/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39652.9609 - val_loss: 40698.6680\n",
      "Epoch 483/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39640.5039 - val_loss: 40691.1211\n",
      "Epoch 484/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39628.3203 - val_loss: 40683.7539\n",
      "Epoch 485/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39616.1484 - val_loss: 40676.7383\n",
      "Epoch 486/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39604.3203 - val_loss: 40669.5117\n",
      "Epoch 487/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39592.4961 - val_loss: 40661.9883\n",
      "Epoch 488/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39580.8633 - val_loss: 40654.7969\n",
      "Epoch 489/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39569.4531 - val_loss: 40648.4961\n",
      "Epoch 490/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39558.0391 - val_loss: 40642.4883\n",
      "Epoch 491/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39546.9609 - val_loss: 40636.3398\n",
      "Epoch 492/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39535.9609 - val_loss: 40629.7070\n",
      "Epoch 493/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39525.0117 - val_loss: 40623.2539\n",
      "Epoch 494/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39514.3516 - val_loss: 40617.0820\n",
      "Epoch 495/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39503.8164 - val_loss: 40611.1680\n",
      "Epoch 496/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39493.3281 - val_loss: 40605.6680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 497/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39483.0039 - val_loss: 40600.4180\n",
      "Epoch 498/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39472.8594 - val_loss: 40595.1289\n",
      "Epoch 499/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39462.8555 - val_loss: 40589.8633\n",
      "Epoch 500/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39452.9961 - val_loss: 40584.5430\n",
      "Epoch 501/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39443.3203 - val_loss: 40579.2461\n",
      "Epoch 502/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39433.6719 - val_loss: 40574.1602\n",
      "Epoch 503/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39424.1836 - val_loss: 40569.3320\n",
      "Epoch 504/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39414.9844 - val_loss: 40564.4570\n",
      "Epoch 505/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39405.6836 - val_loss: 40559.8867\n",
      "Epoch 506/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39396.6445 - val_loss: 40555.7539\n",
      "Epoch 507/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39387.6719 - val_loss: 40551.6250\n",
      "Epoch 508/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39378.9570 - val_loss: 40547.1133\n",
      "Epoch 509/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39370.1719 - val_loss: 40542.5820\n",
      "Epoch 510/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39361.5469 - val_loss: 40538.1133\n",
      "Epoch 511/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39353.1406 - val_loss: 40534.2461\n",
      "Epoch 512/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39344.8203 - val_loss: 40530.7461\n",
      "Epoch 513/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39336.5039 - val_loss: 40527.1211\n",
      "Epoch 514/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39328.4531 - val_loss: 40523.5430\n",
      "Epoch 515/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39320.4531 - val_loss: 40519.9648\n",
      "Epoch 516/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 39312.4961 - val_loss: 40516.4883\n",
      "Epoch 517/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39304.6719 - val_loss: 40513.1211\n",
      "Epoch 518/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39297.0117 - val_loss: 40509.7070\n",
      "Epoch 519/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39289.5000 - val_loss: 40506.4961\n",
      "Epoch 520/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39282.0391 - val_loss: 40503.6289\n",
      "Epoch 521/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39274.8125 - val_loss: 40500.7539\n",
      "Epoch 522/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39267.5039 - val_loss: 40497.9570\n",
      "Epoch 523/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39260.4531 - val_loss: 40495.2070\n",
      "Epoch 524/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39253.4531 - val_loss: 40492.5117\n",
      "Epoch 525/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39246.4883 - val_loss: 40489.7383\n",
      "Epoch 526/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39239.6406 - val_loss: 40487.0820\n",
      "Epoch 527/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39232.9570 - val_loss: 40484.6133\n",
      "Epoch 528/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39226.3203 - val_loss: 40482.2852\n",
      "Epoch 529/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39219.8164 - val_loss: 40479.9531\n",
      "Epoch 530/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39213.3281 - val_loss: 40477.7070\n",
      "Epoch 531/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39206.9883 - val_loss: 40475.2070\n",
      "Epoch 532/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39200.6719 - val_loss: 40472.6602\n",
      "Epoch 533/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39194.5039 - val_loss: 40470.5430\n",
      "Epoch 534/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39188.4531 - val_loss: 40468.8398\n",
      "Epoch 535/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39182.4531 - val_loss: 40467.2070\n",
      "Epoch 536/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39176.4883 - val_loss: 40465.5117\n",
      "Epoch 537/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39170.5469 - val_loss: 40463.6367\n",
      "Epoch 538/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39164.8477 - val_loss: 40461.4883\n",
      "Epoch 539/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39159.0391 - val_loss: 40459.4570\n",
      "Epoch 540/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39153.4883 - val_loss: 40458.0000\n",
      "Epoch 541/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39147.9570 - val_loss: 40456.7539\n",
      "Epoch 542/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39142.4609 - val_loss: 40455.1289\n",
      "Epoch 543/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39136.9961 - val_loss: 40453.2617\n",
      "Epoch 544/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39131.5508 - val_loss: 40451.7617\n",
      "Epoch 545/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39126.3555 - val_loss: 40450.6367\n",
      "Epoch 546/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39121.1523 - val_loss: 40449.7148\n",
      "Epoch 547/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39116.0352 - val_loss: 40448.8320\n",
      "Epoch 548/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39111.0039 - val_loss: 40447.4570\n",
      "Epoch 549/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39106.0039 - val_loss: 40445.7461\n",
      "Epoch 550/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39101.0156 - val_loss: 40444.4102\n",
      "Epoch 551/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39096.1406 - val_loss: 40443.3867\n",
      "Epoch 552/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39091.3516 - val_loss: 40442.1211\n",
      "Epoch 553/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39086.5156 - val_loss: 40440.6680\n",
      "Epoch 554/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39081.8594 - val_loss: 40439.2617\n",
      "Epoch 555/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39077.1836 - val_loss: 40438.2617\n",
      "Epoch 556/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39072.6445 - val_loss: 40437.6602\n",
      "Epoch 557/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39068.0430 - val_loss: 40436.8633\n",
      "Epoch 558/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39063.5469 - val_loss: 40435.6133\n",
      "Epoch 559/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39059.1719 - val_loss: 40434.1211\n",
      "Epoch 560/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39054.8477 - val_loss: 40433.0469\n",
      "Epoch 561/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39050.4961 - val_loss: 40432.2930\n",
      "Epoch 562/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39046.1758 - val_loss: 40431.3320\n",
      "Epoch 563/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 39041.9883 - val_loss: 40430.0469\n",
      "Epoch 564/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39037.8164 - val_loss: 40429.0820\n",
      "Epoch 565/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39033.6484 - val_loss: 40428.3398\n",
      "Epoch 566/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39029.5352 - val_loss: 40427.4961\n",
      "Epoch 567/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39025.5039 - val_loss: 40426.7461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 568/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39021.5039 - val_loss: 40425.8711\n",
      "Epoch 569/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39017.5352 - val_loss: 40425.0117\n",
      "Epoch 570/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39013.6484 - val_loss: 40424.0430\n",
      "Epoch 571/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 39009.8516 - val_loss: 40423.0898\n",
      "Epoch 572/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39006.0117 - val_loss: 40422.3633\n",
      "Epoch 573/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 39002.3281 - val_loss: 40422.1602\n",
      "Epoch 574/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38998.5469 - val_loss: 40421.8320\n",
      "Epoch 575/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38994.9883 - val_loss: 40421.2070\n",
      "Epoch 576/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38991.3516 - val_loss: 40420.2461\n",
      "Epoch 577/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38987.8125 - val_loss: 40419.2148\n",
      "Epoch 578/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38984.1719 - val_loss: 40418.3711\n",
      "Epoch 579/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38980.6719 - val_loss: 40417.5352\n",
      "Epoch 580/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38977.1758 - val_loss: 40417.2070\n",
      "Epoch 581/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38973.8164 - val_loss: 40417.1211\n",
      "Epoch 582/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38970.3516 - val_loss: 40416.8711\n",
      "Epoch 583/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38966.9883 - val_loss: 40416.0430\n",
      "Epoch 584/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38963.5391 - val_loss: 40414.9180\n",
      "Epoch 585/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38960.1836 - val_loss: 40414.3867\n",
      "Epoch 586/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38956.9844 - val_loss: 40414.1367\n",
      "Epoch 587/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38953.6445 - val_loss: 40413.7930\n",
      "Epoch 588/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38950.4883 - val_loss: 40413.2070\n",
      "Epoch 589/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38947.1875 - val_loss: 40412.3711\n",
      "Epoch 590/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38944.0391 - val_loss: 40411.3398\n",
      "Epoch 591/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38940.9648 - val_loss: 40410.7383\n",
      "Epoch 592/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38937.8477 - val_loss: 40410.5117\n",
      "Epoch 593/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38934.6836 - val_loss: 40410.1367\n",
      "Epoch 594/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38931.6523 - val_loss: 40409.3711\n",
      "Epoch 595/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38928.6445 - val_loss: 40408.2383\n",
      "Epoch 596/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38925.6445 - val_loss: 40407.8320\n",
      "Epoch 597/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38922.6484 - val_loss: 40408.0039\n",
      "Epoch 598/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38919.6758 - val_loss: 40407.7930\n",
      "Epoch 599/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38916.8164 - val_loss: 40407.2852\n",
      "Epoch 600/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38913.8594 - val_loss: 40406.3789\n",
      "Epoch 601/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38910.9844 - val_loss: 40405.9961\n",
      "Epoch 602/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38908.0352 - val_loss: 40405.8711\n",
      "Epoch 603/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38905.1836 - val_loss: 40405.5039\n",
      "Epoch 604/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38902.4531 - val_loss: 40404.7930\n",
      "Epoch 605/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38899.5469 - val_loss: 40404.1602\n",
      "Epoch 606/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38896.8516 - val_loss: 40404.0352\n",
      "Epoch 607/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38894.0391 - val_loss: 40403.5469\n",
      "Epoch 608/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38891.3555 - val_loss: 40403.0117\n",
      "Epoch 609/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38888.6484 - val_loss: 40402.8867\n",
      "Epoch 610/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38886.0039 - val_loss: 40402.7539\n",
      "Epoch 611/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38883.3555 - val_loss: 40402.5117\n",
      "Epoch 612/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38880.6836 - val_loss: 40402.1289\n",
      "Epoch 613/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38878.1406 - val_loss: 40401.5898\n",
      "Epoch 614/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38875.5117 - val_loss: 40401.0039\n",
      "Epoch 615/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38872.9883 - val_loss: 40400.6211\n",
      "Epoch 616/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38870.4531 - val_loss: 40400.1602\n",
      "Epoch 617/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38867.8594 - val_loss: 40399.6680\n",
      "Epoch 618/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38865.3516 - val_loss: 40399.2500\n",
      "Epoch 619/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38862.8516 - val_loss: 40398.8633\n",
      "Epoch 620/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38860.3477 - val_loss: 40398.3711\n",
      "Epoch 621/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38857.8477 - val_loss: 40397.9961\n",
      "Epoch 622/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38855.3516 - val_loss: 40397.8711\n",
      "Epoch 623/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38852.8594 - val_loss: 40397.7852\n",
      "Epoch 624/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38850.4609 - val_loss: 40397.2461\n",
      "Epoch 625/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38847.9883 - val_loss: 40396.3789\n",
      "Epoch 626/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38845.4961 - val_loss: 40395.7461\n",
      "Epoch 627/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38843.0117 - val_loss: 40395.2461\n",
      "Epoch 628/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38840.5469 - val_loss: 40394.6289\n",
      "Epoch 629/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38838.1719 - val_loss: 40394.1133\n",
      "Epoch 630/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38835.8164 - val_loss: 40393.5820\n",
      "Epoch 631/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38833.4492 - val_loss: 40393.0039\n",
      "Epoch 632/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38830.9961 - val_loss: 40392.6680\n",
      "Epoch 633/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38828.5469 - val_loss: 40391.9102\n",
      "Epoch 634/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38826.1836 - val_loss: 40391.4102\n",
      "Epoch 635/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38823.8516 - val_loss: 40391.6367\n",
      "Epoch 636/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38821.4883 - val_loss: 40391.4883\n",
      "Epoch 637/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38819.0430 - val_loss: 40390.2930\n",
      "Epoch 638/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38816.8125 - val_loss: 40388.8867\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 639/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38814.4531 - val_loss: 40388.3320\n",
      "Epoch 640/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38812.0156 - val_loss: 40388.0000\n",
      "Epoch 641/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38809.6445 - val_loss: 40387.4648\n",
      "Epoch 642/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38807.1875 - val_loss: 40386.4961\n",
      "Epoch 643/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38804.9531 - val_loss: 40385.8633\n",
      "Epoch 644/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38802.5156 - val_loss: 40385.2852\n",
      "Epoch 645/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38800.3242 - val_loss: 40384.3711\n",
      "Epoch 646/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38798.0039 - val_loss: 40383.5430\n",
      "Epoch 647/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38795.8125 - val_loss: 40382.8398\n",
      "Epoch 648/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38793.5039 - val_loss: 40382.3711\n",
      "Epoch 649/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38791.3125 - val_loss: 40381.8789\n",
      "Epoch 650/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38789.0039 - val_loss: 40381.2070\n",
      "Epoch 651/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38786.8125 - val_loss: 40380.1367\n",
      "Epoch 652/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38784.5039 - val_loss: 40379.2031\n",
      "Epoch 653/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38782.3242 - val_loss: 40378.3867\n",
      "Epoch 654/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38780.0156 - val_loss: 40377.9570\n",
      "Epoch 655/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38777.8516 - val_loss: 40377.3867\n",
      "Epoch 656/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38775.6445 - val_loss: 40376.4961\n",
      "Epoch 657/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38773.4609 - val_loss: 40375.1289\n",
      "Epoch 658/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38771.1875 - val_loss: 40374.3867\n",
      "Epoch 659/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38769.0156 - val_loss: 40374.3398\n",
      "Epoch 660/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38766.9531 - val_loss: 40373.9102\n",
      "Epoch 661/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38764.6758 - val_loss: 40372.6367\n",
      "Epoch 662/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38762.5156 - val_loss: 40371.7539\n",
      "Epoch 663/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38760.4492 - val_loss: 40371.2539\n",
      "Epoch 664/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38758.1875 - val_loss: 40370.7969\n",
      "Epoch 665/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38756.0391 - val_loss: 40369.5898\n",
      "Epoch 666/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38753.9609 - val_loss: 40368.7500\n",
      "Epoch 667/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38751.8164 - val_loss: 40368.6680\n",
      "Epoch 668/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38749.6445 - val_loss: 40367.9961\n",
      "Epoch 669/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38747.4961 - val_loss: 40366.2930\n",
      "Epoch 670/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38745.3555 - val_loss: 40365.3789\n",
      "Epoch 671/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38743.1836 - val_loss: 40365.6367\n",
      "Epoch 672/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38741.0391 - val_loss: 40365.1367\n",
      "Epoch 673/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38738.9883 - val_loss: 40363.6211\n",
      "Epoch 674/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38736.8477 - val_loss: 40362.1680\n",
      "Epoch 675/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38734.6758 - val_loss: 40361.3320\n",
      "Epoch 676/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38732.5391 - val_loss: 40361.0352\n",
      "Epoch 677/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38730.4609 - val_loss: 40360.3320\n",
      "Epoch 678/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38728.3242 - val_loss: 40359.3320\n",
      "Epoch 679/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38726.1758 - val_loss: 40358.0352\n",
      "Epoch 680/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38724.0391 - val_loss: 40356.8398\n",
      "Epoch 681/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38721.9844 - val_loss: 40356.3320\n",
      "Epoch 682/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38719.8477 - val_loss: 40355.8750\n",
      "Epoch 683/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38717.6758 - val_loss: 40355.1211\n",
      "Epoch 684/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38715.5469 - val_loss: 40353.9648\n",
      "Epoch 685/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38713.5039 - val_loss: 40352.8398\n",
      "Epoch 686/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38711.4844 - val_loss: 40351.8711\n",
      "Epoch 687/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38709.3555 - val_loss: 40351.2617\n",
      "Epoch 688/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38707.3242 - val_loss: 40350.9883\n",
      "Epoch 689/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38705.1875 - val_loss: 40349.8633\n",
      "Epoch 690/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38703.1484 - val_loss: 40348.2617\n",
      "Epoch 691/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38701.0391 - val_loss: 40347.2383\n",
      "Epoch 692/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38699.0039 - val_loss: 40346.8711\n",
      "Epoch 693/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38696.9844 - val_loss: 40346.6289\n",
      "Epoch 694/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38694.9531 - val_loss: 40345.1367\n",
      "Epoch 695/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38692.8242 - val_loss: 40343.5039\n",
      "Epoch 696/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38690.6875 - val_loss: 40343.0039\n",
      "Epoch 697/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38688.6523 - val_loss: 40342.8867\n",
      "Epoch 698/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38686.5508 - val_loss: 40341.8711\n",
      "Epoch 699/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38684.5156 - val_loss: 40340.5039\n",
      "Epoch 700/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38682.4961 - val_loss: 40339.8711\n",
      "Epoch 701/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38680.4531 - val_loss: 40339.0117\n",
      "Epoch 702/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38678.3477 - val_loss: 40337.6289\n",
      "Epoch 703/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38676.3125 - val_loss: 40336.8711\n",
      "Epoch 704/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38674.1758 - val_loss: 40336.4961\n",
      "Epoch 705/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38672.0469 - val_loss: 40335.6602\n",
      "Epoch 706/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38670.0039 - val_loss: 40334.3789\n",
      "Epoch 707/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38667.9609 - val_loss: 40333.9102\n",
      "Epoch 708/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38665.8594 - val_loss: 40333.4531\n",
      "Epoch 709/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38663.8125 - val_loss: 40332.0352\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 710/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38661.6523 - val_loss: 40330.9961\n",
      "Epoch 711/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38659.5391 - val_loss: 40330.2148\n",
      "Epoch 712/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38657.5039 - val_loss: 40329.7930\n",
      "Epoch 713/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38655.4844 - val_loss: 40329.0039\n",
      "Epoch 714/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38653.4492 - val_loss: 40328.0430\n",
      "Epoch 715/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38651.3555 - val_loss: 40327.2461\n",
      "Epoch 716/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38649.3242 - val_loss: 40326.7070\n",
      "Epoch 717/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38647.1875 - val_loss: 40325.8711\n",
      "Epoch 718/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38645.1523 - val_loss: 40324.8867\n",
      "Epoch 719/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38643.0430 - val_loss: 40324.1367\n",
      "Epoch 720/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38641.0391 - val_loss: 40323.5391\n",
      "Epoch 721/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38639.0039 - val_loss: 40322.7617\n",
      "Epoch 722/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38636.9961 - val_loss: 40321.9570\n",
      "Epoch 723/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38634.9961 - val_loss: 40320.9570\n",
      "Epoch 724/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38632.9844 - val_loss: 40319.9961\n",
      "Epoch 725/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38630.9531 - val_loss: 40319.2383\n",
      "Epoch 726/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38628.8242 - val_loss: 40318.6211\n",
      "Epoch 727/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38626.6758 - val_loss: 40317.6289\n",
      "Epoch 728/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38624.5391 - val_loss: 40316.8281\n",
      "Epoch 729/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38622.4961 - val_loss: 40316.2383\n",
      "Epoch 730/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38620.3516 - val_loss: 40315.6641\n",
      "Epoch 731/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38618.1758 - val_loss: 40314.7148\n",
      "Epoch 732/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38616.0039 - val_loss: 40313.5117\n",
      "Epoch 733/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38613.8477 - val_loss: 40313.0039\n",
      "Epoch 734/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38611.5508 - val_loss: 40312.3789\n",
      "Epoch 735/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38609.4531 - val_loss: 40311.2344\n",
      "Epoch 736/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38607.1484 - val_loss: 40310.2461\n",
      "Epoch 737/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38604.9570 - val_loss: 40309.4961\n",
      "Epoch 738/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38602.6445 - val_loss: 40308.4609\n",
      "Epoch 739/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38600.3555 - val_loss: 40307.5430\n",
      "Epoch 740/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38598.0039 - val_loss: 40306.7969\n",
      "Epoch 741/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38595.6484 - val_loss: 40305.8711\n",
      "Epoch 742/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38593.3555 - val_loss: 40304.7539\n",
      "Epoch 743/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38591.1758 - val_loss: 40303.5039\n",
      "Epoch 744/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38589.0039 - val_loss: 40301.8711\n",
      "Epoch 745/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38586.8516 - val_loss: 40300.7617\n",
      "Epoch 746/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38584.6445 - val_loss: 40300.2891\n",
      "Epoch 747/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38582.4883 - val_loss: 40299.4570\n",
      "Epoch 748/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38580.3242 - val_loss: 40298.3398\n",
      "Epoch 749/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38578.1758 - val_loss: 40297.5117\n",
      "Epoch 750/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38576.1406 - val_loss: 40296.6289\n",
      "Epoch 751/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38574.0391 - val_loss: 40294.8711\n",
      "Epoch 752/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38572.0039 - val_loss: 40293.7617\n",
      "Epoch 753/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38569.9609 - val_loss: 40292.9219\n",
      "Epoch 754/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38567.8477 - val_loss: 40292.2539\n",
      "Epoch 755/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38565.8125 - val_loss: 40291.1133\n",
      "Epoch 756/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38563.8125 - val_loss: 40290.0430\n",
      "Epoch 757/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38561.6523 - val_loss: 40289.2070\n",
      "Epoch 758/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38559.5469 - val_loss: 40287.3398\n",
      "Epoch 759/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38557.4961 - val_loss: 40286.2461\n",
      "Epoch 760/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38555.3555 - val_loss: 40285.5156\n",
      "Epoch 761/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38553.1484 - val_loss: 40284.4180\n",
      "Epoch 762/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38550.9883 - val_loss: 40282.8867\n",
      "Epoch 763/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38548.8125 - val_loss: 40280.7617\n",
      "Epoch 764/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38546.5156 - val_loss: 40280.2852\n",
      "Epoch 765/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38544.4492 - val_loss: 40279.9570\n",
      "Epoch 766/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38542.1758 - val_loss: 40279.1211\n",
      "Epoch 767/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38539.9961 - val_loss: 40277.9648\n",
      "Epoch 768/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38537.6836 - val_loss: 40277.2109\n",
      "Epoch 769/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38535.4961 - val_loss: 40275.9883\n",
      "Epoch 770/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38533.1875 - val_loss: 40274.7109\n",
      "Epoch 771/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38530.9883 - val_loss: 40273.2383\n",
      "Epoch 772/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38528.6758 - val_loss: 40272.3633\n",
      "Epoch 773/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38526.4961 - val_loss: 40271.3711\n",
      "Epoch 774/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38524.3555 - val_loss: 40270.0859\n",
      "Epoch 775/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38522.1875 - val_loss: 40268.7461\n",
      "Epoch 776/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38520.1836 - val_loss: 40267.7539\n",
      "Epoch 777/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38518.1875 - val_loss: 40267.2070\n",
      "Epoch 778/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38516.3125 - val_loss: 40266.6367\n",
      "Epoch 779/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38514.3242 - val_loss: 40265.8398\n",
      "Epoch 780/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38512.3555 - val_loss: 40264.6602\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 781/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38510.4531 - val_loss: 40263.8867\n",
      "Epoch 782/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38508.4844 - val_loss: 40263.2109\n",
      "Epoch 783/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38506.5039 - val_loss: 40261.8398\n",
      "Epoch 784/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38504.5391 - val_loss: 40260.5820\n",
      "Epoch 785/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38502.6445 - val_loss: 40259.4961\n",
      "Epoch 786/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38500.6523 - val_loss: 40258.5820\n",
      "Epoch 787/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38498.6836 - val_loss: 40257.3633\n",
      "Epoch 788/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38496.6875 - val_loss: 40255.8906\n",
      "Epoch 789/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38494.6758 - val_loss: 40254.4570\n",
      "Epoch 790/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38492.6445 - val_loss: 40253.7852\n",
      "Epoch 791/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38490.5508 - val_loss: 40252.7383\n",
      "Epoch 792/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38488.5391 - val_loss: 40251.2148\n",
      "Epoch 793/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38486.5039 - val_loss: 40249.5820\n",
      "Epoch 794/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38484.4883 - val_loss: 40248.8633\n",
      "Epoch 795/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38482.4492 - val_loss: 40247.6289\n",
      "Epoch 796/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38480.3242 - val_loss: 40246.0117\n",
      "Epoch 797/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38478.1523 - val_loss: 40244.4219\n",
      "Epoch 798/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38476.0469 - val_loss: 40243.1367\n",
      "Epoch 799/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38474.0039 - val_loss: 40241.9141\n",
      "Epoch 800/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38471.9844 - val_loss: 40240.8633\n",
      "Epoch 801/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38469.8594 - val_loss: 40239.3359\n",
      "Epoch 802/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38467.8594 - val_loss: 40238.0000\n",
      "Epoch 803/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38465.9609 - val_loss: 40237.2070\n",
      "Epoch 804/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38464.0039 - val_loss: 40236.6289\n",
      "Epoch 805/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38462.0469 - val_loss: 40234.7539\n",
      "Epoch 806/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38460.1797 - val_loss: 40233.3633\n",
      "Epoch 807/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38458.3242 - val_loss: 40233.0781\n",
      "Epoch 808/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38456.4609 - val_loss: 40232.6133\n",
      "Epoch 809/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38454.5156 - val_loss: 40230.7070\n",
      "Epoch 810/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38452.6445 - val_loss: 40229.0430\n",
      "Epoch 811/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38450.6875 - val_loss: 40228.4180\n",
      "Epoch 812/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38448.8477 - val_loss: 40227.9531\n",
      "Epoch 813/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38446.9844 - val_loss: 40226.9648\n",
      "Epoch 814/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38445.0391 - val_loss: 40225.3281\n",
      "Epoch 815/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38443.1758 - val_loss: 40224.1680\n",
      "Epoch 816/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38441.3477 - val_loss: 40223.8633\n",
      "Epoch 817/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38439.4609 - val_loss: 40223.1133\n",
      "Epoch 818/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38437.5039 - val_loss: 40221.7539\n",
      "Epoch 819/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38435.5508 - val_loss: 40220.5039\n",
      "Epoch 820/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38433.6875 - val_loss: 40219.8398\n",
      "Epoch 821/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38431.8242 - val_loss: 40218.7461\n",
      "Epoch 822/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38429.9531 - val_loss: 40217.4570\n",
      "Epoch 823/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38427.9883 - val_loss: 40216.6133\n",
      "Epoch 824/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38426.0156 - val_loss: 40215.7461\n",
      "Epoch 825/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38424.1445 - val_loss: 40214.7539\n",
      "Epoch 826/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38422.1875 - val_loss: 40213.7070\n",
      "Epoch 827/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38420.3555 - val_loss: 40212.8633\n",
      "Epoch 828/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38418.4961 - val_loss: 40211.6250\n",
      "Epoch 829/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38416.6445 - val_loss: 40210.6641\n",
      "Epoch 830/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38414.8242 - val_loss: 40209.5430\n",
      "Epoch 831/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38412.9961 - val_loss: 40209.0430\n",
      "Epoch 832/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38411.1406 - val_loss: 40208.5781\n",
      "Epoch 833/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38409.3125 - val_loss: 40207.4883\n",
      "Epoch 834/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38407.4531 - val_loss: 40206.8711\n",
      "Epoch 835/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38405.5117 - val_loss: 40205.7930\n",
      "Epoch 836/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38403.6445 - val_loss: 40204.6094\n",
      "Epoch 837/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38401.8125 - val_loss: 40203.7461\n",
      "Epoch 838/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38399.8594 - val_loss: 40203.1641\n",
      "Epoch 839/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38397.9961 - val_loss: 40202.6367\n",
      "Epoch 840/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38396.0430 - val_loss: 40201.2461\n",
      "Epoch 841/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38394.1758 - val_loss: 40200.2930\n",
      "Epoch 842/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38392.3203 - val_loss: 40199.5039\n",
      "Epoch 843/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38390.3555 - val_loss: 40199.1133\n",
      "Epoch 844/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38388.4609 - val_loss: 40198.3711\n",
      "Epoch 845/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38386.4961 - val_loss: 40197.1211\n",
      "Epoch 846/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38384.5117 - val_loss: 40196.1289\n",
      "Epoch 847/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38382.5391 - val_loss: 40195.2109\n",
      "Epoch 848/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38380.5469 - val_loss: 40193.7852\n",
      "Epoch 849/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38378.5469 - val_loss: 40192.4102\n",
      "Epoch 850/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38376.5469 - val_loss: 40191.3320\n",
      "Epoch 851/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38374.6445 - val_loss: 40190.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 852/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38372.6797 - val_loss: 40188.2070\n",
      "Epoch 853/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38370.8125 - val_loss: 40186.0117\n",
      "Epoch 854/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38368.8242 - val_loss: 40185.1133\n",
      "Epoch 855/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38366.8477 - val_loss: 40184.3711\n",
      "Epoch 856/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38364.8594 - val_loss: 40182.6719\n",
      "Epoch 857/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38362.9531 - val_loss: 40181.3633\n",
      "Epoch 858/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38360.9609 - val_loss: 40180.1211\n",
      "Epoch 859/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38358.9883 - val_loss: 40179.1211\n",
      "Epoch 860/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38356.9883 - val_loss: 40178.0469\n",
      "Epoch 861/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38354.9609 - val_loss: 40176.8594\n",
      "Epoch 862/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38352.9609 - val_loss: 40176.3281\n",
      "Epoch 863/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38350.9844 - val_loss: 40175.1406\n",
      "Epoch 864/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38349.0039 - val_loss: 40174.2148\n",
      "Epoch 865/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38347.1797 - val_loss: 40173.5117\n",
      "Epoch 866/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38345.4531 - val_loss: 40172.7539\n",
      "Epoch 867/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38343.5469 - val_loss: 40171.7383\n",
      "Epoch 868/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38341.8242 - val_loss: 40171.1680\n",
      "Epoch 869/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38340.0117 - val_loss: 40170.5117\n",
      "Epoch 870/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38338.3242 - val_loss: 40169.7891\n",
      "Epoch 871/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38336.5469 - val_loss: 40169.4570\n",
      "Epoch 872/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38334.9609 - val_loss: 40168.3633\n",
      "Epoch 873/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38333.1797 - val_loss: 40167.3281\n",
      "Epoch 874/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38331.5000 - val_loss: 40166.8711\n",
      "Epoch 875/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38329.8203 - val_loss: 40166.4609\n",
      "Epoch 876/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38328.0156 - val_loss: 40166.0352\n",
      "Epoch 877/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38326.3477 - val_loss: 40165.3281\n",
      "Epoch 878/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38324.5469 - val_loss: 40164.9102\n",
      "Epoch 879/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38322.8594 - val_loss: 40164.2070\n",
      "Epoch 880/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38321.0469 - val_loss: 40163.3398\n",
      "Epoch 881/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38319.4492 - val_loss: 40163.1719\n",
      "Epoch 882/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38317.6445 - val_loss: 40162.7617\n",
      "Epoch 883/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38315.9531 - val_loss: 40161.7930\n",
      "Epoch 884/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38314.1445 - val_loss: 40160.9219\n",
      "Epoch 885/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38312.4531 - val_loss: 40160.7148\n",
      "Epoch 886/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38310.6445 - val_loss: 40160.5898\n",
      "Epoch 887/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38308.9609 - val_loss: 40160.1133\n",
      "Epoch 888/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38307.1523 - val_loss: 40159.2617\n",
      "Epoch 889/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38305.4609 - val_loss: 40158.3633\n",
      "Epoch 890/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38303.6523 - val_loss: 40157.7930\n",
      "Epoch 891/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38301.9609 - val_loss: 40157.8711\n",
      "Epoch 892/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38300.1758 - val_loss: 40157.7383\n",
      "Epoch 893/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38298.4883 - val_loss: 40156.9180\n",
      "Epoch 894/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38296.6875 - val_loss: 40156.3320\n",
      "Epoch 895/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38295.0039 - val_loss: 40155.6289\n",
      "Epoch 896/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38293.3242 - val_loss: 40155.5820\n",
      "Epoch 897/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38291.5391 - val_loss: 40155.2930\n",
      "Epoch 898/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38289.8594 - val_loss: 40154.6133\n",
      "Epoch 899/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38288.1445 - val_loss: 40153.7539\n",
      "Epoch 900/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38286.4609 - val_loss: 40153.4648\n",
      "Epoch 901/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38284.6758 - val_loss: 40153.3633\n",
      "Epoch 902/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38282.9961 - val_loss: 40152.7617\n",
      "Epoch 903/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38281.3125 - val_loss: 40151.3594\n",
      "Epoch 904/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38279.5039 - val_loss: 40150.5820\n",
      "Epoch 905/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38277.8203 - val_loss: 40151.2461\n",
      "Epoch 906/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38276.0391 - val_loss: 40150.9180\n",
      "Epoch 907/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38274.3477 - val_loss: 40149.8633\n",
      "Epoch 908/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38272.6445 - val_loss: 40149.1367\n",
      "Epoch 909/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38270.9609 - val_loss: 40149.0000\n",
      "Epoch 910/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38269.1758 - val_loss: 40148.5430\n",
      "Epoch 911/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38267.4961 - val_loss: 40147.8633\n",
      "Epoch 912/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38265.8125 - val_loss: 40147.3320\n",
      "Epoch 913/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38264.0391 - val_loss: 40147.1289\n",
      "Epoch 914/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38262.3555 - val_loss: 40146.5898\n",
      "Epoch 915/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38260.6445 - val_loss: 40145.7383\n",
      "Epoch 916/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38258.9609 - val_loss: 40145.0430\n",
      "Epoch 917/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38257.1797 - val_loss: 40144.7344\n",
      "Epoch 918/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38255.5039 - val_loss: 40144.2070\n",
      "Epoch 919/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38253.8242 - val_loss: 40144.3320\n",
      "Epoch 920/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38252.0430 - val_loss: 40144.1133\n",
      "Epoch 921/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38250.4492 - val_loss: 40143.6406\n",
      "Epoch 922/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38248.6523 - val_loss: 40142.9844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 923/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38246.9961 - val_loss: 40142.4570\n",
      "Epoch 924/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38245.3125 - val_loss: 40142.4961\n",
      "Epoch 925/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38243.5391 - val_loss: 40141.7891\n",
      "Epoch 926/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38241.9531 - val_loss: 40141.1289\n",
      "Epoch 927/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38240.1523 - val_loss: 40140.9961\n",
      "Epoch 928/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38238.4961 - val_loss: 40140.8398\n",
      "Epoch 929/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 38236.8125 - val_loss: 40140.5430\n",
      "Epoch 930/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38235.0391 - val_loss: 40139.4102\n",
      "Epoch 931/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38233.4492 - val_loss: 40139.0430\n",
      "Epoch 932/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38231.6523 - val_loss: 40139.2539\n",
      "Epoch 933/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 38229.9961 - val_loss: 40138.6289\n",
      "Epoch 934/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38228.3125 - val_loss: 40137.4570\n",
      "Epoch 935/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38226.5469 - val_loss: 40136.8750\n",
      "Epoch 936/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38224.9609 - val_loss: 40136.7539\n",
      "Epoch 937/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38223.1875 - val_loss: 40136.6250\n",
      "Epoch 938/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38221.5039 - val_loss: 40135.4961\n",
      "Epoch 939/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38219.8477 - val_loss: 40134.7031\n",
      "Epoch 940/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38218.1445 - val_loss: 40134.6211\n",
      "Epoch 941/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38216.4961 - val_loss: 40133.6289\n",
      "Epoch 942/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38214.8242 - val_loss: 40132.7461\n",
      "Epoch 943/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38213.0469 - val_loss: 40132.3711\n",
      "Epoch 944/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38211.4609 - val_loss: 40132.2383\n",
      "Epoch 945/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38209.6875 - val_loss: 40131.5898\n",
      "Epoch 946/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38208.0391 - val_loss: 40130.7031\n",
      "Epoch 947/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38206.4531 - val_loss: 40130.4141\n",
      "Epoch 948/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38204.6875 - val_loss: 40130.1133\n",
      "Epoch 949/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38203.0156 - val_loss: 40129.3789\n",
      "Epoch 950/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38201.3555 - val_loss: 40128.9102\n",
      "Epoch 951/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38199.6758 - val_loss: 40128.6094\n",
      "Epoch 952/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38198.0039 - val_loss: 40127.7930\n",
      "Epoch 953/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38196.3477 - val_loss: 40127.2930\n",
      "Epoch 954/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38194.6523 - val_loss: 40127.1719\n",
      "Epoch 955/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38192.9961 - val_loss: 40126.2617\n",
      "Epoch 956/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38191.3242 - val_loss: 40125.5039\n",
      "Epoch 957/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38189.6445 - val_loss: 40125.1211\n",
      "Epoch 958/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38187.9883 - val_loss: 40124.6367\n",
      "Epoch 959/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38186.3125 - val_loss: 40123.4570\n",
      "Epoch 960/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38184.5508 - val_loss: 40123.0039\n",
      "Epoch 961/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38182.9844 - val_loss: 40122.4883\n",
      "Epoch 962/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38181.3125 - val_loss: 40121.9531\n",
      "Epoch 963/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38179.5391 - val_loss: 40121.0898\n",
      "Epoch 964/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38177.9609 - val_loss: 40120.8633\n",
      "Epoch 965/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38176.1875 - val_loss: 40120.4102\n",
      "Epoch 966/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38174.5391 - val_loss: 40119.5430\n",
      "Epoch 967/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38172.9531 - val_loss: 40118.4883\n",
      "Epoch 968/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38171.1797 - val_loss: 40117.6719\n",
      "Epoch 969/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38169.5039 - val_loss: 40117.3398\n",
      "Epoch 970/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 38167.8477 - val_loss: 40116.4219\n",
      "Epoch 971/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38166.1445 - val_loss: 40115.6289\n",
      "Epoch 972/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38164.4961 - val_loss: 40115.1367\n",
      "Epoch 973/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38162.8203 - val_loss: 40114.0781\n",
      "Epoch 974/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38161.0391 - val_loss: 40113.4219\n",
      "Epoch 975/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38159.4531 - val_loss: 40113.3711\n",
      "Epoch 976/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38157.6758 - val_loss: 40112.3789\n",
      "Epoch 977/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38155.9961 - val_loss: 40111.3398\n",
      "Epoch 978/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38154.3242 - val_loss: 40111.2617\n",
      "Epoch 979/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38152.5508 - val_loss: 40110.7461\n",
      "Epoch 980/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38150.9883 - val_loss: 40109.9648\n",
      "Epoch 981/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38149.3203 - val_loss: 40109.1719\n",
      "Epoch 982/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38147.5508 - val_loss: 40108.5898\n",
      "Epoch 983/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38145.9883 - val_loss: 40107.9570\n",
      "Epoch 984/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38144.3203 - val_loss: 40106.9883\n",
      "Epoch 985/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38142.5508 - val_loss: 40106.6211\n",
      "Epoch 986/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38140.9844 - val_loss: 40106.4883\n",
      "Epoch 987/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38139.3203 - val_loss: 40105.7617\n",
      "Epoch 988/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38137.5508 - val_loss: 40104.6602\n",
      "Epoch 989/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38135.9844 - val_loss: 40103.6289\n",
      "Epoch 990/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38134.3125 - val_loss: 40103.1602\n",
      "Epoch 991/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38132.5469 - val_loss: 40103.2852\n",
      "Epoch 992/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38130.9844 - val_loss: 40102.5430\n",
      "Epoch 993/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38129.3203 - val_loss: 40101.2930\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 994/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38127.6445 - val_loss: 40100.6719\n",
      "Epoch 995/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38125.9883 - val_loss: 40100.3398\n",
      "Epoch 996/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38124.3203 - val_loss: 40099.7070\n",
      "Epoch 997/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38122.6445 - val_loss: 40098.7539\n",
      "Epoch 998/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38121.0000 - val_loss: 40098.5781\n",
      "Epoch 999/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38119.3555 - val_loss: 40098.3711\n",
      "Epoch 1000/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38117.6797 - val_loss: 40097.9570\n",
      "Epoch 1001/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38116.0117 - val_loss: 40097.3711\n",
      "Epoch 1002/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38114.4492 - val_loss: 40096.5820\n",
      "Epoch 1003/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38112.6875 - val_loss: 40095.9609\n",
      "Epoch 1004/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38111.0391 - val_loss: 40095.5039\n",
      "Epoch 1005/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38109.4609 - val_loss: 40094.5117\n",
      "Epoch 1006/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38107.8125 - val_loss: 40093.9102\n",
      "Epoch 1007/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38106.0469 - val_loss: 40094.1289\n",
      "Epoch 1008/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38104.4844 - val_loss: 40093.3906\n",
      "Epoch 1009/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38102.8203 - val_loss: 40092.0430\n",
      "Epoch 1010/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38101.1445 - val_loss: 40091.2383\n",
      "Epoch 1011/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38099.4961 - val_loss: 40091.6406\n",
      "Epoch 1012/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38097.8438 - val_loss: 40091.2070\n",
      "Epoch 1013/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38096.1523 - val_loss: 40089.6211\n",
      "Epoch 1014/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38094.5039 - val_loss: 40088.8789\n",
      "Epoch 1015/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38092.8477 - val_loss: 40089.1211\n",
      "Epoch 1016/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38091.1523 - val_loss: 40088.3633\n",
      "Epoch 1017/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38089.5039 - val_loss: 40087.4844\n",
      "Epoch 1018/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38087.8477 - val_loss: 40086.7539\n",
      "Epoch 1019/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38086.1562 - val_loss: 40085.7383\n",
      "Epoch 1020/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38084.5039 - val_loss: 40086.2656\n",
      "Epoch 1021/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38082.9531 - val_loss: 40086.3320\n",
      "Epoch 1022/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38081.1797 - val_loss: 40084.4102\n",
      "Epoch 1023/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38079.5117 - val_loss: 40083.7383\n",
      "Epoch 1024/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38077.8555 - val_loss: 40083.9883\n",
      "Epoch 1025/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38076.1797 - val_loss: 40082.8633\n",
      "Epoch 1026/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38074.5039 - val_loss: 40082.4102\n",
      "Epoch 1027/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38072.8438 - val_loss: 40081.7383\n",
      "Epoch 1028/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38071.1445 - val_loss: 40081.3633\n",
      "Epoch 1029/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38069.4844 - val_loss: 40080.6289\n",
      "Epoch 1030/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38067.8125 - val_loss: 40079.9570\n",
      "Epoch 1031/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38066.0391 - val_loss: 40079.0898\n",
      "Epoch 1032/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38064.4492 - val_loss: 40078.7383\n",
      "Epoch 1033/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38062.6797 - val_loss: 40078.2539\n",
      "Epoch 1034/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38060.9961 - val_loss: 40077.0781\n",
      "Epoch 1035/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38059.1914 - val_loss: 40076.5430\n",
      "Epoch 1036/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38057.5039 - val_loss: 40075.7617\n",
      "Epoch 1037/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38055.8125 - val_loss: 40074.6133\n",
      "Epoch 1038/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38053.9961 - val_loss: 40074.4570\n",
      "Epoch 1039/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 38052.1797 - val_loss: 40073.6211\n",
      "Epoch 1040/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38050.4492 - val_loss: 40072.1250\n",
      "Epoch 1041/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38048.5039 - val_loss: 40071.3320\n",
      "Epoch 1042/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38046.6523 - val_loss: 40070.8789\n",
      "Epoch 1043/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38044.8203 - val_loss: 40069.5898\n",
      "Epoch 1044/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38042.9609 - val_loss: 40067.9219\n",
      "Epoch 1045/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38040.9844 - val_loss: 40067.7930\n",
      "Epoch 1046/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38038.9961 - val_loss: 40067.0430\n",
      "Epoch 1047/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38037.0039 - val_loss: 40065.7852\n",
      "Epoch 1048/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38034.9961 - val_loss: 40065.2539\n",
      "Epoch 1049/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38032.9609 - val_loss: 40064.2617\n",
      "Epoch 1050/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38030.9531 - val_loss: 40063.0469\n",
      "Epoch 1051/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38028.8477 - val_loss: 40061.8398\n",
      "Epoch 1052/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38026.8125 - val_loss: 40061.1406\n",
      "Epoch 1053/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38024.5508 - val_loss: 40060.4570\n",
      "Epoch 1054/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38022.4961 - val_loss: 40058.8633\n",
      "Epoch 1055/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38020.4492 - val_loss: 40057.2461\n",
      "Epoch 1056/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 38018.1523 - val_loss: 40056.7070\n",
      "Epoch 1057/10000\n",
      "1164/1164 [==============================] - 0s 26us/step - loss: 38015.9570 - val_loss: 40056.2852\n",
      "Epoch 1058/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38013.6445 - val_loss: 40054.5117\n",
      "Epoch 1059/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38011.3555 - val_loss: 40052.6719\n",
      "Epoch 1060/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38009.1797 - val_loss: 40051.5898\n",
      "Epoch 1061/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 38007.0039 - val_loss: 40050.8281\n",
      "Epoch 1062/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 38004.8203 - val_loss: 40050.4141\n",
      "Epoch 1063/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 38002.6445 - val_loss: 40049.3789\n",
      "Epoch 1064/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 38000.5039 - val_loss: 40048.9961\n",
      "Epoch 1065/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37998.4492 - val_loss: 40048.3320\n",
      "Epoch 1066/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37996.1875 - val_loss: 40047.0352\n",
      "Epoch 1067/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37994.0469 - val_loss: 40046.3789\n",
      "Epoch 1068/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37991.9961 - val_loss: 40046.5117\n",
      "Epoch 1069/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37989.9883 - val_loss: 40045.1680\n",
      "Epoch 1070/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37988.0039 - val_loss: 40044.7383\n",
      "Epoch 1071/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37986.0391 - val_loss: 40045.7852\n",
      "Epoch 1072/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37984.1562 - val_loss: 40045.2852\n",
      "Epoch 1073/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37982.3203 - val_loss: 40043.4531\n",
      "Epoch 1074/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37980.4883 - val_loss: 40043.4883\n",
      "Epoch 1075/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37978.6797 - val_loss: 40044.5898\n",
      "Epoch 1076/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37976.9883 - val_loss: 40043.8789\n",
      "Epoch 1077/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37975.1875 - val_loss: 40042.0820\n",
      "Epoch 1078/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37973.5156 - val_loss: 40041.7852\n",
      "Epoch 1079/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37971.8555 - val_loss: 40042.7148\n",
      "Epoch 1080/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37970.1523 - val_loss: 40041.6211\n",
      "Epoch 1081/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37968.3555 - val_loss: 40039.8633\n",
      "Epoch 1082/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37966.5039 - val_loss: 40039.2617\n",
      "Epoch 1083/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37964.6523 - val_loss: 40039.4961\n",
      "Epoch 1084/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37962.8203 - val_loss: 40038.0352\n",
      "Epoch 1085/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37960.9609 - val_loss: 40036.2617\n",
      "Epoch 1086/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37959.0039 - val_loss: 40037.2852\n",
      "Epoch 1087/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37957.0391 - val_loss: 40037.4570\n",
      "Epoch 1088/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37955.1875 - val_loss: 40035.5391\n",
      "Epoch 1089/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37953.4492 - val_loss: 40034.7109\n",
      "Epoch 1090/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37951.5039 - val_loss: 40035.1250\n",
      "Epoch 1091/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37949.6445 - val_loss: 40034.0039\n",
      "Epoch 1092/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37947.6562 - val_loss: 40032.5352\n",
      "Epoch 1093/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37945.8086 - val_loss: 40033.3398\n",
      "Epoch 1094/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37943.8555 - val_loss: 40034.1641\n",
      "Epoch 1095/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37941.9844 - val_loss: 40033.3789\n",
      "Epoch 1096/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37940.0117 - val_loss: 40032.7891\n",
      "Epoch 1097/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37938.0391 - val_loss: 40033.3867\n",
      "Epoch 1098/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37936.1797 - val_loss: 40033.4844\n",
      "Epoch 1099/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37934.4531 - val_loss: 40032.7070\n",
      "Epoch 1100/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37932.5391 - val_loss: 40032.7383\n",
      "Epoch 1101/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37930.8125 - val_loss: 40033.6602\n",
      "Epoch 1102/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37928.9961 - val_loss: 40033.6211\n",
      "Epoch 1103/10000\n",
      "1164/1164 [==============================] - 0s 26us/step - loss: 37927.1797 - val_loss: 40032.7383\n",
      "Epoch 1104/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37925.4609 - val_loss: 40032.1094\n",
      "Epoch 1105/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37923.6445 - val_loss: 40032.2070\n",
      "Epoch 1106/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37921.9570 - val_loss: 40031.2070\n",
      "Epoch 1107/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37920.1445 - val_loss: 40030.7656\n",
      "Epoch 1108/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37918.4492 - val_loss: 40030.2930\n",
      "Epoch 1109/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37916.5391 - val_loss: 40029.3320\n",
      "Epoch 1110/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37914.8125 - val_loss: 40029.5352\n",
      "Epoch 1111/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37912.9961 - val_loss: 40029.2461\n",
      "Epoch 1112/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37911.1797 - val_loss: 40027.6602\n",
      "Epoch 1113/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37909.4492 - val_loss: 40026.7617\n",
      "Epoch 1114/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37907.5117 - val_loss: 40028.2383\n",
      "Epoch 1115/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37905.6797 - val_loss: 40027.9883\n",
      "Epoch 1116/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37903.8555 - val_loss: 40025.1602\n",
      "Epoch 1117/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37902.0039 - val_loss: 40025.4180\n",
      "Epoch 1118/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37900.1562 - val_loss: 40027.0352\n",
      "Epoch 1119/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37898.3477 - val_loss: 40025.2617\n",
      "Epoch 1120/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37896.5039 - val_loss: 40022.6289\n",
      "Epoch 1121/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37894.6875 - val_loss: 40023.8789\n",
      "Epoch 1122/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37892.9609 - val_loss: 40025.0430\n",
      "Epoch 1123/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37891.1562 - val_loss: 40021.5898\n",
      "Epoch 1124/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37889.4609 - val_loss: 40020.8789\n",
      "Epoch 1125/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37887.6797 - val_loss: 40022.7930\n",
      "Epoch 1126/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37885.9961 - val_loss: 40021.2461\n",
      "Epoch 1127/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37884.3125 - val_loss: 40018.6367\n",
      "Epoch 1128/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37882.5156 - val_loss: 40019.7148\n",
      "Epoch 1129/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37880.8555 - val_loss: 40020.7617\n",
      "Epoch 1130/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37879.1562 - val_loss: 40018.2461\n",
      "Epoch 1131/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37877.5039 - val_loss: 40016.7031\n",
      "Epoch 1132/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37875.8555 - val_loss: 40017.3789\n",
      "Epoch 1133/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37874.1562 - val_loss: 40017.6211\n",
      "Epoch 1134/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37872.4961 - val_loss: 40015.3789\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1135/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37870.8555 - val_loss: 40014.2461\n",
      "Epoch 1136/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37869.1562 - val_loss: 40015.0859\n",
      "Epoch 1137/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37867.4961 - val_loss: 40013.9570\n",
      "Epoch 1138/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37865.8203 - val_loss: 40012.2617\n",
      "Epoch 1139/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37864.1445 - val_loss: 40012.0039\n",
      "Epoch 1140/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37862.4961 - val_loss: 40011.7656\n",
      "Epoch 1141/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37860.8438 - val_loss: 40010.5430\n",
      "Epoch 1142/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37859.1445 - val_loss: 40009.5039\n",
      "Epoch 1143/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37857.4961 - val_loss: 40009.2656\n",
      "Epoch 1144/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37855.8438 - val_loss: 40008.3789\n",
      "Epoch 1145/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37854.1523 - val_loss: 40007.0430\n",
      "Epoch 1146/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37852.4961 - val_loss: 40006.8789\n",
      "Epoch 1147/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37850.8438 - val_loss: 40006.9648\n",
      "Epoch 1148/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37849.1562 - val_loss: 40006.3633\n",
      "Epoch 1149/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37847.5039 - val_loss: 40004.7383\n",
      "Epoch 1150/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37845.8477 - val_loss: 40004.2109\n",
      "Epoch 1151/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37844.1523 - val_loss: 40003.9961\n",
      "Epoch 1152/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37842.4961 - val_loss: 40002.8633\n",
      "Epoch 1153/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37840.8203 - val_loss: 40002.3711\n",
      "Epoch 1154/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37839.1445 - val_loss: 40001.4961\n",
      "Epoch 1155/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37837.4961 - val_loss: 40000.8633\n",
      "Epoch 1156/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37835.8125 - val_loss: 40000.5469\n",
      "Epoch 1157/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37834.0469 - val_loss: 40000.5117\n",
      "Epoch 1158/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37832.4844 - val_loss: 39999.7148\n",
      "Epoch 1159/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37830.8125 - val_loss: 39999.0469\n",
      "Epoch 1160/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37829.0391 - val_loss: 39998.4844\n",
      "Epoch 1161/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37827.4531 - val_loss: 39997.8789\n",
      "Epoch 1162/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37825.6797 - val_loss: 39997.6211\n",
      "Epoch 1163/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37824.0039 - val_loss: 39996.8398\n",
      "Epoch 1164/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37822.3438 - val_loss: 39995.8789\n",
      "Epoch 1165/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37820.6523 - val_loss: 39995.0352\n",
      "Epoch 1166/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37818.9961 - val_loss: 39994.8867\n",
      "Epoch 1167/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37817.3203 - val_loss: 39993.7109\n",
      "Epoch 1168/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37815.5391 - val_loss: 39993.5898\n",
      "Epoch 1169/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37813.9570 - val_loss: 39993.4180\n",
      "Epoch 1170/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37812.1797 - val_loss: 39992.2461\n",
      "Epoch 1171/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37810.5039 - val_loss: 39991.7617\n",
      "Epoch 1172/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37808.8438 - val_loss: 39991.8711\n",
      "Epoch 1173/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37807.0469 - val_loss: 39991.2617\n",
      "Epoch 1174/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37805.4609 - val_loss: 39989.8750\n",
      "Epoch 1175/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37803.6875 - val_loss: 39989.2539\n",
      "Epoch 1176/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37802.0039 - val_loss: 39989.9570\n",
      "Epoch 1177/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37800.3477 - val_loss: 39989.3633\n",
      "Epoch 1178/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37798.6445 - val_loss: 39987.9883\n",
      "Epoch 1179/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37796.9609 - val_loss: 39987.9883\n",
      "Epoch 1180/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37795.1797 - val_loss: 39987.7539\n",
      "Epoch 1181/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37793.5039 - val_loss: 39987.0039\n",
      "Epoch 1182/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37791.8203 - val_loss: 39985.7070\n",
      "Epoch 1183/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37790.0391 - val_loss: 39985.7070\n",
      "Epoch 1184/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37788.3555 - val_loss: 39985.7070\n",
      "Epoch 1185/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37786.6523 - val_loss: 39984.5781\n",
      "Epoch 1186/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37784.9883 - val_loss: 39984.2539\n",
      "Epoch 1187/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37783.1875 - val_loss: 39984.6367\n",
      "Epoch 1188/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37781.5039 - val_loss: 39983.5352\n",
      "Epoch 1189/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37779.8438 - val_loss: 39983.2070\n",
      "Epoch 1190/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37778.0430 - val_loss: 39983.2930\n",
      "Epoch 1191/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37776.4492 - val_loss: 39982.7617\n",
      "Epoch 1192/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37774.6562 - val_loss: 39981.3906\n",
      "Epoch 1193/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37772.9883 - val_loss: 39981.7930\n",
      "Epoch 1194/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37771.1875 - val_loss: 39981.3320\n",
      "Epoch 1195/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37769.5039 - val_loss: 39980.2383\n",
      "Epoch 1196/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37767.8203 - val_loss: 39980.0039\n",
      "Epoch 1197/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37766.0391 - val_loss: 39979.9570\n",
      "Epoch 1198/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37764.3516 - val_loss: 39978.0430\n",
      "Epoch 1199/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37762.6445 - val_loss: 39978.5430\n",
      "Epoch 1200/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37760.9570 - val_loss: 39977.8633\n",
      "Epoch 1201/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37759.1562 - val_loss: 39976.2383\n",
      "Epoch 1202/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37757.4844 - val_loss: 39976.4961\n",
      "Epoch 1203/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37755.6914 - val_loss: 39976.0469\n",
      "Epoch 1204/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37753.9961 - val_loss: 39974.3867\n",
      "Epoch 1205/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 24us/step - loss: 37752.3086 - val_loss: 39974.9180\n",
      "Epoch 1206/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37750.5117 - val_loss: 39974.2969\n",
      "Epoch 1207/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37748.8203 - val_loss: 39973.0117\n",
      "Epoch 1208/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37747.0156 - val_loss: 39972.9219\n",
      "Epoch 1209/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37745.3203 - val_loss: 39972.9883\n",
      "Epoch 1210/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37743.5156 - val_loss: 39971.4219\n",
      "Epoch 1211/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37741.8203 - val_loss: 39971.4180\n",
      "Epoch 1212/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37740.0391 - val_loss: 39970.7656\n",
      "Epoch 1213/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37738.3516 - val_loss: 39969.8711\n",
      "Epoch 1214/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37736.5391 - val_loss: 39969.7383\n",
      "Epoch 1215/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37734.8438 - val_loss: 39969.3320\n",
      "Epoch 1216/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37733.0430 - val_loss: 39968.5039\n",
      "Epoch 1217/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37731.3516 - val_loss: 39968.1641\n",
      "Epoch 1218/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37729.5469 - val_loss: 39967.5430\n",
      "Epoch 1219/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37727.8516 - val_loss: 39965.9883\n",
      "Epoch 1220/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37726.0391 - val_loss: 39965.8711\n",
      "Epoch 1221/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37724.3203 - val_loss: 39966.2031\n",
      "Epoch 1222/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37722.5039 - val_loss: 39964.7461\n",
      "Epoch 1223/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37720.8125 - val_loss: 39962.6641\n",
      "Epoch 1224/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37719.0000 - val_loss: 39962.2070\n",
      "Epoch 1225/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37717.1562 - val_loss: 39962.9961\n",
      "Epoch 1226/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37715.4609 - val_loss: 39962.4648\n",
      "Epoch 1227/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37713.6445 - val_loss: 39960.7461\n",
      "Epoch 1228/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37711.8516 - val_loss: 39960.4570\n",
      "Epoch 1229/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37710.0156 - val_loss: 39960.3320\n",
      "Epoch 1230/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37708.1914 - val_loss: 39959.4570\n",
      "Epoch 1231/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37706.4844 - val_loss: 39958.5391\n",
      "Epoch 1232/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37704.6445 - val_loss: 39958.2383\n",
      "Epoch 1233/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 37702.8203 - val_loss: 39957.5352\n",
      "Epoch 1234/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37700.9883 - val_loss: 39956.6211\n",
      "Epoch 1235/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37699.1445 - val_loss: 39954.9570\n",
      "Epoch 1236/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37697.3203 - val_loss: 39954.3789\n",
      "Epoch 1237/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37695.4609 - val_loss: 39954.8320\n",
      "Epoch 1238/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37693.5156 - val_loss: 39953.0898\n",
      "Epoch 1239/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37691.6484 - val_loss: 39951.9609\n",
      "Epoch 1240/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37689.6914 - val_loss: 39952.1289\n",
      "Epoch 1241/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37687.8125 - val_loss: 39951.5430\n",
      "Epoch 1242/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37685.8438 - val_loss: 39949.7070\n",
      "Epoch 1243/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37683.8438 - val_loss: 39949.3281\n",
      "Epoch 1244/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37681.8438 - val_loss: 39949.5352\n",
      "Epoch 1245/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37679.8203 - val_loss: 39947.2148\n",
      "Epoch 1246/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37677.8125 - val_loss: 39945.8867\n",
      "Epoch 1247/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37675.6836 - val_loss: 39946.7500\n",
      "Epoch 1248/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37673.6484 - val_loss: 39945.2852\n",
      "Epoch 1249/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37671.5508 - val_loss: 39943.4531\n",
      "Epoch 1250/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37669.6445 - val_loss: 39943.7148\n",
      "Epoch 1251/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37667.6484 - val_loss: 39943.1367\n",
      "Epoch 1252/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37665.6562 - val_loss: 39942.3281\n",
      "Epoch 1253/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37663.5508 - val_loss: 39941.9961\n",
      "Epoch 1254/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37661.5391 - val_loss: 39942.2617\n",
      "Epoch 1255/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37659.5156 - val_loss: 39941.4883\n",
      "Epoch 1256/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37657.5000 - val_loss: 39941.3750\n",
      "Epoch 1257/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37655.4883 - val_loss: 39940.5781\n",
      "Epoch 1258/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37653.4609 - val_loss: 39940.7891\n",
      "Epoch 1259/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37651.4609 - val_loss: 39940.3906\n",
      "Epoch 1260/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37649.4609 - val_loss: 39939.8867\n",
      "Epoch 1261/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37647.4961 - val_loss: 39940.4883\n",
      "Epoch 1262/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37645.5469 - val_loss: 39939.8633\n",
      "Epoch 1263/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37643.6797 - val_loss: 39939.3789\n",
      "Epoch 1264/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37641.8164 - val_loss: 39938.8789\n",
      "Epoch 1265/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37639.8555 - val_loss: 39939.2539\n",
      "Epoch 1266/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37637.9961 - val_loss: 39938.6367\n",
      "Epoch 1267/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37636.0391 - val_loss: 39937.7969\n",
      "Epoch 1268/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37634.1836 - val_loss: 39937.1289\n",
      "Epoch 1269/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37632.3438 - val_loss: 39937.0117\n",
      "Epoch 1270/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37630.4844 - val_loss: 39937.6719\n",
      "Epoch 1271/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37628.5508 - val_loss: 39936.6133\n",
      "Epoch 1272/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37626.8164 - val_loss: 39936.6680\n",
      "Epoch 1273/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37624.9961 - val_loss: 39935.7383\n",
      "Epoch 1274/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37623.1484 - val_loss: 39935.8594\n",
      "Epoch 1275/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37621.3516 - val_loss: 39936.2852\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1276/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37619.5039 - val_loss: 39935.0039\n",
      "Epoch 1277/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37617.6797 - val_loss: 39934.4141\n",
      "Epoch 1278/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37615.8555 - val_loss: 39934.6289\n",
      "Epoch 1279/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37614.0039 - val_loss: 39933.5469\n",
      "Epoch 1280/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37612.1562 - val_loss: 39933.2500\n",
      "Epoch 1281/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37610.3203 - val_loss: 39933.4219\n",
      "Epoch 1282/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37608.4609 - val_loss: 39932.3320\n",
      "Epoch 1283/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37606.5156 - val_loss: 39931.0352\n",
      "Epoch 1284/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37604.6484 - val_loss: 39931.0156\n",
      "Epoch 1285/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37602.6914 - val_loss: 39930.6406\n",
      "Epoch 1286/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37600.8438 - val_loss: 39930.8320\n",
      "Epoch 1287/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37598.9844 - val_loss: 39929.7461\n",
      "Epoch 1288/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37597.0039 - val_loss: 39928.8594\n",
      "Epoch 1289/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37595.0391 - val_loss: 39928.5859\n",
      "Epoch 1290/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37593.1875 - val_loss: 39926.3320\n",
      "Epoch 1291/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37591.3438 - val_loss: 39926.1250\n",
      "Epoch 1292/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37589.4492 - val_loss: 39926.0898\n",
      "Epoch 1293/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37587.4883 - val_loss: 39924.7383\n",
      "Epoch 1294/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37585.5156 - val_loss: 39924.6289\n",
      "Epoch 1295/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37583.6445 - val_loss: 39923.8398\n",
      "Epoch 1296/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37581.6797 - val_loss: 39922.9961\n",
      "Epoch 1297/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37579.8125 - val_loss: 39921.4961\n",
      "Epoch 1298/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37577.8555 - val_loss: 39921.2461\n",
      "Epoch 1299/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37575.9570 - val_loss: 39919.7930\n",
      "Epoch 1300/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37573.9531 - val_loss: 39919.8711\n",
      "Epoch 1301/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37571.8555 - val_loss: 39918.5898\n",
      "Epoch 1302/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37569.8438 - val_loss: 39916.6602\n",
      "Epoch 1303/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37567.8125 - val_loss: 39917.7461\n",
      "Epoch 1304/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37565.6836 - val_loss: 39915.9883\n",
      "Epoch 1305/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37563.6797 - val_loss: 39914.9883\n",
      "Epoch 1306/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37561.6562 - val_loss: 39914.2656\n",
      "Epoch 1307/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37559.6797 - val_loss: 39913.9102\n",
      "Epoch 1308/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37557.6484 - val_loss: 39912.7656\n",
      "Epoch 1309/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37555.5508 - val_loss: 39911.3867\n",
      "Epoch 1310/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37553.5469 - val_loss: 39912.4961\n",
      "Epoch 1311/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37551.5039 - val_loss: 39911.2383\n",
      "Epoch 1312/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37549.5039 - val_loss: 39910.6367\n",
      "Epoch 1313/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37547.5117 - val_loss: 39910.3867\n",
      "Epoch 1314/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37545.5156 - val_loss: 39909.3633\n",
      "Epoch 1315/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37543.5039 - val_loss: 39907.8867\n",
      "Epoch 1316/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37541.5039 - val_loss: 39908.4609\n",
      "Epoch 1317/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37539.5117 - val_loss: 39907.7070\n",
      "Epoch 1318/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37537.5156 - val_loss: 39906.7070\n",
      "Epoch 1319/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37535.5156 - val_loss: 39906.5430\n",
      "Epoch 1320/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37533.5117 - val_loss: 39906.4883\n",
      "Epoch 1321/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37531.5156 - val_loss: 39904.8867\n",
      "Epoch 1322/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37529.5156 - val_loss: 39904.4883\n",
      "Epoch 1323/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37527.5156 - val_loss: 39903.7461\n",
      "Epoch 1324/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37525.5039 - val_loss: 39903.6367\n",
      "Epoch 1325/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37523.5039 - val_loss: 39903.3867\n",
      "Epoch 1326/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37521.5000 - val_loss: 39901.8281\n",
      "Epoch 1327/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37519.4961 - val_loss: 39901.5430\n",
      "Epoch 1328/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37517.4961 - val_loss: 39901.6211\n",
      "Epoch 1329/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37515.4844 - val_loss: 39900.5039\n",
      "Epoch 1330/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37513.4492 - val_loss: 39901.3633\n",
      "Epoch 1331/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37511.3203 - val_loss: 39899.8789\n",
      "Epoch 1332/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37509.3086 - val_loss: 39899.4883\n",
      "Epoch 1333/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37507.1875 - val_loss: 39899.6289\n",
      "Epoch 1334/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37505.1562 - val_loss: 39898.9883\n",
      "Epoch 1335/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37503.1445 - val_loss: 39897.6094\n",
      "Epoch 1336/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37501.0469 - val_loss: 39896.5039\n",
      "Epoch 1337/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37499.0430 - val_loss: 39896.2656\n",
      "Epoch 1338/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37497.0156 - val_loss: 39894.3711\n",
      "Epoch 1339/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37495.0117 - val_loss: 39894.0391\n",
      "Epoch 1340/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37492.9961 - val_loss: 39893.3633\n",
      "Epoch 1341/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37490.9609 - val_loss: 39891.5039\n",
      "Epoch 1342/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37488.8555 - val_loss: 39890.8281\n",
      "Epoch 1343/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37486.8203 - val_loss: 39889.7344\n",
      "Epoch 1344/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37484.6797 - val_loss: 39889.0430\n",
      "Epoch 1345/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37482.5391 - val_loss: 39889.1367\n",
      "Epoch 1346/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 21us/step - loss: 37480.4961 - val_loss: 39887.5859\n",
      "Epoch 1347/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37478.3516 - val_loss: 39887.1211\n",
      "Epoch 1348/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37476.3086 - val_loss: 39885.4219\n",
      "Epoch 1349/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37474.0469 - val_loss: 39884.0781\n",
      "Epoch 1350/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37471.9531 - val_loss: 39883.8398\n",
      "Epoch 1351/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37469.6797 - val_loss: 39882.6602\n",
      "Epoch 1352/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37467.5391 - val_loss: 39880.4180\n",
      "Epoch 1353/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37465.5039 - val_loss: 39881.2617\n",
      "Epoch 1354/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37463.5000 - val_loss: 39877.7930\n",
      "Epoch 1355/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37461.4961 - val_loss: 39878.1289\n",
      "Epoch 1356/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37459.4961 - val_loss: 39878.0430\n",
      "Epoch 1357/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37457.4961 - val_loss: 39875.7539\n",
      "Epoch 1358/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37455.4961 - val_loss: 39874.7461\n",
      "Epoch 1359/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37453.4844 - val_loss: 39875.0430\n",
      "Epoch 1360/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37451.4609 - val_loss: 39871.5469\n",
      "Epoch 1361/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37449.4609 - val_loss: 39871.3398\n",
      "Epoch 1362/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37447.4492 - val_loss: 39871.1367\n",
      "Epoch 1363/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37445.3516 - val_loss: 39870.0117\n",
      "Epoch 1364/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37443.3398 - val_loss: 39868.5781\n",
      "Epoch 1365/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37441.3164 - val_loss: 39867.7891\n",
      "Epoch 1366/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37439.1914 - val_loss: 39866.8906\n",
      "Epoch 1367/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37437.1836 - val_loss: 39865.2891\n",
      "Epoch 1368/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37435.1445 - val_loss: 39864.8711\n",
      "Epoch 1369/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37433.0391 - val_loss: 39863.1211\n",
      "Epoch 1370/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37431.0039 - val_loss: 39861.8320\n",
      "Epoch 1371/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37428.9844 - val_loss: 39861.6367\n",
      "Epoch 1372/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37426.9570 - val_loss: 39859.6133\n",
      "Epoch 1373/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37424.8438 - val_loss: 39859.6680\n",
      "Epoch 1374/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37422.8164 - val_loss: 39858.1289\n",
      "Epoch 1375/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37420.8125 - val_loss: 39856.4844\n",
      "Epoch 1376/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37418.6953 - val_loss: 39856.1211\n",
      "Epoch 1377/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37416.6836 - val_loss: 39855.7461\n",
      "Epoch 1378/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37414.6484 - val_loss: 39854.8789\n",
      "Epoch 1379/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37412.5508 - val_loss: 39852.5352\n",
      "Epoch 1380/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37410.5469 - val_loss: 39853.1406\n",
      "Epoch 1381/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37408.5156 - val_loss: 39851.3711\n",
      "Epoch 1382/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37406.5039 - val_loss: 39849.8281\n",
      "Epoch 1383/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37404.4844 - val_loss: 39851.1289\n",
      "Epoch 1384/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37402.4531 - val_loss: 39849.5430\n",
      "Epoch 1385/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37400.3438 - val_loss: 39847.0469\n",
      "Epoch 1386/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37398.1914 - val_loss: 39848.6289\n",
      "Epoch 1387/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37396.1562 - val_loss: 39847.2656\n",
      "Epoch 1388/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37394.0430 - val_loss: 39844.5430\n",
      "Epoch 1389/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37392.0000 - val_loss: 39846.5859\n",
      "Epoch 1390/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37389.9570 - val_loss: 39844.1602\n",
      "Epoch 1391/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37387.8438 - val_loss: 39841.6367\n",
      "Epoch 1392/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37385.6914 - val_loss: 39844.2344\n",
      "Epoch 1393/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37383.5508 - val_loss: 39840.8789\n",
      "Epoch 1394/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37381.4961 - val_loss: 39838.8633\n",
      "Epoch 1395/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37379.4492 - val_loss: 39841.2969\n",
      "Epoch 1396/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37377.1914 - val_loss: 39838.2109\n",
      "Epoch 1397/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37375.0430 - val_loss: 39835.4609\n",
      "Epoch 1398/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37372.9844 - val_loss: 39838.6289\n",
      "Epoch 1399/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37370.8086 - val_loss: 39835.2070\n",
      "Epoch 1400/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37368.5391 - val_loss: 39834.2344\n",
      "Epoch 1401/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37366.4492 - val_loss: 39835.6602\n",
      "Epoch 1402/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37364.1562 - val_loss: 39831.8789\n",
      "Epoch 1403/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37361.9883 - val_loss: 39832.1289\n",
      "Epoch 1404/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37359.8086 - val_loss: 39833.2930\n",
      "Epoch 1405/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37357.5117 - val_loss: 39829.3789\n",
      "Epoch 1406/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37355.3164 - val_loss: 39830.0117\n",
      "Epoch 1407/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37353.0039 - val_loss: 39828.1367\n",
      "Epoch 1408/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37350.6914 - val_loss: 39827.0430\n",
      "Epoch 1409/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37348.3555 - val_loss: 39827.5898\n",
      "Epoch 1410/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37346.0156 - val_loss: 39825.6094\n",
      "Epoch 1411/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37343.6836 - val_loss: 39825.2461\n",
      "Epoch 1412/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37341.1914 - val_loss: 39825.0820\n",
      "Epoch 1413/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37338.8516 - val_loss: 39821.4570\n",
      "Epoch 1414/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37336.4492 - val_loss: 39822.6211\n",
      "Epoch 1415/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37333.8555 - val_loss: 39820.2461\n",
      "Epoch 1416/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37331.3516 - val_loss: 39818.7500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1417/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37328.9609 - val_loss: 39819.6289\n",
      "Epoch 1418/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37326.5430 - val_loss: 39816.2617\n",
      "Epoch 1419/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37324.1836 - val_loss: 39816.5820\n",
      "Epoch 1420/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37321.9609 - val_loss: 39815.7930\n",
      "Epoch 1421/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37319.6836 - val_loss: 39813.6250\n",
      "Epoch 1422/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37317.4844 - val_loss: 39814.4570\n",
      "Epoch 1423/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37315.1562 - val_loss: 39812.9570\n",
      "Epoch 1424/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37312.9844 - val_loss: 39812.3359\n",
      "Epoch 1425/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37310.8086 - val_loss: 39812.4219\n",
      "Epoch 1426/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37308.5430 - val_loss: 39809.7930\n",
      "Epoch 1427/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37306.4492 - val_loss: 39810.0469\n",
      "Epoch 1428/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37304.1445 - val_loss: 39808.8281\n",
      "Epoch 1429/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37301.8438 - val_loss: 39806.7070\n",
      "Epoch 1430/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37299.5039 - val_loss: 39808.7070\n",
      "Epoch 1431/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37297.0469 - val_loss: 39804.7461\n",
      "Epoch 1432/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37294.6562 - val_loss: 39804.1719\n",
      "Epoch 1433/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37292.3164 - val_loss: 39803.7383\n",
      "Epoch 1434/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37289.9570 - val_loss: 39800.5430\n",
      "Epoch 1435/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37287.5039 - val_loss: 39800.7148\n",
      "Epoch 1436/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37285.0469 - val_loss: 39800.3789\n",
      "Epoch 1437/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37282.8086 - val_loss: 39796.5781\n",
      "Epoch 1438/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37280.5039 - val_loss: 39798.4570\n",
      "Epoch 1439/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37278.1836 - val_loss: 39796.0430\n",
      "Epoch 1440/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37275.9883 - val_loss: 39794.7930\n",
      "Epoch 1441/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37273.6953 - val_loss: 39794.6602\n",
      "Epoch 1442/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37271.6445 - val_loss: 39792.2461\n",
      "Epoch 1443/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37269.5156 - val_loss: 39792.2891\n",
      "Epoch 1444/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37267.4570 - val_loss: 39791.9570\n",
      "Epoch 1445/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37265.1914 - val_loss: 39789.9648\n",
      "Epoch 1446/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37263.0039 - val_loss: 39790.7539\n",
      "Epoch 1447/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37260.8086 - val_loss: 39789.3867\n",
      "Epoch 1448/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37258.5156 - val_loss: 39788.5000\n",
      "Epoch 1449/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37256.3164 - val_loss: 39789.4180\n",
      "Epoch 1450/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37254.0039 - val_loss: 39786.7461\n",
      "Epoch 1451/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37251.6836 - val_loss: 39785.2383\n",
      "Epoch 1452/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37249.4609 - val_loss: 39785.3789\n",
      "Epoch 1453/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37247.1562 - val_loss: 39783.3633\n",
      "Epoch 1454/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37244.9570 - val_loss: 39783.7383\n",
      "Epoch 1455/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37242.6445 - val_loss: 39783.4609\n",
      "Epoch 1456/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37240.4492 - val_loss: 39782.4219\n",
      "Epoch 1457/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37238.1562 - val_loss: 39782.1094\n",
      "Epoch 1458/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37235.9570 - val_loss: 39779.2461\n",
      "Epoch 1459/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37233.6562 - val_loss: 39779.8633\n",
      "Epoch 1460/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37231.4570 - val_loss: 39778.1211\n",
      "Epoch 1461/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37229.1484 - val_loss: 39777.3359\n",
      "Epoch 1462/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37226.9531 - val_loss: 39777.0117\n",
      "Epoch 1463/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37224.5508 - val_loss: 39775.4648\n",
      "Epoch 1464/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37222.3398 - val_loss: 39773.3789\n",
      "Epoch 1465/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37220.0039 - val_loss: 39773.6719\n",
      "Epoch 1466/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37217.6914 - val_loss: 39771.6250\n",
      "Epoch 1467/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37215.4844 - val_loss: 39770.5039\n",
      "Epoch 1468/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37213.1445 - val_loss: 39771.2383\n",
      "Epoch 1469/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37210.8516 - val_loss: 39767.2461\n",
      "Epoch 1470/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37208.5156 - val_loss: 39768.9570\n",
      "Epoch 1471/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37206.3047 - val_loss: 39765.4570\n",
      "Epoch 1472/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37203.9844 - val_loss: 39765.0430\n",
      "Epoch 1473/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37201.5508 - val_loss: 39764.6289\n",
      "Epoch 1474/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37199.3164 - val_loss: 39761.2109\n",
      "Epoch 1475/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37196.9883 - val_loss: 39762.6133\n",
      "Epoch 1476/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37194.5469 - val_loss: 39759.7930\n",
      "Epoch 1477/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37192.3047 - val_loss: 39760.8320\n",
      "Epoch 1478/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37189.9570 - val_loss: 39757.8711\n",
      "Epoch 1479/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37187.5156 - val_loss: 39758.0781\n",
      "Epoch 1480/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37185.1914 - val_loss: 39758.1211\n",
      "Epoch 1481/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37182.9609 - val_loss: 39753.9102\n",
      "Epoch 1482/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 37180.5508 - val_loss: 39755.6289\n",
      "Epoch 1483/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37178.3086 - val_loss: 39753.8750\n",
      "Epoch 1484/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37175.9609 - val_loss: 39752.1602\n",
      "Epoch 1485/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37173.5156 - val_loss: 39751.4883\n",
      "Epoch 1486/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37171.1836 - val_loss: 39751.1406\n",
      "Epoch 1487/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 23us/step - loss: 37168.8555 - val_loss: 39748.3398\n",
      "Epoch 1488/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37166.5039 - val_loss: 39748.4648\n",
      "Epoch 1489/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37164.1484 - val_loss: 39747.5391\n",
      "Epoch 1490/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37161.8125 - val_loss: 39745.6289\n",
      "Epoch 1491/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37159.4844 - val_loss: 39746.8359\n",
      "Epoch 1492/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37157.0391 - val_loss: 39743.1289\n",
      "Epoch 1493/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37154.6562 - val_loss: 39743.6133\n",
      "Epoch 1494/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37152.3398 - val_loss: 39741.7148\n",
      "Epoch 1495/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37149.9844 - val_loss: 39739.8789\n",
      "Epoch 1496/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37147.5156 - val_loss: 39739.9570\n",
      "Epoch 1497/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37145.1562 - val_loss: 39740.2539\n",
      "Epoch 1498/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37142.8086 - val_loss: 39737.3711\n",
      "Epoch 1499/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37140.4492 - val_loss: 39737.8789\n",
      "Epoch 1500/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37138.0039 - val_loss: 39735.2461\n",
      "Epoch 1501/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37135.5469 - val_loss: 39735.1719\n",
      "Epoch 1502/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37133.1836 - val_loss: 39734.5039\n",
      "Epoch 1503/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37130.8438 - val_loss: 39733.9961\n",
      "Epoch 1504/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37128.4531 - val_loss: 39731.7539\n",
      "Epoch 1505/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37125.9961 - val_loss: 39729.8867\n",
      "Epoch 1506/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37123.5391 - val_loss: 39729.7891\n",
      "Epoch 1507/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37121.1484 - val_loss: 39727.2070\n",
      "Epoch 1508/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37118.6914 - val_loss: 39727.1367\n",
      "Epoch 1509/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37116.3047 - val_loss: 39727.3633\n",
      "Epoch 1510/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37113.8477 - val_loss: 39724.1367\n",
      "Epoch 1511/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37111.4531 - val_loss: 39726.6367\n",
      "Epoch 1512/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37108.9844 - val_loss: 39722.2383\n",
      "Epoch 1513/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37106.5039 - val_loss: 39723.1133\n",
      "Epoch 1514/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37104.0156 - val_loss: 39721.5820\n",
      "Epoch 1515/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37101.5391 - val_loss: 39719.4648\n",
      "Epoch 1516/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37099.0469 - val_loss: 39718.4609\n",
      "Epoch 1517/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37096.6562 - val_loss: 39720.6211\n",
      "Epoch 1518/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37094.1836 - val_loss: 39715.4961\n",
      "Epoch 1519/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37091.6836 - val_loss: 39716.3867\n",
      "Epoch 1520/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37089.1914 - val_loss: 39713.4219\n",
      "Epoch 1521/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37086.8008 - val_loss: 39714.7461\n",
      "Epoch 1522/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37084.1992 - val_loss: 39712.0117\n",
      "Epoch 1523/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37081.6953 - val_loss: 39711.8633\n",
      "Epoch 1524/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37079.1914 - val_loss: 39711.3789\n",
      "Epoch 1525/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37076.6875 - val_loss: 39710.3633\n",
      "Epoch 1526/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37074.1562 - val_loss: 39706.3320\n",
      "Epoch 1527/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37071.6484 - val_loss: 39709.2461\n",
      "Epoch 1528/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37069.0391 - val_loss: 39703.7383\n",
      "Epoch 1529/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37066.4883 - val_loss: 39705.6680\n",
      "Epoch 1530/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37063.8438 - val_loss: 39703.2617\n",
      "Epoch 1531/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37061.1836 - val_loss: 39702.8633\n",
      "Epoch 1532/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37058.5391 - val_loss: 39702.6289\n",
      "Epoch 1533/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37055.9883 - val_loss: 39698.6367\n",
      "Epoch 1534/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37053.3438 - val_loss: 39700.2969\n",
      "Epoch 1535/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37050.6953 - val_loss: 39697.5430\n",
      "Epoch 1536/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37048.0469 - val_loss: 39696.9531\n",
      "Epoch 1537/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37045.4844 - val_loss: 39696.3320\n",
      "Epoch 1538/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37042.6953 - val_loss: 39694.7109\n",
      "Epoch 1539/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37040.0039 - val_loss: 39693.1133\n",
      "Epoch 1540/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37037.1914 - val_loss: 39693.8320\n",
      "Epoch 1541/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37034.5039 - val_loss: 39689.2070\n",
      "Epoch 1542/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37031.6953 - val_loss: 39693.1641\n",
      "Epoch 1543/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37028.9570 - val_loss: 39685.7070\n",
      "Epoch 1544/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37026.1875 - val_loss: 39692.1367\n",
      "Epoch 1545/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37023.5039 - val_loss: 39683.8906\n",
      "Epoch 1546/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37020.8516 - val_loss: 39688.3906\n",
      "Epoch 1547/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37018.3047 - val_loss: 39685.0430\n",
      "Epoch 1548/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 37015.5508 - val_loss: 39683.1367\n",
      "Epoch 1549/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37012.9570 - val_loss: 39686.2852\n",
      "Epoch 1550/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37010.1836 - val_loss: 39679.8867\n",
      "Epoch 1551/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 37007.4844 - val_loss: 39683.9961\n",
      "Epoch 1552/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37004.6836 - val_loss: 39679.0352\n",
      "Epoch 1553/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 37001.9531 - val_loss: 39680.6680\n",
      "Epoch 1554/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36999.0469 - val_loss: 39676.9219\n",
      "Epoch 1555/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36996.3477 - val_loss: 39678.0000\n",
      "Epoch 1556/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36993.5156 - val_loss: 39675.9570\n",
      "Epoch 1557/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36990.8125 - val_loss: 39673.5039\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1558/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36988.0469 - val_loss: 39675.5039\n",
      "Epoch 1559/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36985.4961 - val_loss: 39670.2617\n",
      "Epoch 1560/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36982.8125 - val_loss: 39673.1289\n",
      "Epoch 1561/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36980.1523 - val_loss: 39668.0430\n",
      "Epoch 1562/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36977.4961 - val_loss: 39668.5039\n",
      "Epoch 1563/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36974.8164 - val_loss: 39665.8281\n",
      "Epoch 1564/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36972.0430 - val_loss: 39665.7461\n",
      "Epoch 1565/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36969.3438 - val_loss: 39663.0430\n",
      "Epoch 1566/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36966.5430 - val_loss: 39663.5898\n",
      "Epoch 1567/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36963.9531 - val_loss: 39662.2500\n",
      "Epoch 1568/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36961.1836 - val_loss: 39663.1602\n",
      "Epoch 1569/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36958.5039 - val_loss: 39661.4648\n",
      "Epoch 1570/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36955.9531 - val_loss: 39660.3633\n",
      "Epoch 1571/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36953.1875 - val_loss: 39660.6680\n",
      "Epoch 1572/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36950.5039 - val_loss: 39658.4141\n",
      "Epoch 1573/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36947.8125 - val_loss: 39659.2539\n",
      "Epoch 1574/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36945.0117 - val_loss: 39656.6719\n",
      "Epoch 1575/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36942.3164 - val_loss: 39657.3320\n",
      "Epoch 1576/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36939.5156 - val_loss: 39655.9570\n",
      "Epoch 1577/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36936.8164 - val_loss: 39654.8789\n",
      "Epoch 1578/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36934.0430 - val_loss: 39654.4844\n",
      "Epoch 1579/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36931.3438 - val_loss: 39652.8789\n",
      "Epoch 1580/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36928.6484 - val_loss: 39650.8906\n",
      "Epoch 1581/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36925.9883 - val_loss: 39650.6211\n",
      "Epoch 1582/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36923.1914 - val_loss: 39648.2070\n",
      "Epoch 1583/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36920.5000 - val_loss: 39647.2852\n",
      "Epoch 1584/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36917.8086 - val_loss: 39646.6602\n",
      "Epoch 1585/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36914.9961 - val_loss: 39646.1406\n",
      "Epoch 1586/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36912.1992 - val_loss: 39644.9180\n",
      "Epoch 1587/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36909.4961 - val_loss: 39642.6641\n",
      "Epoch 1588/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36906.6953 - val_loss: 39642.7148\n",
      "Epoch 1589/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36904.0039 - val_loss: 39639.4648\n",
      "Epoch 1590/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36901.3047 - val_loss: 39643.2109\n",
      "Epoch 1591/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36898.5039 - val_loss: 39635.2461\n",
      "Epoch 1592/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36895.8164 - val_loss: 39642.2070\n",
      "Epoch 1593/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36893.0156 - val_loss: 39628.1094\n",
      "Epoch 1594/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36890.4492 - val_loss: 39645.9570\n",
      "Epoch 1595/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36887.8008 - val_loss: 39621.2148\n",
      "Epoch 1596/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36885.0391 - val_loss: 39644.3789\n",
      "Epoch 1597/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36882.3125 - val_loss: 39619.1602\n",
      "Epoch 1598/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36879.4844 - val_loss: 39637.0469\n",
      "Epoch 1599/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36876.5391 - val_loss: 39621.4883\n",
      "Epoch 1600/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36873.5508 - val_loss: 39627.2070\n",
      "Epoch 1601/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36870.8086 - val_loss: 39626.8789\n",
      "Epoch 1602/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36868.0039 - val_loss: 39619.9648\n",
      "Epoch 1603/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36865.1914 - val_loss: 39627.7617\n",
      "Epoch 1604/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36862.4844 - val_loss: 39613.8633\n",
      "Epoch 1605/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36859.6484 - val_loss: 39625.2461\n",
      "Epoch 1606/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36856.9609 - val_loss: 39608.2070\n",
      "Epoch 1607/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36854.1523 - val_loss: 39625.2617\n",
      "Epoch 1608/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36851.3125 - val_loss: 39607.0781\n",
      "Epoch 1609/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36848.4961 - val_loss: 39624.4570\n",
      "Epoch 1610/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36845.6836 - val_loss: 39603.2070\n",
      "Epoch 1611/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36842.8516 - val_loss: 39620.9180\n",
      "Epoch 1612/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36840.0039 - val_loss: 39600.8633\n",
      "Epoch 1613/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36837.1523 - val_loss: 39614.7539\n",
      "Epoch 1614/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36834.3047 - val_loss: 39602.5820\n",
      "Epoch 1615/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36831.3164 - val_loss: 39608.3281\n",
      "Epoch 1616/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36828.4609 - val_loss: 39606.7148\n",
      "Epoch 1617/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36825.5508 - val_loss: 39601.0469\n",
      "Epoch 1618/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36822.6953 - val_loss: 39605.7070\n",
      "Epoch 1619/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36819.9570 - val_loss: 39593.7617\n",
      "Epoch 1620/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36817.1562 - val_loss: 39606.2539\n",
      "Epoch 1621/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36814.3398 - val_loss: 39589.3633\n",
      "Epoch 1622/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36811.4961 - val_loss: 39607.9883\n",
      "Epoch 1623/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36808.6953 - val_loss: 39584.4570\n",
      "Epoch 1624/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36805.8516 - val_loss: 39605.3867\n",
      "Epoch 1625/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36803.0039 - val_loss: 39580.8398\n",
      "Epoch 1626/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36800.1875 - val_loss: 39603.5898\n",
      "Epoch 1627/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36797.1875 - val_loss: 39581.8711\n",
      "Epoch 1628/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 36794.3125 - val_loss: 39597.5039\n",
      "Epoch 1629/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36791.3398 - val_loss: 39583.2148\n",
      "Epoch 1630/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36788.3398 - val_loss: 39589.2891\n",
      "Epoch 1631/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36785.4844 - val_loss: 39583.2070\n",
      "Epoch 1632/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36782.4961 - val_loss: 39584.2383\n",
      "Epoch 1633/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36779.5117 - val_loss: 39585.1406\n",
      "Epoch 1634/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36776.6602 - val_loss: 39580.7148\n",
      "Epoch 1635/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36773.6953 - val_loss: 39588.0430\n",
      "Epoch 1636/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36770.8438 - val_loss: 39571.3359\n",
      "Epoch 1637/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36768.0469 - val_loss: 39592.9648\n",
      "Epoch 1638/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36765.4492 - val_loss: 39558.9570\n",
      "Epoch 1639/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36762.9570 - val_loss: 39601.9180\n",
      "Epoch 1640/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36760.5430 - val_loss: 39545.8789\n",
      "Epoch 1641/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36758.3398 - val_loss: 39605.7852\n",
      "Epoch 1642/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36755.5391 - val_loss: 39543.2070\n",
      "Epoch 1643/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36752.4961 - val_loss: 39596.7070\n",
      "Epoch 1644/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36748.6602 - val_loss: 39558.0039\n",
      "Epoch 1645/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36744.6953 - val_loss: 39571.9883\n",
      "Epoch 1646/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36741.3125 - val_loss: 39579.1211\n",
      "Epoch 1647/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36738.4961 - val_loss: 39549.8750\n",
      "Epoch 1648/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36736.0469 - val_loss: 39591.6133\n",
      "Epoch 1649/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36733.5117 - val_loss: 39542.8398\n",
      "Epoch 1650/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36730.4883 - val_loss: 39584.3711\n",
      "Epoch 1651/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36726.9883 - val_loss: 39551.4961\n",
      "Epoch 1652/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36723.4609 - val_loss: 39565.6289\n",
      "Epoch 1653/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36720.0469 - val_loss: 39568.9961\n",
      "Epoch 1654/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36717.3047 - val_loss: 39547.0039\n",
      "Epoch 1655/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36714.6523 - val_loss: 39580.2969\n",
      "Epoch 1656/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36712.0391 - val_loss: 39538.1289\n",
      "Epoch 1657/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36709.1875 - val_loss: 39577.0391\n",
      "Epoch 1658/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36705.9883 - val_loss: 39543.4219\n",
      "Epoch 1659/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36702.4844 - val_loss: 39559.5391\n",
      "Epoch 1660/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36699.0039 - val_loss: 39557.6250\n",
      "Epoch 1661/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36695.9961 - val_loss: 39542.2656\n",
      "Epoch 1662/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36693.1562 - val_loss: 39568.6133\n",
      "Epoch 1663/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36690.5391 - val_loss: 39528.6602\n",
      "Epoch 1664/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36687.9609 - val_loss: 39571.8906\n",
      "Epoch 1665/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36684.9961 - val_loss: 39528.9648\n",
      "Epoch 1666/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36681.5391 - val_loss: 39557.7031\n",
      "Epoch 1667/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36677.9844 - val_loss: 39542.8711\n",
      "Epoch 1668/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36674.5391 - val_loss: 39537.9961\n",
      "Epoch 1669/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36671.5156 - val_loss: 39554.0820\n",
      "Epoch 1670/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36668.8047 - val_loss: 39524.2461\n",
      "Epoch 1671/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36665.9961 - val_loss: 39555.7930\n",
      "Epoch 1672/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36663.0039 - val_loss: 39520.5430\n",
      "Epoch 1673/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36659.9531 - val_loss: 39549.7070\n",
      "Epoch 1674/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36656.5039 - val_loss: 39527.5430\n",
      "Epoch 1675/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36653.1562 - val_loss: 39538.2617\n",
      "Epoch 1676/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36649.9570 - val_loss: 39537.1367\n",
      "Epoch 1677/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36646.8125 - val_loss: 39525.2383\n",
      "Epoch 1678/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36643.8125 - val_loss: 39541.1250\n",
      "Epoch 1679/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36640.8516 - val_loss: 39515.0469\n",
      "Epoch 1680/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36637.9570 - val_loss: 39542.2344\n",
      "Epoch 1681/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36634.8516 - val_loss: 39510.3711\n",
      "Epoch 1682/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36631.8438 - val_loss: 39540.8281\n",
      "Epoch 1683/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36628.6602 - val_loss: 39510.2461\n",
      "Epoch 1684/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36625.4844 - val_loss: 39534.5781\n",
      "Epoch 1685/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36622.1484 - val_loss: 39512.1367\n",
      "Epoch 1686/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36618.8438 - val_loss: 39524.3711\n",
      "Epoch 1687/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36615.5039 - val_loss: 39519.0391\n",
      "Epoch 1688/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36612.3125 - val_loss: 39512.6094\n",
      "Epoch 1689/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36609.1914 - val_loss: 39526.9570\n",
      "Epoch 1690/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36606.3008 - val_loss: 39500.1250\n",
      "Epoch 1691/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36603.3477 - val_loss: 39533.3359\n",
      "Epoch 1692/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36600.5508 - val_loss: 39489.7461\n",
      "Epoch 1693/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36597.8125 - val_loss: 39536.7656\n",
      "Epoch 1694/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36594.8047 - val_loss: 39486.1211\n",
      "Epoch 1695/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36591.5156 - val_loss: 39531.2383\n",
      "Epoch 1696/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36587.9531 - val_loss: 39494.7617\n",
      "Epoch 1697/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36584.1523 - val_loss: 39514.7461\n",
      "Epoch 1698/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36580.5039 - val_loss: 39507.1680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1699/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36577.1523 - val_loss: 39497.5039\n",
      "Epoch 1700/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36574.0117 - val_loss: 39514.8789\n",
      "Epoch 1701/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36571.0156 - val_loss: 39486.5117\n",
      "Epoch 1702/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36568.0508 - val_loss: 39520.0117\n",
      "Epoch 1703/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36565.0156 - val_loss: 39481.4180\n",
      "Epoch 1704/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36561.9531 - val_loss: 39518.5430\n",
      "Epoch 1705/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36558.5508 - val_loss: 39480.1367\n",
      "Epoch 1706/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36555.1992 - val_loss: 39512.5430\n",
      "Epoch 1707/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36551.8125 - val_loss: 39483.1211\n",
      "Epoch 1708/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36548.1953 - val_loss: 39500.6289\n",
      "Epoch 1709/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36544.6992 - val_loss: 39489.2539\n",
      "Epoch 1710/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36541.3516 - val_loss: 39490.2344\n",
      "Epoch 1711/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36538.0156 - val_loss: 39494.6289\n",
      "Epoch 1712/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36534.8125 - val_loss: 39481.9570\n",
      "Epoch 1713/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36531.5508 - val_loss: 39500.2461\n",
      "Epoch 1714/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36528.4883 - val_loss: 39473.7070\n",
      "Epoch 1715/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36525.4844 - val_loss: 39505.2539\n",
      "Epoch 1716/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36522.4453 - val_loss: 39464.0039\n",
      "Epoch 1717/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36519.4492 - val_loss: 39508.2930\n",
      "Epoch 1718/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36516.4570 - val_loss: 39454.9961\n",
      "Epoch 1719/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36513.5039 - val_loss: 39510.5039\n",
      "Epoch 1720/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36510.4570 - val_loss: 39450.4648\n",
      "Epoch 1721/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36507.1562 - val_loss: 39506.6211\n",
      "Epoch 1722/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36503.5156 - val_loss: 39454.2617\n",
      "Epoch 1723/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36499.8086 - val_loss: 39494.4102\n",
      "Epoch 1724/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36495.8516 - val_loss: 39463.6211\n",
      "Epoch 1725/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36492.0039 - val_loss: 39477.1133\n",
      "Epoch 1726/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36488.4570 - val_loss: 39473.2852\n",
      "Epoch 1727/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36485.0039 - val_loss: 39462.2617\n",
      "Epoch 1728/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 36481.8047 - val_loss: 39480.4102\n",
      "Epoch 1729/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36478.5508 - val_loss: 39451.2852\n",
      "Epoch 1730/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36475.5039 - val_loss: 39486.2539\n",
      "Epoch 1731/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36472.4961 - val_loss: 39440.6133\n",
      "Epoch 1732/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36469.5039 - val_loss: 39490.3867\n",
      "Epoch 1733/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36466.3477 - val_loss: 39435.0781\n",
      "Epoch 1734/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36463.0156 - val_loss: 39487.5039\n",
      "Epoch 1735/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36459.4844 - val_loss: 39436.1133\n",
      "Epoch 1736/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36455.6992 - val_loss: 39477.5039\n",
      "Epoch 1737/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36451.8477 - val_loss: 39444.9531\n",
      "Epoch 1738/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36447.9570 - val_loss: 39461.1211\n",
      "Epoch 1739/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36444.1523 - val_loss: 39456.9141\n",
      "Epoch 1740/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36440.6523 - val_loss: 39447.2461\n",
      "Epoch 1741/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36437.3398 - val_loss: 39464.6680\n",
      "Epoch 1742/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36434.1523 - val_loss: 39435.5781\n",
      "Epoch 1743/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36431.0039 - val_loss: 39470.0898\n",
      "Epoch 1744/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36427.8477 - val_loss: 39424.3906\n",
      "Epoch 1745/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36424.8125 - val_loss: 39473.4219\n",
      "Epoch 1746/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36421.8008 - val_loss: 39414.3906\n",
      "Epoch 1747/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36418.6992 - val_loss: 39474.1289\n",
      "Epoch 1748/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36415.4492 - val_loss: 39410.0039\n",
      "Epoch 1749/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36411.9961 - val_loss: 39468.3789\n",
      "Epoch 1750/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36408.0039 - val_loss: 39415.9180\n",
      "Epoch 1751/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36403.8398 - val_loss: 39452.3789\n",
      "Epoch 1752/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36399.6641 - val_loss: 39427.9844\n",
      "Epoch 1753/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36395.6992 - val_loss: 39433.2383\n",
      "Epoch 1754/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36392.0391 - val_loss: 39440.6641\n",
      "Epoch 1755/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36388.6992 - val_loss: 39416.4844\n",
      "Epoch 1756/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36385.5508 - val_loss: 39451.5000\n",
      "Epoch 1757/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36382.6641 - val_loss: 39401.2344\n",
      "Epoch 1758/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36379.9531 - val_loss: 39459.2617\n",
      "Epoch 1759/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36376.9844 - val_loss: 39391.4570\n",
      "Epoch 1760/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36373.9531 - val_loss: 39458.2383\n",
      "Epoch 1761/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36370.1562 - val_loss: 39392.3594\n",
      "Epoch 1762/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36366.0508 - val_loss: 39444.0352\n",
      "Epoch 1763/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36361.5430 - val_loss: 39406.1641\n",
      "Epoch 1764/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36357.1523 - val_loss: 39420.3867\n",
      "Epoch 1765/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36353.1523 - val_loss: 39424.2070\n",
      "Epoch 1766/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36349.6641 - val_loss: 39399.3750\n",
      "Epoch 1767/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36346.5391 - val_loss: 39437.1602\n",
      "Epoch 1768/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36343.6523 - val_loss: 39384.0156\n",
      "Epoch 1769/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 36340.6992 - val_loss: 39442.6289\n",
      "Epoch 1770/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36337.4961 - val_loss: 39377.2852\n",
      "Epoch 1771/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36333.9844 - val_loss: 39435.9609\n",
      "Epoch 1772/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36329.8047 - val_loss: 39383.9648\n",
      "Epoch 1773/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36325.3086 - val_loss: 39416.0859\n",
      "Epoch 1774/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36320.8477 - val_loss: 39399.5859\n",
      "Epoch 1775/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36316.8398 - val_loss: 39393.4141\n",
      "Epoch 1776/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36313.1953 - val_loss: 39413.5898\n",
      "Epoch 1777/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36309.9609 - val_loss: 39377.5039\n",
      "Epoch 1778/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36306.6992 - val_loss: 39420.7383\n",
      "Epoch 1779/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 36303.4570 - val_loss: 39368.2852\n",
      "Epoch 1780/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36300.0039 - val_loss: 39420.1289\n",
      "Epoch 1781/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36296.3008 - val_loss: 39366.6602\n",
      "Epoch 1782/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36292.3477 - val_loss: 39411.8633\n",
      "Epoch 1783/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36288.1875 - val_loss: 39371.0430\n",
      "Epoch 1784/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36283.9961 - val_loss: 39397.1602\n",
      "Epoch 1785/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36279.8125 - val_loss: 39378.7383\n",
      "Epoch 1786/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36275.8398 - val_loss: 39382.6211\n",
      "Epoch 1787/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36271.9961 - val_loss: 39385.3867\n",
      "Epoch 1788/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36268.1992 - val_loss: 39370.0117\n",
      "Epoch 1789/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36264.5508 - val_loss: 39391.6211\n",
      "Epoch 1790/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36261.1523 - val_loss: 39355.2656\n",
      "Epoch 1791/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36257.9570 - val_loss: 39400.8789\n",
      "Epoch 1792/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36254.8125 - val_loss: 39339.7617\n",
      "Epoch 1793/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36252.0469 - val_loss: 39413.3711\n",
      "Epoch 1794/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36249.5117 - val_loss: 39323.0430\n",
      "Epoch 1795/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36247.5117 - val_loss: 39423.2930\n",
      "Epoch 1796/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36244.5430 - val_loss: 39316.3398\n",
      "Epoch 1797/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36240.9531 - val_loss: 39410.8789\n",
      "Epoch 1798/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36235.0156 - val_loss: 39332.5469\n",
      "Epoch 1799/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36228.9570 - val_loss: 39374.6133\n",
      "Epoch 1800/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36223.1523 - val_loss: 39366.4219\n",
      "Epoch 1801/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36218.9961 - val_loss: 39338.8398\n",
      "Epoch 1802/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36216.1875 - val_loss: 39391.3711\n",
      "Epoch 1803/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36213.6992 - val_loss: 39322.0117\n",
      "Epoch 1804/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36210.6523 - val_loss: 39389.7891\n",
      "Epoch 1805/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36206.3477 - val_loss: 39326.7148\n",
      "Epoch 1806/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36201.4609 - val_loss: 39364.9219\n",
      "Epoch 1807/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36196.4609 - val_loss: 39345.6367\n",
      "Epoch 1808/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36192.0156 - val_loss: 39336.8789\n",
      "Epoch 1809/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36188.4492 - val_loss: 39363.7930\n",
      "Epoch 1810/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36185.1875 - val_loss: 39318.1250\n",
      "Epoch 1811/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36182.0469 - val_loss: 39372.3398\n",
      "Epoch 1812/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36178.6602 - val_loss: 39312.3750\n",
      "Epoch 1813/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36174.8398 - val_loss: 39366.6211\n",
      "Epoch 1814/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36170.4844 - val_loss: 39318.4961\n",
      "Epoch 1815/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36165.8438 - val_loss: 39347.8789\n",
      "Epoch 1816/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36161.1953 - val_loss: 39333.2461\n",
      "Epoch 1817/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36157.0039 - val_loss: 39326.7344\n",
      "Epoch 1818/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36153.1875 - val_loss: 39346.0898\n",
      "Epoch 1819/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36149.6523 - val_loss: 39310.2148\n",
      "Epoch 1820/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36146.1875 - val_loss: 39353.8711\n",
      "Epoch 1821/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36142.6992 - val_loss: 39299.0469\n",
      "Epoch 1822/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36139.1602 - val_loss: 39354.0781\n",
      "Epoch 1823/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36135.3086 - val_loss: 39296.2539\n",
      "Epoch 1824/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36131.0156 - val_loss: 39344.7539\n",
      "Epoch 1825/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36126.5547 - val_loss: 39300.5352\n",
      "Epoch 1826/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36122.0469 - val_loss: 39329.6133\n",
      "Epoch 1827/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36117.5547 - val_loss: 39309.0117\n",
      "Epoch 1828/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36113.3125 - val_loss: 39314.0117\n",
      "Epoch 1829/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36109.1602 - val_loss: 39315.7617\n",
      "Epoch 1830/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36105.1602 - val_loss: 39302.1719\n",
      "Epoch 1831/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36101.3008 - val_loss: 39319.6289\n",
      "Epoch 1832/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36097.4492 - val_loss: 39291.3633\n",
      "Epoch 1833/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36093.5508 - val_loss: 39323.7539\n",
      "Epoch 1834/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36089.8438 - val_loss: 39281.0039\n",
      "Epoch 1835/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36086.1562 - val_loss: 39328.8867\n",
      "Epoch 1836/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36082.5430 - val_loss: 39268.3750\n",
      "Epoch 1837/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36079.1953 - val_loss: 39335.5352\n",
      "Epoch 1838/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36075.8477 - val_loss: 39256.0430\n",
      "Epoch 1839/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36072.6914 - val_loss: 39340.9961\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1840/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36069.1602 - val_loss: 39247.6289\n",
      "Epoch 1841/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36065.6641 - val_loss: 39338.5039\n",
      "Epoch 1842/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36060.9961 - val_loss: 39249.2148\n",
      "Epoch 1843/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36056.0469 - val_loss: 39321.2383\n",
      "Epoch 1844/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36050.3359 - val_loss: 39264.5117\n",
      "Epoch 1845/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36044.5039 - val_loss: 39288.5781\n",
      "Epoch 1846/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36039.4453 - val_loss: 39287.2383\n",
      "Epoch 1847/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36035.1875 - val_loss: 39259.2539\n",
      "Epoch 1848/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36031.8125 - val_loss: 39306.9570\n",
      "Epoch 1849/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 36028.8438 - val_loss: 39237.4648\n",
      "Epoch 1850/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 36026.0391 - val_loss: 39317.2383\n",
      "Epoch 1851/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36022.6523 - val_loss: 39227.9570\n",
      "Epoch 1852/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 36018.8008 - val_loss: 39311.3398\n",
      "Epoch 1853/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36013.8125 - val_loss: 39233.2109\n",
      "Epoch 1854/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 36008.1953 - val_loss: 39285.7617\n",
      "Epoch 1855/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 36002.3477 - val_loss: 39253.7344\n",
      "Epoch 1856/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35997.0391 - val_loss: 39253.6367\n",
      "Epoch 1857/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35992.6992 - val_loss: 39274.7891\n",
      "Epoch 1858/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35989.0508 - val_loss: 39230.6719\n",
      "Epoch 1859/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35985.8008 - val_loss: 39286.2344\n",
      "Epoch 1860/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35982.1875 - val_loss: 39220.3633\n",
      "Epoch 1861/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35978.1992 - val_loss: 39284.1094\n",
      "Epoch 1862/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35973.8125 - val_loss: 39221.1133\n",
      "Epoch 1863/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35969.0039 - val_loss: 39271.1133\n",
      "Epoch 1864/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35963.9961 - val_loss: 39231.7617\n",
      "Epoch 1865/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35958.9883 - val_loss: 39251.5039\n",
      "Epoch 1866/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35954.1875 - val_loss: 39244.8594\n",
      "Epoch 1867/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35949.8047 - val_loss: 39234.1641\n",
      "Epoch 1868/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35945.6523 - val_loss: 39255.7344\n",
      "Epoch 1869/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35941.6992 - val_loss: 39217.3789\n",
      "Epoch 1870/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35938.0039 - val_loss: 39266.6211\n",
      "Epoch 1871/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35934.4961 - val_loss: 39202.3633\n",
      "Epoch 1872/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35931.0117 - val_loss: 39274.0352\n",
      "Epoch 1873/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35927.4492 - val_loss: 39191.4219\n",
      "Epoch 1874/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35923.9531 - val_loss: 39277.1211\n",
      "Epoch 1875/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 35919.8047 - val_loss: 39186.3789\n",
      "Epoch 1876/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35915.5547 - val_loss: 39271.2070\n",
      "Epoch 1877/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35910.5000 - val_loss: 39190.2969\n",
      "Epoch 1878/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35905.1875 - val_loss: 39252.5781\n",
      "Epoch 1879/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35899.4453 - val_loss: 39205.2891\n",
      "Epoch 1880/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35893.8125 - val_loss: 39225.9961\n",
      "Epoch 1881/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35888.8047 - val_loss: 39221.7070\n",
      "Epoch 1882/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35884.3008 - val_loss: 39204.7383\n",
      "Epoch 1883/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35880.1562 - val_loss: 39234.1133\n",
      "Epoch 1884/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35876.3398 - val_loss: 39185.1367\n",
      "Epoch 1885/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35872.8438 - val_loss: 39247.4883\n",
      "Epoch 1886/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35869.5391 - val_loss: 39167.7539\n",
      "Epoch 1887/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35866.5156 - val_loss: 39258.0039\n",
      "Epoch 1888/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35863.0508 - val_loss: 39156.9531\n",
      "Epoch 1889/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35859.5547 - val_loss: 39258.4570\n",
      "Epoch 1890/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35854.9492 - val_loss: 39156.6602\n",
      "Epoch 1891/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35849.8047 - val_loss: 39242.3594\n",
      "Epoch 1892/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35843.4961 - val_loss: 39170.9648\n",
      "Epoch 1893/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35837.0469 - val_loss: 39210.3789\n",
      "Epoch 1894/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 35831.1562 - val_loss: 39194.2109\n",
      "Epoch 1895/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35826.0508 - val_loss: 39178.7539\n",
      "Epoch 1896/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35822.0039 - val_loss: 39216.2461\n",
      "Epoch 1897/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35818.6680 - val_loss: 39155.0430\n",
      "Epoch 1898/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35815.5391 - val_loss: 39231.1719\n",
      "Epoch 1899/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35812.3008 - val_loss: 39141.3750\n",
      "Epoch 1900/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35808.8398 - val_loss: 39234.0469\n",
      "Epoch 1901/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35804.4609 - val_loss: 39138.9180\n",
      "Epoch 1902/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35799.4844 - val_loss: 39219.1367\n",
      "Epoch 1903/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35793.3086 - val_loss: 39152.3789\n",
      "Epoch 1904/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35786.9961 - val_loss: 39187.2383\n",
      "Epoch 1905/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35781.0117 - val_loss: 39176.8281\n",
      "Epoch 1906/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35776.1523 - val_loss: 39155.5117\n",
      "Epoch 1907/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35772.1875 - val_loss: 39200.2539\n",
      "Epoch 1908/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35768.9844 - val_loss: 39132.9609\n",
      "Epoch 1909/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35765.8086 - val_loss: 39213.9961\n",
      "Epoch 1910/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 24us/step - loss: 35762.4453 - val_loss: 39119.0781\n",
      "Epoch 1911/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35759.1562 - val_loss: 39219.2109\n",
      "Epoch 1912/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35754.8125 - val_loss: 39113.7109\n",
      "Epoch 1913/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35750.3359 - val_loss: 39210.0039\n",
      "Epoch 1914/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35744.3047 - val_loss: 39123.9570\n",
      "Epoch 1915/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35737.5391 - val_loss: 39179.1133\n",
      "Epoch 1916/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35730.9492 - val_loss: 39148.7461\n",
      "Epoch 1917/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35725.0508 - val_loss: 39143.7539\n",
      "Epoch 1918/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35720.4961 - val_loss: 39172.6406\n",
      "Epoch 1919/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35716.6953 - val_loss: 39118.1133\n",
      "Epoch 1920/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35713.3047 - val_loss: 39187.3594\n",
      "Epoch 1921/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35709.8008 - val_loss: 39102.7539\n",
      "Epoch 1922/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35705.9531 - val_loss: 39188.5039\n",
      "Epoch 1923/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35701.3047 - val_loss: 39099.0820\n",
      "Epoch 1924/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35696.0430 - val_loss: 39175.2930\n",
      "Epoch 1925/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35690.1562 - val_loss: 39108.6211\n",
      "Epoch 1926/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35683.9961 - val_loss: 39149.7070\n",
      "Epoch 1927/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35677.9961 - val_loss: 39127.6289\n",
      "Epoch 1928/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35672.5547 - val_loss: 39123.4141\n",
      "Epoch 1929/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35667.9492 - val_loss: 39143.5859\n",
      "Epoch 1930/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35663.5391 - val_loss: 39102.2031\n",
      "Epoch 1931/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35659.5430 - val_loss: 39156.9648\n",
      "Epoch 1932/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35655.8359 - val_loss: 39083.1367\n",
      "Epoch 1933/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35652.3477 - val_loss: 39167.2148\n",
      "Epoch 1934/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35648.6562 - val_loss: 39068.3398\n",
      "Epoch 1935/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35645.3008 - val_loss: 39172.2461\n",
      "Epoch 1936/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35640.9492 - val_loss: 39060.3906\n",
      "Epoch 1937/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35636.8438 - val_loss: 39169.2461\n",
      "Epoch 1938/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35631.1992 - val_loss: 39062.4141\n",
      "Epoch 1939/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35625.5156 - val_loss: 39151.1211\n",
      "Epoch 1940/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35618.5117 - val_loss: 39080.4648\n",
      "Epoch 1941/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35611.3438 - val_loss: 39114.1719\n",
      "Epoch 1942/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35605.0156 - val_loss: 39107.0000\n",
      "Epoch 1943/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 35599.9883 - val_loss: 39081.1602\n",
      "Epoch 1944/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35595.8477 - val_loss: 39129.6133\n",
      "Epoch 1945/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35592.4453 - val_loss: 39057.5898\n",
      "Epoch 1946/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35589.0039 - val_loss: 39140.0430\n",
      "Epoch 1947/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35585.0117 - val_loss: 39047.1211\n",
      "Epoch 1948/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35580.3477 - val_loss: 39132.6211\n",
      "Epoch 1949/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35574.5547 - val_loss: 39052.3398\n",
      "Epoch 1950/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35568.3086 - val_loss: 39109.0898\n",
      "Epoch 1951/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35561.8477 - val_loss: 39068.5430\n",
      "Epoch 1952/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35555.8359 - val_loss: 39081.7930\n",
      "Epoch 1953/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 35550.4844 - val_loss: 39085.2539\n",
      "Epoch 1954/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35545.5391 - val_loss: 39060.1094\n",
      "Epoch 1955/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35541.0430 - val_loss: 39100.0430\n",
      "Epoch 1956/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35536.9883 - val_loss: 39040.7617\n",
      "Epoch 1957/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35533.0508 - val_loss: 39114.1680\n",
      "Epoch 1958/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35529.5039 - val_loss: 39022.9531\n",
      "Epoch 1959/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35526.2969 - val_loss: 39126.6602\n",
      "Epoch 1960/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35522.8438 - val_loss: 39007.5117\n",
      "Epoch 1961/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35519.9492 - val_loss: 39134.5352\n",
      "Epoch 1962/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35515.6680 - val_loss: 38999.7383\n",
      "Epoch 1963/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35511.4453 - val_loss: 39126.9844\n",
      "Epoch 1964/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35504.5508 - val_loss: 39007.8867\n",
      "Epoch 1965/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35497.2969 - val_loss: 39095.9648\n",
      "Epoch 1966/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35488.8359 - val_loss: 39035.7383\n",
      "Epoch 1967/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35481.1641 - val_loss: 39050.7969\n",
      "Epoch 1968/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35475.3047 - val_loss: 39070.8281\n",
      "Epoch 1969/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35471.0508 - val_loss: 39017.4180\n",
      "Epoch 1970/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35467.9492 - val_loss: 39093.6719\n",
      "Epoch 1971/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35464.5508 - val_loss: 39000.7383\n",
      "Epoch 1972/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35460.8086 - val_loss: 39097.8711\n",
      "Epoch 1973/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35455.9492 - val_loss: 38999.7461\n",
      "Epoch 1974/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35449.9492 - val_loss: 39080.2617\n",
      "Epoch 1975/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35443.0430 - val_loss: 39013.8398\n",
      "Epoch 1976/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35436.1914 - val_loss: 39049.4961\n",
      "Epoch 1977/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35429.8438 - val_loss: 39034.8867\n",
      "Epoch 1978/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35424.3438 - val_loss: 39020.9961\n",
      "Epoch 1979/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 35419.5430 - val_loss: 39052.5781\n",
      "Epoch 1980/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35415.3047 - val_loss: 38998.5469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1981/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35411.3047 - val_loss: 39066.4883\n",
      "Epoch 1982/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35407.4570 - val_loss: 38981.7148\n",
      "Epoch 1983/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35403.5508 - val_loss: 39075.3711\n",
      "Epoch 1984/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35399.4492 - val_loss: 38971.1094\n",
      "Epoch 1985/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35395.3086 - val_loss: 39077.7930\n",
      "Epoch 1986/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35390.1914 - val_loss: 38967.4883\n",
      "Epoch 1987/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35385.3320 - val_loss: 39072.4219\n",
      "Epoch 1988/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35379.0156 - val_loss: 38974.1367\n",
      "Epoch 1989/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35372.3438 - val_loss: 39050.2148\n",
      "Epoch 1990/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35365.1602 - val_loss: 38991.7383\n",
      "Epoch 1991/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35358.3047 - val_loss: 39019.3281\n",
      "Epoch 1992/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35352.0508 - val_loss: 39012.8789\n",
      "Epoch 1993/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35346.6953 - val_loss: 38991.2500\n",
      "Epoch 1994/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35341.9883 - val_loss: 39029.9844\n",
      "Epoch 1995/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35337.6562 - val_loss: 38969.1719\n",
      "Epoch 1996/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35333.7031 - val_loss: 39046.1406\n",
      "Epoch 1997/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35330.1992 - val_loss: 38948.3711\n",
      "Epoch 1998/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35327.3359 - val_loss: 39064.6680\n",
      "Epoch 1999/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35324.5430 - val_loss: 38927.7383\n",
      "Epoch 2000/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35323.4453 - val_loss: 39085.3633\n",
      "Epoch 2001/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35320.8438 - val_loss: 38913.4961\n",
      "Epoch 2002/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35318.6562 - val_loss: 39087.5039\n",
      "Epoch 2003/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35311.8359 - val_loss: 38919.4961\n",
      "Epoch 2004/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35303.3320 - val_loss: 39047.8633\n",
      "Epoch 2005/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35290.8438 - val_loss: 38961.8906\n",
      "Epoch 2006/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35279.6562 - val_loss: 38975.1680\n",
      "Epoch 2007/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35273.0039 - val_loss: 39020.9648\n",
      "Epoch 2008/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35270.8008 - val_loss: 38927.6680\n",
      "Epoch 2009/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35270.0469 - val_loss: 39050.8711\n",
      "Epoch 2010/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35267.4570 - val_loss: 38915.3711\n",
      "Epoch 2011/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35262.5156 - val_loss: 39035.0430\n",
      "Epoch 2012/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 35253.8398 - val_loss: 38937.7539\n",
      "Epoch 2013/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35244.1562 - val_loss: 38982.7461\n",
      "Epoch 2014/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35235.9570 - val_loss: 38982.2930\n",
      "Epoch 2015/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35230.6562 - val_loss: 38934.0430\n",
      "Epoch 2016/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35227.6680 - val_loss: 39017.0352\n",
      "Epoch 2017/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35225.1953 - val_loss: 38911.4570\n",
      "Epoch 2018/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35221.5117 - val_loss: 39018.4570\n",
      "Epoch 2019/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35215.5117 - val_loss: 38915.5039\n",
      "Epoch 2020/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35208.2969 - val_loss: 38989.3906\n",
      "Epoch 2021/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35200.0430 - val_loss: 38940.9570\n",
      "Epoch 2022/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35192.7969 - val_loss: 38949.4570\n",
      "Epoch 2023/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35186.8438 - val_loss: 38971.5117\n",
      "Epoch 2024/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35182.2969 - val_loss: 38919.5039\n",
      "Epoch 2025/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35178.3438 - val_loss: 38991.4219\n",
      "Epoch 2026/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35174.3320 - val_loss: 38904.4883\n",
      "Epoch 2027/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35169.9531 - val_loss: 38994.5117\n",
      "Epoch 2028/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35164.5156 - val_loss: 38902.1289\n",
      "Epoch 2029/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35158.5547 - val_loss: 38981.3750\n",
      "Epoch 2030/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35151.7031 - val_loss: 38913.4609\n",
      "Epoch 2031/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35144.8086 - val_loss: 38957.6133\n",
      "Epoch 2032/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35138.0039 - val_loss: 38932.3711\n",
      "Epoch 2033/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35131.7969 - val_loss: 38933.0430\n",
      "Epoch 2034/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35126.1641 - val_loss: 38949.3594\n",
      "Epoch 2035/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35120.9961 - val_loss: 38913.6680\n",
      "Epoch 2036/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35116.0117 - val_loss: 38961.1133\n",
      "Epoch 2037/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 35111.2969 - val_loss: 38896.6680\n",
      "Epoch 2038/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35106.8359 - val_loss: 38973.4883\n",
      "Epoch 2039/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35102.6562 - val_loss: 38878.3789\n",
      "Epoch 2040/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35099.0430 - val_loss: 38989.2461\n",
      "Epoch 2041/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35095.6992 - val_loss: 38858.4531\n",
      "Epoch 2042/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35093.5430 - val_loss: 39006.2852\n",
      "Epoch 2043/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35090.3320 - val_loss: 38843.8398\n",
      "Epoch 2044/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35087.7031 - val_loss: 39010.6641\n",
      "Epoch 2045/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35081.4492 - val_loss: 38845.1289\n",
      "Epoch 2046/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35074.2031 - val_loss: 38984.1680\n",
      "Epoch 2047/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35063.1641 - val_loss: 38871.4883\n",
      "Epoch 2048/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35052.4570 - val_loss: 38929.7461\n",
      "Epoch 2049/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35042.9883 - val_loss: 38916.2500\n",
      "Epoch 2050/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35036.6562 - val_loss: 38878.6211\n",
      "Epoch 2051/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 35032.9570 - val_loss: 38953.6641\n",
      "Epoch 2052/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35030.3438 - val_loss: 38848.2930\n",
      "Epoch 2053/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35027.9961 - val_loss: 38971.5352\n",
      "Epoch 2054/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35024.1953 - val_loss: 38836.8867\n",
      "Epoch 2055/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35019.1562 - val_loss: 38962.1211\n",
      "Epoch 2056/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 35011.5039 - val_loss: 38846.4570\n",
      "Epoch 2057/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 35002.9961 - val_loss: 38927.2930\n",
      "Epoch 2058/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34993.9961 - val_loss: 38874.4961\n",
      "Epoch 2059/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34986.0430 - val_loss: 38885.3398\n",
      "Epoch 2060/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34979.7930 - val_loss: 38908.2070\n",
      "Epoch 2061/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34974.9961 - val_loss: 38852.7383\n",
      "Epoch 2062/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34971.1641 - val_loss: 38931.9570\n",
      "Epoch 2063/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34967.4844 - val_loss: 38833.5391\n",
      "Epoch 2064/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34963.4570 - val_loss: 38939.1133\n",
      "Epoch 2065/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34958.1953 - val_loss: 38826.8633\n",
      "Epoch 2066/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34952.5547 - val_loss: 38930.1719\n",
      "Epoch 2067/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34945.5430 - val_loss: 38832.6211\n",
      "Epoch 2068/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34938.3047 - val_loss: 38906.9570\n",
      "Epoch 2069/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34930.5039 - val_loss: 38848.6211\n",
      "Epoch 2070/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34923.2031 - val_loss: 38878.2500\n",
      "Epoch 2071/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34916.4961 - val_loss: 38868.3789\n",
      "Epoch 2072/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34910.4570 - val_loss: 38852.4844\n",
      "Epoch 2073/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34904.9570 - val_loss: 38885.8789\n",
      "Epoch 2074/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34899.9570 - val_loss: 38829.7891\n",
      "Epoch 2075/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34895.3008 - val_loss: 38901.1211\n",
      "Epoch 2076/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34891.0039 - val_loss: 38807.0430\n",
      "Epoch 2077/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34887.6719 - val_loss: 38922.0820\n",
      "Epoch 2078/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34884.8438 - val_loss: 38781.7109\n",
      "Epoch 2079/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34884.2031 - val_loss: 38952.4648\n",
      "Epoch 2080/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34883.2031 - val_loss: 38759.4961\n",
      "Epoch 2081/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34884.0039 - val_loss: 38972.8594\n",
      "Epoch 2082/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34879.8320 - val_loss: 38755.1719\n",
      "Epoch 2083/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34873.4453 - val_loss: 38946.1406\n",
      "Epoch 2084/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34859.2031 - val_loss: 38784.3281\n",
      "Epoch 2085/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34844.0117 - val_loss: 38870.2539\n",
      "Epoch 2086/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34830.3438 - val_loss: 38847.6602\n",
      "Epoch 2087/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34822.8086 - val_loss: 38798.2070\n",
      "Epoch 2088/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34820.5000 - val_loss: 38899.9102\n",
      "Epoch 2089/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34819.5430 - val_loss: 38765.2617\n",
      "Epoch 2090/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34818.0039 - val_loss: 38911.9883\n",
      "Epoch 2091/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34812.0117 - val_loss: 38767.0117\n",
      "Epoch 2092/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34803.8047 - val_loss: 38880.2617\n",
      "Epoch 2093/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34792.8438 - val_loss: 38799.1211\n",
      "Epoch 2094/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34782.6562 - val_loss: 38824.9141\n",
      "Epoch 2095/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34774.9570 - val_loss: 38845.2344\n",
      "Epoch 2096/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34770.0156 - val_loss: 38782.3711\n",
      "Epoch 2097/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34766.6719 - val_loss: 38872.8867\n",
      "Epoch 2098/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34762.9531 - val_loss: 38764.4961\n",
      "Epoch 2099/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34758.3320 - val_loss: 38873.0898\n",
      "Epoch 2100/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34751.5547 - val_loss: 38768.2930\n",
      "Epoch 2101/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34744.0430 - val_loss: 38849.7383\n",
      "Epoch 2102/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34735.5508 - val_loss: 38788.8633\n",
      "Epoch 2103/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34727.4883 - val_loss: 38813.7461\n",
      "Epoch 2104/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34720.1914 - val_loss: 38816.2930\n",
      "Epoch 2105/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34714.0430 - val_loss: 38782.1367\n",
      "Epoch 2106/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34708.9531 - val_loss: 38838.2539\n",
      "Epoch 2107/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34704.1641 - val_loss: 38758.0781\n",
      "Epoch 2108/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34700.0039 - val_loss: 38855.9883\n",
      "Epoch 2109/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34695.5547 - val_loss: 38740.3711\n",
      "Epoch 2110/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34691.8438 - val_loss: 38868.9102\n",
      "Epoch 2111/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34687.0508 - val_loss: 38729.2656\n",
      "Epoch 2112/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34682.5117 - val_loss: 38870.3711\n",
      "Epoch 2113/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34675.9570 - val_loss: 38728.6367\n",
      "Epoch 2114/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34669.0156 - val_loss: 38853.8867\n",
      "Epoch 2115/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34660.1680 - val_loss: 38741.8711\n",
      "Epoch 2116/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34651.0430 - val_loss: 38819.6094\n",
      "Epoch 2117/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34642.0117 - val_loss: 38763.8281\n",
      "Epoch 2118/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34634.0156 - val_loss: 38785.0430\n",
      "Epoch 2119/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34627.0469 - val_loss: 38785.1133\n",
      "Epoch 2120/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34620.9883 - val_loss: 38758.2930\n",
      "Epoch 2121/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34615.4492 - val_loss: 38803.4531\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2122/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34610.3008 - val_loss: 38735.2461\n",
      "Epoch 2123/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34605.9844 - val_loss: 38826.2461\n",
      "Epoch 2124/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34602.1602 - val_loss: 38710.9883\n",
      "Epoch 2125/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34599.6602 - val_loss: 38854.3711\n",
      "Epoch 2126/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34597.5430 - val_loss: 38685.0781\n",
      "Epoch 2127/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34598.1953 - val_loss: 38888.4648\n",
      "Epoch 2128/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34597.9844 - val_loss: 38664.4219\n",
      "Epoch 2129/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34597.8398 - val_loss: 38898.3398\n",
      "Epoch 2130/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34590.8438 - val_loss: 38668.1719\n",
      "Epoch 2131/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34579.2070 - val_loss: 38845.5820\n",
      "Epoch 2132/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34560.9570 - val_loss: 38713.5781\n",
      "Epoch 2133/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34544.5508 - val_loss: 38756.8281\n",
      "Epoch 2134/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34533.9570 - val_loss: 38784.3789\n",
      "Epoch 2135/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34530.0039 - val_loss: 38692.2031\n",
      "Epoch 2136/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34529.9844 - val_loss: 38832.0391\n",
      "Epoch 2137/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34529.2070 - val_loss: 38668.0859\n",
      "Epoch 2138/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34526.2930 - val_loss: 38832.0391\n",
      "Epoch 2139/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34517.4961 - val_loss: 38680.8281\n",
      "Epoch 2140/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34506.6992 - val_loss: 38787.6133\n",
      "Epoch 2141/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34494.3008 - val_loss: 38723.4141\n",
      "Epoch 2142/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34484.1562 - val_loss: 38726.8711\n",
      "Epoch 2143/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34477.4453 - val_loss: 38771.4609\n",
      "Epoch 2144/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 34473.5039 - val_loss: 38684.2930\n",
      "Epoch 2145/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34470.6641 - val_loss: 38796.8281\n",
      "Epoch 2146/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34466.7070 - val_loss: 38668.6250\n",
      "Epoch 2147/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34461.3398 - val_loss: 38791.3789\n",
      "Epoch 2148/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34453.4570 - val_loss: 38677.8359\n",
      "Epoch 2149/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34444.5508 - val_loss: 38761.9531\n",
      "Epoch 2150/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34435.1680 - val_loss: 38705.3711\n",
      "Epoch 2151/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34426.5547 - val_loss: 38724.1133\n",
      "Epoch 2152/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34419.4453 - val_loss: 38736.8711\n",
      "Epoch 2153/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34413.4961 - val_loss: 38693.4219\n",
      "Epoch 2154/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34408.4453 - val_loss: 38759.4883\n",
      "Epoch 2155/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34403.6719 - val_loss: 38669.5430\n",
      "Epoch 2156/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34399.4570 - val_loss: 38774.9961\n",
      "Epoch 2157/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34394.8008 - val_loss: 38651.8867\n",
      "Epoch 2158/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34390.8398 - val_loss: 38787.4219\n",
      "Epoch 2159/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34385.9961 - val_loss: 38638.6719\n",
      "Epoch 2160/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34381.9570 - val_loss: 38795.4531\n",
      "Epoch 2161/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34376.4453 - val_loss: 38631.2109\n",
      "Epoch 2162/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34370.9492 - val_loss: 38790.1641\n",
      "Epoch 2163/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34363.1562 - val_loss: 38633.7539\n",
      "Epoch 2164/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34354.9492 - val_loss: 38764.9219\n",
      "Epoch 2165/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34344.7070 - val_loss: 38650.3281\n",
      "Epoch 2166/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34334.8281 - val_loss: 38724.7656\n",
      "Epoch 2167/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34325.0547 - val_loss: 38679.4609\n",
      "Epoch 2168/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34316.8320 - val_loss: 38684.1211\n",
      "Epoch 2169/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34310.0430 - val_loss: 38708.4531\n",
      "Epoch 2170/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34304.5039 - val_loss: 38651.9219\n",
      "Epoch 2171/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34300.0039 - val_loss: 38735.7930\n",
      "Epoch 2172/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34296.2070 - val_loss: 38624.4219\n",
      "Epoch 2173/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34293.6602 - val_loss: 38764.4570\n",
      "Epoch 2174/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34291.3008 - val_loss: 38599.8867\n",
      "Epoch 2175/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34290.6992 - val_loss: 38794.7617\n",
      "Epoch 2176/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34289.3008 - val_loss: 38580.9961\n",
      "Epoch 2177/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34288.7891 - val_loss: 38810.4609\n",
      "Epoch 2178/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34283.6641 - val_loss: 38577.7891\n",
      "Epoch 2179/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34275.8398 - val_loss: 38782.1367\n",
      "Epoch 2180/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34261.3359 - val_loss: 38603.6211\n",
      "Epoch 2181/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34245.5430 - val_loss: 38710.5781\n",
      "Epoch 2182/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34230.3359 - val_loss: 38661.5039\n",
      "Epoch 2183/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34220.1992 - val_loss: 38638.2500\n",
      "Epoch 2184/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34215.4883 - val_loss: 38720.8789\n",
      "Epoch 2185/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34214.0039 - val_loss: 38595.4219\n",
      "Epoch 2186/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34213.5586 - val_loss: 38753.5781\n",
      "Epoch 2187/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34210.6992 - val_loss: 38581.6406\n",
      "Epoch 2188/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34206.3008 - val_loss: 38749.3633\n",
      "Epoch 2189/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34197.0508 - val_loss: 38593.9570\n",
      "Epoch 2190/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34186.4961 - val_loss: 38707.5859\n",
      "Epoch 2191/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34174.1953 - val_loss: 38630.4609\n",
      "Epoch 2192/10000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1164/1164 [==============================] - 0s 22us/step - loss: 34163.5547 - val_loss: 38651.0781\n",
      "Epoch 2193/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34155.8008 - val_loss: 38673.1289\n",
      "Epoch 2194/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34150.5117 - val_loss: 38608.8281\n",
      "Epoch 2195/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34146.8008 - val_loss: 38703.8867\n",
      "Epoch 2196/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34143.2930 - val_loss: 38584.7930\n",
      "Epoch 2197/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34139.6992 - val_loss: 38717.9141\n",
      "Epoch 2198/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34134.6719 - val_loss: 38574.9883\n",
      "Epoch 2199/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34129.4844 - val_loss: 38717.7070\n",
      "Epoch 2200/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34122.3281 - val_loss: 38575.9961\n",
      "Epoch 2201/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34114.9844 - val_loss: 38702.2891\n",
      "Epoch 2202/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34105.9961 - val_loss: 38588.3789\n",
      "Epoch 2203/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34096.9844 - val_loss: 38673.2461\n",
      "Epoch 2204/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34087.7109 - val_loss: 38610.0039\n",
      "Epoch 2205/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34079.1680 - val_loss: 38638.1094\n",
      "Epoch 2206/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34071.5117 - val_loss: 38633.7891\n",
      "Epoch 2207/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34064.9492 - val_loss: 38608.3789\n",
      "Epoch 2208/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34059.0039 - val_loss: 38655.6719\n",
      "Epoch 2209/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34053.8398 - val_loss: 38582.9219\n",
      "Epoch 2210/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34049.4453 - val_loss: 38679.9570\n",
      "Epoch 2211/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34045.6719 - val_loss: 38555.7656\n",
      "Epoch 2212/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34044.2109 - val_loss: 38720.3711\n",
      "Epoch 2213/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34044.6719 - val_loss: 38522.7891\n",
      "Epoch 2214/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 34049.2109 - val_loss: 38774.5781\n",
      "Epoch 2215/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34054.0156 - val_loss: 38495.3711\n",
      "Epoch 2216/10000\n",
      "1164/1164 [==============================] - 0s 21us/step - loss: 34060.5039 - val_loss: 38803.0859\n",
      "Epoch 2217/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 34056.0547 - val_loss: 38495.9219\n",
      "Epoch 2218/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34042.2031 - val_loss: 38735.0430\n",
      "Epoch 2219/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 34014.3281 - val_loss: 38552.1133\n",
      "Epoch 2220/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33988.4453 - val_loss: 38607.8633\n",
      "Epoch 2221/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33973.5547 - val_loss: 38655.3906\n",
      "Epoch 2222/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33972.6641 - val_loss: 38524.0781\n",
      "Epoch 2223/10000\n",
      "1164/1164 [==============================] - 0s 25us/step - loss: 33978.6641 - val_loss: 38722.1133\n",
      "Epoch 2224/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33980.5039 - val_loss: 38506.8359\n",
      "Epoch 2225/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33974.8320 - val_loss: 38695.7070\n",
      "Epoch 2226/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33958.7891 - val_loss: 38545.1289\n",
      "Epoch 2227/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 33941.6641 - val_loss: 38610.3281\n",
      "Epoch 2228/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33928.5039 - val_loss: 38620.1133\n",
      "Epoch 2229/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33923.0430 - val_loss: 38538.2070\n",
      "Epoch 2230/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33922.9492 - val_loss: 38674.2344\n",
      "Epoch 2231/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33922.4844 - val_loss: 38512.2109\n",
      "Epoch 2232/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 33919.2891 - val_loss: 38672.8789\n",
      "Epoch 2233/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33910.1719 - val_loss: 38526.3359\n",
      "Epoch 2234/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33898.5586 - val_loss: 38624.2930\n",
      "Epoch 2235/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33886.3281 - val_loss: 38570.2656\n",
      "Epoch 2236/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33876.5156 - val_loss: 38565.7383\n",
      "Epoch 2237/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33870.2891 - val_loss: 38618.6289\n",
      "Epoch 2238/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33866.4961 - val_loss: 38528.1367\n",
      "Epoch 2239/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33863.5039 - val_loss: 38645.0117\n",
      "Epoch 2240/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 33859.4961 - val_loss: 38514.5117\n",
      "Epoch 2241/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33854.0156 - val_loss: 38641.0391\n",
      "Epoch 2242/10000\n",
      "1164/1164 [==============================] - 0s 22us/step - loss: 33846.1992 - val_loss: 38522.5781\n",
      "Epoch 2243/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33837.2891 - val_loss: 38612.3867\n",
      "Epoch 2244/10000\n",
      "1164/1164 [==============================] - 0s 23us/step - loss: 33827.7109 - val_loss: 38546.3359\n",
      "Epoch 2245/10000\n",
      "1164/1164 [==============================] - 0s 24us/step - loss: 33818.8359 - val_loss: 38574.3750\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 02245: early stopping\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "NN_500E_Adam31_LReLU05 = Sequential()\n",
    "NN_500E_Adam31_LReLU05.add(Dense(512,input_dim = IN_DIM))\n",
    "NN_500E_Adam31_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam31_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam31_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam31_LReLU05.add(Dense(512))\n",
    "NN_500E_Adam31_LReLU05.add(LeakyReLU(alpha=0.5))\n",
    "NN_500E_Adam31_LReLU05.add(Dense(1))\n",
    "\n",
    "newAdam = Adam(learning_rate=0.0001)\n",
    "es = EarlyStopping(monitor='val_loss', patience = 30, mode='min', restore_best_weights=True, verbose=1)\n",
    "NN_500E_Adam31_LReLU05.compile(loss=root_mean_squared_error, optimizer=newAdam)\n",
    "history = NN_500E_Adam31_LReLU05.fit(x=X,y=y,batch_size=TRAIN_LEN,epochs=10000,validation_split=0.2,callbacks=[es])\n",
    "\n",
    "plt = plotHistory(history)\n",
    "plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>119317.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>230065.593750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>196312.937500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>189931.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>170798.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1454</th>\n",
       "      <td>2915</td>\n",
       "      <td>92498.351562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1455</th>\n",
       "      <td>2916</td>\n",
       "      <td>100146.835938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1456</th>\n",
       "      <td>2917</td>\n",
       "      <td>192835.781250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1457</th>\n",
       "      <td>2918</td>\n",
       "      <td>109085.664062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1458</th>\n",
       "      <td>2919</td>\n",
       "      <td>217190.984375</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1459 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Id      SalePrice\n",
       "0     1461  119317.781250\n",
       "1     1462  230065.593750\n",
       "2     1463  196312.937500\n",
       "3     1464  189931.125000\n",
       "4     1465  170798.781250\n",
       "...    ...            ...\n",
       "1454  2915   92498.351562\n",
       "1455  2916  100146.835938\n",
       "1456  2917  192835.781250\n",
       "1457  2918  109085.664062\n",
       "1458  2919  217190.984375\n",
       "\n",
       "[1459 rows x 2 columns]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_df = pd.DataFrame(pred(NN_500E_Adam31_LReLU05))\n",
    "y_df = y_df.rename(columns={0:'SalePrice'})\n",
    "out = Id.copy()\n",
    "out = out.join(y_df)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.to_csv(r'~/Datas/KaggleHouse/NN_500E_Adam31_LReLU05_V2_Nolog.csv',index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
